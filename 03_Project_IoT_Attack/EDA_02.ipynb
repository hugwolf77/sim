{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from config import data_setting, predict_setting\n",
    "from dataFactory import Read_DataList, CIC_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - [ Start Read Data-List And Load Data-files ] - \n",
      "\n",
      "Now Loading........TCP_IP-DoS-SYN1_train.pcap.csv...Check.left/Total_file..(48/48)\n",
      "Skip:..Recon-OS_Scan_train.pcap.csv..is..in...the..List..of..skip..Category..Class...(47/48)\n",
      "Now Loading........TCP_IP-DDoS-UDP8_train.pcap.csv...Check.left/Total_file..(46/48)\n",
      "Now Loading........TCP_IP-DoS-ICMP4_train.pcap.csv...Check.left/Total_file..(45/48)\n",
      "Now Loading........TCP_IP-DDoS-SYN4_train.pcap.csv...Check.left/Total_file..(44/48)\n",
      "Now Loading........TCP_IP-DoS-TCP3_train.pcap.csv...Check.left/Total_file..(43/48)\n",
      "Skip:..Recon-VulScan_train.pcap.csv..is..in...the..List..of..skip..Category..Class...(42/48)\n",
      "Now Loading........TCP_IP-DDoS-ICMP8_train.pcap.csv...Check.left/Total_file..(41/48)\n",
      "Now Loading........TCP_IP-DDoS-TCP4_train.pcap.csv...Check.left/Total_file..(40/48)\n",
      "Now Loading........TCP_IP-DDoS-UDP5_train.pcap.csv...Check.left/Total_file..(39/48)\n",
      "Now Loading........TCP_IP-DoS-ICMP2_train.pcap.csv...Check.left/Total_file..(38/48)\n",
      "Now Loading........TCP_IP-DoS-ICMP3_train.pcap.csv...Check.left/Total_file..(37/48)\n",
      "Now Loading........TCP_IP-DDoS-ICMP5_train.pcap.csv...Check.left/Total_file..(36/48)\n",
      "Skip:..Recon-Ping_Sweep_train.pcap.csv..is..in...the..List..of..skip..Category..Class...(35/48)\n",
      "Now Loading........TCP_IP-DoS-UDP2_train.pcap.csv...Check.left/Total_file..(34/48)\n",
      "Now Loading........TCP_IP-DoS-TCP4_train.pcap.csv...Check.left/Total_file..(33/48)\n",
      "Now Loading........TCP_IP-DDoS-UDP4_train.pcap.csv...Check.left/Total_file..(32/48)\n",
      "Now Loading........TCP_IP-DDoS-UDP7_train.pcap.csv...Check.left/Total_file..(31/48)\n",
      "Now Loading........TCP_IP-DoS-TCP2_train.pcap.csv...Check.left/Total_file..(30/48)\n",
      "Now Loading........TCP_IP-DDoS-ICMP4_train.pcap.csv...Check.left/Total_file..(29/48)\n",
      "Now Loading........MQTT-DoS-Publish_Flood_train.pcap.csv...Check.left/Total_file..(28/48)\n",
      "Now Loading........TCP_IP-DoS-TCP1_train.pcap.csv...Check.left/Total_file..(27/48)\n",
      "Now Loading........TCP_IP-DoS-UDP1_train.pcap.csv...Check.left/Total_file..(26/48)\n",
      "Now Loading........MQTT-DoS-Connect_Flood_train.pcap.csv...Check.left/Total_file..(25/48)\n",
      "Now Loading........TCP_IP-DoS-ICMP1_train.pcap.csv...Check.left/Total_file..(24/48)\n",
      "Skip:..ARP_Spoofing_train.pcap.csv..is..in...the..List..of..skip..Category..Class...(23/48)\n",
      "Now Loading........TCP_IP-DDoS-SYN3_train.pcap.csv...Check.left/Total_file..(22/48)\n",
      "Now Loading........TCP_IP-DDoS-ICMP3_train.pcap.csv...Check.left/Total_file..(21/48)\n",
      "Now Loading........TCP_IP-DDoS-ICMP6_train.pcap.csv...Check.left/Total_file..(20/48)\n",
      "Now Loading........TCP_IP-DoS-SYN2_train.pcap.csv...Check.left/Total_file..(19/48)\n",
      "Now Loading........TCP_IP-DDoS-UDP2_train.pcap.csv...Check.left/Total_file..(18/48)\n",
      "Now Loading........TCP_IP-DoS-SYN4_train.pcap.csv...Check.left/Total_file..(17/48)\n",
      "Now Loading........TCP_IP-DDoS-UDP6_train.pcap.csv...Check.left/Total_file..(16/48)\n",
      "Now Loading........TCP_IP-DDoS-TCP2_train.pcap.csv...Check.left/Total_file..(15/48)\n",
      "Now Loading........TCP_IP-DDoS-UDP1_train.pcap.csv...Check.left/Total_file..(14/48)\n",
      "Skip:..MQTT-Malformed_Data_train.pcap.csv..is..in...the..List..of..skip..Category..Class...(13/48)\n",
      "Skip:..Recon-Port_Scan_train.pcap.csv..is..in...the..List..of..skip..Category..Class...(12/48)\n",
      "Now Loading........TCP_IP-DDoS-UDP3_train.pcap.csv...Check.left/Total_file..(11/48)\n",
      "Now Loading........TCP_IP-DDoS-TCP3_train.pcap.csv...Check.left/Total_file..(10/48)\n",
      "Now Loading........TCP_IP-DoS-UDP4_train.pcap.csv...Check.left/Total_file..(9/48)\n",
      "Now Loading........TCP_IP-DoS-SYN3_train.pcap.csv...Check.left/Total_file..(8/48)\n",
      "Now Loading........Benign_train.pcap.csv...Check.left/Total_file..(7/48)\n",
      "Now Loading........MQTT-DDoS-Publish_Flood_train.pcap.csv...Check.left/Total_file..(6/48)\n",
      "Now Loading........TCP_IP-DDoS-SYN1_train.pcap.csv...Check.left/Total_file..(5/48)\n",
      "Now Loading........TCP_IP-DDoS-SYN2_train.pcap.csv...Check.left/Total_file..(4/48)\n",
      "Now Loading........TCP_IP-DDoS-ICMP7_train.pcap.csv...Check.left/Total_file..(3/48)\n",
      "Now Loading........TCP_IP-DDoS-TCP1_train.pcap.csv...Check.left/Total_file..(2/48)\n",
      "Now Loading........TCP_IP-DoS-UDP3_train.pcap.csv...Check.left/Total_file..(1/48)\n",
      "\n",
      "Splitting training data at index 5178508 for validation.\n",
      "Skip:..MQTT-Malformed_Data_test.pcap.csv..is..in...the..List..of..skip..Category..Class...(21/21)\n",
      "Now Loading........TCP_IP-DDoS-UDP2_test.pcap.csv...Check.left/Total_file..(20/21)\n",
      "Now Loading........MQTT-DoS-Connect_Flood_test.pcap.csv...Check.left/Total_file..(19/21)\n",
      "Now Loading........Benign_test.pcap.csv...Check.left/Total_file..(18/21)\n",
      "Now Loading........TCP_IP-DoS-UDP_test.pcap.csv...Check.left/Total_file..(17/21)\n",
      "Now Loading........MQTT-DoS-Publish_Flood_test.pcap.csv...Check.left/Total_file..(16/21)\n",
      "Skip:..ARP_Spoofing_test.pcap.csv..is..in...the..List..of..skip..Category..Class...(15/21)\n",
      "Now Loading........MQTT-DDoS-Publish_Flood_test.pcap.csv...Check.left/Total_file..(14/21)\n",
      "Now Loading........TCP_IP-DDoS-TCP_test.pcap.csv...Check.left/Total_file..(13/21)\n",
      "Skip:..Recon-Port_Scan_test.pcap.csv..is..in...the..List..of..skip..Category..Class...(12/21)\n",
      "Now Loading........TCP_IP-DoS-TCP_test.pcap.csv...Check.left/Total_file..(11/21)\n",
      "Skip:..Recon-OS_Scan_test.pcap.csv..is..in...the..List..of..skip..Category..Class...(10/21)\n",
      "Skip:..Recon-Ping_Sweep_test.pcap.csv..is..in...the..List..of..skip..Category..Class...(9/21)\n",
      "Skip:..Recon-VulScan_test.pcap.csv..is..in...the..List..of..skip..Category..Class...(8/21)\n",
      "Now Loading........TCP_IP-DoS-SYN_test.pcap.csv...Check.left/Total_file..(7/21)\n",
      "Now Loading........TCP_IP-DDoS-ICMP1_test.pcap.csv...Check.left/Total_file..(6/21)\n",
      "Now Loading........TCP_IP-DDoS-SYN_test.pcap.csv...Check.left/Total_file..(5/21)\n",
      "Now Loading........TCP_IP-DDoS-ICMP2_test.pcap.csv...Check.left/Total_file..(4/21)\n",
      "Now Loading........TCP_IP-DDoS-UDP1_test.pcap.csv...Check.left/Total_file..(3/21)\n",
      "Now Loading........TCP_IP-DoS-ICMP_test.pcap.csv...Check.left/Total_file..(2/21)\n",
      "Now Loading........MQTT-DDoS-Connect_Flood_test.pcap.csv...Check.left/Total_file..(1/21)\n",
      "\n",
      "Initialized CIC_Dataset (with test) for :\n",
      "                   \t train data : (5178508, 45), train label : (5178508, 3)\n",
      "                   \t val data : (1294628, 45), val label : (1294628, 3)\n",
      "                   \t test data : (1583015, 45), test label : (1583015, 3)\n"
     ]
    }
   ],
   "source": [
    "# set data path & level \n",
    "filePath = data_setting[\"filePath\"]\n",
    "level = 1 # data_setting[\"level\"]\n",
    "scale = data_setting[\"scale\"]\n",
    "val_size = data_setting[\"val_size\"]\n",
    "pred_filePath = predict_setting[\"pred_filePath\"]\n",
    "\n",
    "# set train with dataset\n",
    "num_workers = data_setting[\"num_workers\"]\n",
    "batch_size = data_setting[\"batch_size\"]\n",
    "drop_last = data_setting[\"drop_last\"]\n",
    "shuffle_flag = data_setting[\"shuffle_flag\"]       \n",
    "add_test = True\n",
    "\n",
    "dataloader = Read_DataList(filePath, val_size, level, scale, add_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train input shape: (5178508, 45) \n",
      "train_label shape: (5178508, 3)\n",
      "\n",
      "val input shape: (1294628, 45) \n",
      "val_label shape: (1294628, 3)\n",
      "\n",
      "test input shape: (1583015, 45) \n",
      "test_label shape: (1583015, 3)\n",
      " \n",
      "\n",
      "label_oneHot_categories: [array(['Benign', 'DDoS', 'DoS'], dtype=object)]\n",
      "feature column name's: ['Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IGMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight', 'class_1']\n",
      "fueature columns length: 46\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 읽어 들이고 데이터의 모양을 확인 (클라스로 부터 데이터 불러오기)\n",
    "train,train_label = dataloader.get_train_data()\n",
    "val, val_label = dataloader.get_val_data()\n",
    "test, test_label = dataloader.get_test_data()\n",
    "print (f\"train input shape: {train.shape} \\ntrain_label shape: {train_label.shape}\\n\")\n",
    "print (f\"val input shape: {val.shape} \\nval_label shape: {val_label.shape}\\n\")\n",
    "print (f\"test input shape: {test.shape} \\ntest_label shape: {test_label.shape}\\n \\n\")\n",
    "\n",
    "oneHot = dataloader.get_oneHot()   # 학습된 원핫 인코더 불러오기 \n",
    "print(f\"label_oneHot_categories: {oneHot.categories_}\")\n",
    "\n",
    "colnames = dataloader.colnames\n",
    "# colnames = colnames.remove('class_1')\n",
    "print(f\"feature column name's: {colnames}\")\n",
    "print(f\"fueature columns length: {len(colnames)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 데이터 리샘프링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trasform to DataFrame\n",
    "df_train_label = pd.DataFrame(train_label, columns=[oneHot.categories_[0]])\n",
    "df_val_label = pd.DataFrame(val_label, columns=[oneHot.categories_[0]])\n",
    "df_test_label = pd.DataFrame(test_label, columns=[oneHot.categories_[0]])\n",
    "\n",
    "# label count plot : 라벨 데이터의 클라스별 비율 확인 \n",
    "def plot_column_sums(df, flag:str):\n",
    "    column_sums = df.sum()\n",
    "    print(f\"{flag.upper} columns count : \\n{column_sums}\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    column_sums.plot(kind='bar')\n",
    "    plt.title(f'Sum of {flag.upper()}\\'s Each Column')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Sum')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "\n",
    "tlk = TomekLinks(sampling_strategy='auto')\n",
    "train_tmk, train_label_tmk = tlk.fit_resample(train, train_label)\n",
    "\n",
    "df_train_tmk = pd.DataFrame(train_tmk)\n",
    "df_train_label_tmk = pd.DataFrame(train_label_tmk)\n",
    "df_train_tmk.to_csv('./data/CIC_2024/balanced/df_train_tmk.csv', index=False)\n",
    "df_train_label_tmk.to_csv('./data/CIC_2024/balanced/df_train_label_tmk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tmk, val_label_tmk = tlk.fit_resample(val, val_label)\n",
    "\n",
    "df_val_tmk = pd.DataFrame(val_tmk)\n",
    "df_val_label_tmk = pd.DataFrame(val_label_tmk)\n",
    "df_val_tmk.to_csv('./data/CIC_2024/balanced/df_val_tmk.csv', index=False)\n",
    "df_val_label_tmk.to_csv('./data/CIC_2024/balanced/df_val_label_tmk.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tmk, test_label_tmk = tlk.fit_resample(test, test_label)\n",
    "\n",
    "df_test_tmk = pd.DataFrame(test_tmk)\n",
    "df_test_label_tmk = pd.DataFrame(test_label_tmk)\n",
    "df_test_tmk.to_csv('./data/CIC_2024/balanced/df_test_tmk.csv', index=False)\n",
    "df_test_label_tmk.to_csv('./data/CIC_2024/balanced/df_test_label_tmk.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 군집분석 및 차원축소 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = './data/CIC_2024/balanced/df_train_rus.csv'\n",
    "# train_label = './data/CIC_2024/balanced/df_train_label_rus.csv'\n",
    "\n",
    "val = './data/CIC_2024/balanced/df_val_rus.csv'\n",
    "val_label = './data/CIC_2024/balanced/df_val_label_rus.csv'\n",
    "\n",
    "# df_test = './data/CIC_2024/balanced/df_test_rus.csv'\n",
    "# df_train = pd.read_csv(train)\n",
    "df_val = pd.read_csv(val)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_val_scaled = scaler.fit_transform(df_val)\n",
    "print(f\"{df_val_scaled.shape}\")\n",
    "\n",
    "df_val_label = pd.read_csv(val_label)\n",
    "df_val_label.head()\n",
    "print(f\"{df_val_label.shape}\")\n",
    "df_val_rvs = oneHot.inverse_transform(df_val_label)\n",
    "df_val_rvs = pd.DataFrame(df_val_rvs, columns=['class'])\n",
    "df_val_rvs['class'] = df_val_rvs['class'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column_sums(df_val_label, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. T-sne 차원감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = 30.\n",
    "n_iter = 300\n",
    "n_components_3d = 3\n",
    "tsne_3d = TSNE(n_components=n_components_3d, perplexity=perplexity, n_iter=n_iter, random_state=42)\n",
    "val_embedded = tsne_3d.fit_transform(df_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nt-SNE 3차원 임베딩 후 데이터 형태:\", val_embedded.shape)\n",
    "\n",
    "# 5. t-SNE 결과 시각화 (3차원으로 임베딩했을 경우)\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(val_embedded[:, 0], val_embedded[:, 1], val_embedded[:, 2], c=df_val_rvs['class'].cat.codes, cmap='prism', s=1, alpha=0.6)\n",
    "ax.set_xlabel('t-SNE Component 1')\n",
    "ax.set_ylabel('t-SNE Component 2')\n",
    "ax.set_zlabel('t-SNE Component 3')\n",
    "ax.set_title(f't-SNE 3D visualization of digits dataset (perplexity={perplexity}, n_iter={n_iter})')\n",
    "# fig.colorbar(scatter, label='Digit')\n",
    "ax.view_init(elev=50, azim=-50, roll=0)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca.fit(df_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 주성분 분석 결과 확인\n",
    "print(\"고유값 (설명된 분산):\", pca.explained_variance_)\n",
    "print(\"설명된 분산 비율:\", pca.explained_variance_ratio_)\n",
    "print(\"주성분 (고유 벡터):\")\n",
    "for i, component in enumerate(pca.components_):\n",
    "    print(f\"  주성분 {i+1}: {component}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(df_val)  # Standardize the data\n",
    "factors = 2\n",
    "#  a list of 2 tuples containing titles for and instances of or class\n",
    "fas = [\n",
    "    (\"FA no rotation\", FactorAnalysis(n_components = factors)),\n",
    "    (\"FA varimax\", FactorAnalysis(n_components = factors, rotation=\"varimax\")),\n",
    "]  \n",
    "\n",
    "#  Let's prepare some plots on one canvas (subplots)\n",
    "fig, axes = plt.subplots(ncols=len(fas), figsize=(10, 8))\n",
    "\n",
    "'''\n",
    "And loop over the variants of our analysis `fas`, zipped with the \n",
    "plot axes `axes`\n",
    "'''\n",
    "for ax, (title, fa) in zip(axes, fas):\n",
    "    #  Fit the model to the standardized food data\n",
    "    fa = fa.fit(X)\n",
    "    #  and transpose the component (loading) matrix\n",
    "    factor_matrix = fa.components_.T\n",
    "    #  Plot the data as a heat map\n",
    "    im = ax.imshow(factor_matrix, cmap=\"RdBu_r\", vmax=1, vmin=-1)\n",
    "    #  and add the corresponding value to the center of each cell\n",
    "    for (i,j), z in np.ndenumerate(factor_matrix):\n",
    "        ax.text(j, i, str(z.round(2)), ha=\"center\", va=\"center\")\n",
    "    #  Tell matplotlib about the metadata of the plot\n",
    "    ax.set_yticks(np.arange(len(df_val.columns)))\n",
    "    if ax.get_subplotspec().is_first_col():\n",
    "        ax.set_yticklabels(df_val.columns)\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels([\"Factor 1\", \"Factor 2\"])\n",
    "    #  and squeeze the axes tight, to save space\n",
    "    plt.tight_layout()\n",
    "    \n",
    "#  and add a colorbar\n",
    "cb = fig.colorbar(im, ax=axes, location='right', label=\"loadings\")\n",
    "#  show us the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply Bartlett's test\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(df_val_scaled)\n",
    "print(f'Chi-square value: {chi_square_value}\\nP-value: {p_value}')\n",
    "\n",
    "# Apply KMO test\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "kmo_all, kmo_model = calculate_kmo(df_val_scaled)\n",
    "print(f'KMO Model: {kmo_model}')\n",
    "\n",
    "# Create factor analysis object and perform factor analysis\n",
    "fa = FactorAnalyzer(rotation=\"varimax\")\n",
    "fa.fit(df_val_scaled)\n",
    "\n",
    "# Check Eigenvalues\n",
    "eigen_values, vectors = fa.get_eigenvalues()\n",
    "plt.scatter(range(1, df_val_scaled.shape[1]+1), eigen_values)\n",
    "plt.plot(range(1, df_val_scaled.shape[1]+1), eigen_values)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Factors')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim-0TBU-pA2-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
