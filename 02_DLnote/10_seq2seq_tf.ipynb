{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "599b55d7",
   "metadata": {},
   "source": [
    "### seq2seq with teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1407fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd             #데이터를 Dataframe으로 다루는 라이브러리\n",
    "from pandas import DataFrame\n",
    "import numpy as np              #데이터를 행열 또는 array 형태로 다루는 라이브러리\n",
    "import random\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt #데이터를 그래프 plot으로 보여주는 라이브러리\n",
    "import seaborn as sns\n",
    "\n",
    "import requests                 #네트워크 접근 라이브러리\n",
    "from bs4 import BeautifulSoup   #웹사이트 접근하는 라이브러리\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "torch.manual_seed(77)\n",
    "\n",
    "from torch import nn as nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch import tensor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e938adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "target = '삼성전자'\n",
    "\n",
    "# 종목 이름을 입력하면 종목에 해당하는 코드를 불러와\n",
    "# 네이버 금융(http://finance.naver.com)에 넣어줌\n",
    "\n",
    "code_krx = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13', encoding='cp949', header=0)[0]\n",
    "# 종목코드가 6자리이기 때문에 6자리를 맞춰주기 위해 설정해줌\n",
    "code_krx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0e82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_krx.종목코드 = code_krx.종목코드.map('{:06d}'.format)\n",
    "# 우리가 필요한 것은 회사명과 종목코드이기 때문에 필요없는 column들은 제외해준다.\n",
    "code_krx = code_krx[['회사명', '종목코드']]\n",
    "# code_krx\n",
    "target_code = code_krx[code_krx['회사명']==target]\n",
    "target_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종목코드로 네이버에서 종목 주소 생성 확인\n",
    "def get_url(code): #(item_name, code_df):\n",
    "    url = 'https://finance.naver.com/item/sise_day.naver?code='+'{code}'.format(code=code).lstrip()\n",
    "    return url\n",
    "code = target_code['종목코드'].iloc[0]\n",
    "url = get_url(code)\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd60a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 찾은 네이버 타겟종목 페이지에서 마지막 페이지 크기 확인\n",
    "url_page = url + '&page=1'\n",
    "headers = {'User-agent':'Mozilla/5.0'}\n",
    "html = requests.get(url_page, headers=headers).text\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "tags = soup.find_all('a')\n",
    "# print(tags[11][\"href\"])\n",
    "last_page = tags[11][\"href\"]\n",
    "last_page = last_page.split('=')[2]\n",
    "last_page\n",
    "# print(f\"target_code :회사명 == [{target_code['회사명'].iloc[0]}] 종목코드 == [{target_code['종목코드'].iloc[0]}] 마지막 페이지 == [{last_page}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5b1c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_page = int(150) # 임시 150 페이지 까지만\n",
    "\n",
    "# 일 데이터를 담을 df_code라는 DataFrame 정의\n",
    "df_code = pd.DataFrame()\n",
    "url.lstrip()\n",
    "for page in range(1,int(last_page)+1):\n",
    "    # pg_url = ('{url}&page={page}'.format(url=url, page=page)).lstrip()\n",
    "    # pg = pd.read_html(pg_url,encoding='euc-kr',header=1)[0]\n",
    "    # df_code = pd.concat([df_code,pg])\n",
    "    req = requests.get(f'{url}&page={page}',headers=headers)\n",
    "    df_code = pd.concat([df_code,pd.read_html(req.text,encoding='euc-kr')[0]],ignore_index=True)\n",
    "\n",
    "# df.dropna()를 이용해 결측값 있는 행 제거\n",
    "df_code = df_code.dropna()\n",
    "df_code.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 상위 15개 데이터 확인하기\n",
    "df_code.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9208d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글로 된 컬럼명을 영어로 바꿔줌\n",
    "stock_data = df_code.rename(columns= {'날짜': 'date', '종가': 'close', '전일비': 'diff', '시가': 'open', '고가': 'high', '저가': 'low', '거래량': 'volume'})\n",
    "stock_data['diff'] = stock_data['diff'].str.replace('상승','').str.replace('하락','').str.replace('보합','').str.replace(',','').str.replace(' ','').str.strip()\n",
    "# 데이터의 타입을 int형으로 바꿔줌\n",
    "stock_data[['close', 'diff', 'open', 'high', 'low', 'volume']] = stock_data[['close', 'diff', 'open', 'high', 'low', 'volume']].astype(float)\n",
    "# 컬럼명 'date'의 타입을 date로 바꿔줌\n",
    "stock_data['date'] = pd.to_datetime(stock_data['date'])\n",
    "# 일자(date)를 기준으로  정렬\n",
    "stock_data = stock_data.sort_values(by=['date'],ascending = True)\n",
    "# 상위 5개 데이터 확인\n",
    "print(stock_data.shape)\n",
    "stock_data.reset_index(drop=True, inplace=True)\n",
    "stock_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d6acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stock_data.copy()\n",
    "df = df.set_index('date')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(subplots=True, figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(15,3))\n",
    "ax.plot(df['close'])\n",
    "ax.vlines(df[df.index == df.iloc[round(len(df)*0.79)].name].index,\n",
    "          ymax=(max(df['close'])+2), \n",
    "          ymin=(min(df['close'])-1), colors='red', linestyles= '-')\n",
    "ax.vlines(df[df.index == df.iloc[round(len(df)*0.89)].name].index,\n",
    "          ymax=(max(df['close'])+2), \n",
    "          ymin=(min(df['close'])-1), colors='red', linestyles= ':')\n",
    "ax.vlines(df[df.index == df.iloc[round(len(df)*0.99)].name].index,\n",
    "          ymax=(max(df['close'])+2), \n",
    "          ymin=(min(df['close'])-1), colors='red', linestyles= '-')\n",
    "ax.set_title(target + ' : ' + code)\n",
    "ax.set_xlabel('timestemp')\n",
    "ax.set_ylabel('values')\n",
    "ax.grid()\n",
    "fig.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53be6583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 훈련, 검증, 테스트로 나눈다.\n",
    "df_train = df.iloc[:round(len(df)*0.8)]\n",
    "df_valid = df.iloc[round(len(df)*0.8):round(len(df)*0.9)]\n",
    "df_test = df.iloc[round(len(df)*0.9):len(df)]\n",
    "\n",
    "print(f\" df.shape 0.8 : {round(df.shape[0]*0.8)}\")\n",
    "print(f\" df_train.shape : {df_train.shape}\")\n",
    "print(f\" df_valid.shape : {df_valid.shape}\")\n",
    "print(f\" df_test.shape : {df_test.shape}\")\n",
    "# print(f\" check {(round(df.shape[0]*0.7)) == df_train.shape[0]+df_valid.shape[0]+df_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9392370",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train)\n",
    "df_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df.columns)\n",
    "df_valid_scaled = pd.DataFrame(scaler.transform(df_valid),columns=df.columns)\n",
    "df_test_scaled = pd.DataFrame(scaler.transform(df_test),columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, target, features, sequence_length=5, predict_length=1):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.predict_length = predict_length\n",
    "        self.y = torch.tensor(dataframe[target].values).float()\n",
    "        self.X = torch.tensor(dataframe[features].values).float()\n",
    "        self.datetime = dataframe.index \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - (self.sequence_length + self.predict_length)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.sequence_length\n",
    "        seq_x = self.X[s_begin:s_end]\n",
    "        r_begin = s_end #- self.predict_length\n",
    "        r_end = r_begin + self.predict_length\n",
    "        seq_y = self.y[r_begin :r_end]\n",
    "        return seq_x, seq_y\n",
    "\n",
    " # start point padding type\n",
    "    # def __len__(self):\n",
    "    #     return self.X.shape[0]\n",
    "\n",
    "    # def __getitem__(self, i): \n",
    "    #     if i >= self.sequence_length - 1:\n",
    "    #         i_start = i - self.sequence_length + 1\n",
    "    #         x = self.X[i_start:(i + 1), :]\n",
    "    #     else:\n",
    "    #         padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n",
    "    #         x = self.X[0:(i + 1), :]\n",
    "    #         x = torch.cat((padding, x), 0)\n",
    "\n",
    "        return x, self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8efe56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target='close'\n",
    "features= df.columns\n",
    "sequence_length = 13\n",
    "predict_length = 1\n",
    "\n",
    "train_dataset = SequenceDataset(\n",
    "    df_train_scaled,\n",
    "    target=target,\n",
    "    features=features,\n",
    "    sequence_length=sequence_length,\n",
    "    predict_length = predict_length\n",
    ")\n",
    "\n",
    "i = 27\n",
    "X, y = train_dataset[i]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6344ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train_dataset[i+1]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcda5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'close'\n",
    "features = df_train_scaled.columns\n",
    "batch_size = 30\n",
    "sequence_length = 25\n",
    "predict_length = 7\n",
    "\n",
    "train_dataset = SequenceDataset(\n",
    "    df_train_scaled,\n",
    "    target=target,\n",
    "    features=features,\n",
    "    sequence_length=sequence_length,\n",
    "    predict_length = predict_length\n",
    ")\n",
    "valid_dataset = SequenceDataset(\n",
    "    df_valid_scaled,\n",
    "    target=target,\n",
    "    features=features,\n",
    "    sequence_length=sequence_length,\n",
    "    predict_length = predict_length\n",
    ")\n",
    "test_dataset = SequenceDataset(\n",
    "    df_test_scaled,\n",
    "    target=target,\n",
    "    features=features,\n",
    "    sequence_length=sequence_length,\n",
    "    predict_length = predict_length\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)#, collate_fn=my_collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)#, collate_fn=my_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=True)#,collate_fn=my_collate_fn)\n",
    "\n",
    "X, y = next(iter(train_loader))\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e41c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_features, \n",
    "                 hidden_units,\n",
    "                 predict_length, \n",
    "                 bias=True, \n",
    "                 num_layers=3, \n",
    "                 drop_ratio=0.0,\n",
    "                 bi_dir=False,\n",
    "                 device='cpu'):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features  # feature's = 현재 1\n",
    "        self.hidden_units = hidden_units\n",
    "        self.bias = bias\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.bi_dir = bi_dir\n",
    "        self.device = device\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_features,\n",
    "            hidden_size=hidden_units,\n",
    "            bias=bias,\n",
    "            batch_first=True,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=drop_ratio,\n",
    "            bidirectional=bi_dir\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_().to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_().to(self.device)\n",
    "        out, (hidden_state, cell_state) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        return out, (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1017d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_features, \n",
    "                 hidden_units,\n",
    "                 predict_length, \n",
    "                 bias=True, \n",
    "                 num_layers=3, \n",
    "                 drop_ratio=0.0,\n",
    "                 bi_dir=False,\n",
    "                 device='cpu'):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features  # feature's = 현재 1\n",
    "        self.hidden_units = hidden_units\n",
    "        self.bias = bias\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.bi_dir = bi_dir\n",
    "        self.device = device\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_features,\n",
    "            hidden_size=hidden_units,\n",
    "            bias=bias,\n",
    "            batch_first=True,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=drop_ratio,\n",
    "            bidirectional=bi_dir\n",
    "        )\n",
    "        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1, bias=bias)\n",
    "\n",
    "    def forward(self, initial_input, encoder_outputs, hidden, targets, \n",
    "                teacher_force_probability):\n",
    "        \n",
    "        decoder_sequence_length = targets.shape[1]\n",
    "        outputs = torch.full_like(targets, 0).to(self.device)\n",
    "        \n",
    "        for t in range(decoder_sequence_length):            \n",
    "            input_at_t = initial_input.unsqueeze(1)\n",
    "            output, hidden = self.lstm(input_at_t, hidden)\n",
    "\n",
    "            # outputs[:,t] = self.linear(output)\n",
    "            outputs[:,t]  = self.linear(output).squeeze()\n",
    "            # outputs[:,t] = out\n",
    "            # print(f\"out.shape : {out.shape}\")\n",
    "            # print(f\"outputs[:,t].shape : {outputs[:,t].shape}\")\n",
    "\n",
    "            # Set-up input for next timestep\n",
    "            teacher_force = random.random() < teacher_force_probability\n",
    "            # The next timestep's input will either be this timestep's \n",
    "            # target or output\n",
    "            input_at_t = targets[:,t] if teacher_force else outputs[:,t]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    # def forecast(self, initial_input, encoder_outputs, hidden, predict_length):\n",
    "    #     outputs = torch.zeros_like((predict_length)).to(self.device)\n",
    "    #     input_at_t = initial_input\n",
    "    #     for t in range(predict_length):            \n",
    "    #         output, hidden = self.lstm(input_at_t, hidden)\n",
    "    #         outputs[t] = self.linear(output)\n",
    "    #         input_at_t = outputs[t]\n",
    "    #     return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, encoder_inputs, targets, teacher_force_probability):\n",
    "        encoder_outputs, hidden = self.encoder(encoder_inputs)\n",
    "        \n",
    "        outputs = self.decoder(encoder_inputs[:,-1,:], encoder_outputs,\n",
    "                               hidden, targets, teacher_force_probability)\n",
    "        return outputs\n",
    "\n",
    "    # def forecast(self, encoder_inputs, predict_length):\n",
    "    #     encoder_outputs, hidden = self.encoder(encoder_inputs)\n",
    "    #     outputs = self.decoder.forecast(encoder_inputs[-1], encoder_outputs,\n",
    "    #                            hidden, predict_length)\n",
    "        # return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\" 🤗 is_available device : \\n\\t{device}\")\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "learning_rate = 0.03\n",
    "num_hidden_units = 124\n",
    "hidden_size = 256\n",
    "teacher_force_probability = 0.5\n",
    "\n",
    "seq2seq = Seq2Seq(\n",
    "                    Encoder(\n",
    "                            num_features = len(features), \n",
    "                            hidden_units = num_hidden_units,\n",
    "                            predict_length = predict_length, \n",
    "                            bias=True, \n",
    "                            num_layers=3, \n",
    "                            drop_ratio=0.0,\n",
    "                            bi_dir=False,\n",
    "                            device=device), \n",
    "                    Decoder(\n",
    "                            num_features = len(features), \n",
    "                            hidden_units = num_hidden_units,\n",
    "                            predict_length = predict_length, \n",
    "                            bias=True, \n",
    "                            num_layers=3, \n",
    "                            drop_ratio=0.0,\n",
    "                            bi_dir=False,\n",
    "                            device=device),\n",
    "                )\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "# loss_function = nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.Adam(seq2seq.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"\\n 🤩 loss_function : \\n\\t{loss_function}\\n\")\n",
    "print(f\" 🤓 optimizer : \\n\\t{optimizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50be4f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, loss_function, optimizer, device, teacher_force_probability):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    model.to(device)\n",
    "    \n",
    "    model.train()\n",
    "    for i, (X, y) in enumerate(data_loader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(X,y,teacher_force_probability)\n",
    "        loss = loss_function(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Train loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "\n",
    "def eval_model(data_loader, model, loss_function, device):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    teacher_force_probability = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(X, y, teacher_force_probability)\n",
    "\n",
    "            # print(f\"output.type -  output.shape: {type(output)} - {output.shape}\")\n",
    "            # print(f\"y.shape -  y.type : {type(y)} - {y.shape}\")\n",
    "            # print(f\"one out : {output[0,:]}\")\n",
    "            # print(f\"loss_function(output, y) : {loss_function(output, y)}\")\n",
    "            # raise\n",
    "\n",
    "            total_loss += loss_function(output, y).item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"eval loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def infer_model(data_loader, model, device):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    predict = []\n",
    "    label = []\n",
    "    model.to(device)\n",
    "\n",
    "    teacher_force_probability = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(X, y, teacher_force_probability)\n",
    "            # print(output.shape)\n",
    "            # raise\n",
    "            pred = output.to('cpu').numpy()\n",
    "            np.squeeze(pred)\n",
    "            predict.append(list(pred[0]))\n",
    "            true = y.to('cpu').numpy()\n",
    "            np.squeeze(true)\n",
    "            label.append(list(true[0]))\n",
    "        \n",
    "    return predict, label\n",
    "\n",
    "\n",
    "# def forecasting (data, model, device, predict_length):\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#             X = torch.from_numpy(data).to(torch.float32).to(device)\n",
    "#             X = X.unsqueeze(0)\n",
    "#             output = model.forecast(X, predict_length)\n",
    "#             forecast = output.to('cpu').numpy()\n",
    "#             forecast = np.squeeze(forecast[0])\n",
    "        \n",
    "    # return forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e62d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Untrained evaluate\\n--------\")\n",
    "eval_model(valid_loader, seq2seq, loss_function, device)\n",
    "print()\n",
    "\n",
    "epoch = []\n",
    "train_loss_list = []\n",
    "eval_loss_list = []\n",
    "\n",
    "for ix_epoch in range(epochs):\n",
    "    print(f\"Epoch {ix_epoch}\\n---------\")\n",
    "    train_loss = train_model(train_loader, seq2seq, loss_function, optimizer, device, teacher_force_probability)\n",
    "    eval_loss = eval_model(valid_loader, seq2seq, loss_function, device)\n",
    "    print()\n",
    "    epoch.append(ix_epoch+1)\n",
    "    train_loss_list.append(train_loss)\n",
    "    eval_loss_list.append(eval_loss)\n",
    "result = pd.DataFrame({'epoch':epoch, 'train_loss':train_loss_list, 'eval':eval_loss_list})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(8,5))\n",
    "ax.plot(result['epoch'],result['train_loss'], color='blue', label='train_loss')\n",
    "ax.plot(result['epoch'],result['eval'], color='green', label='eval_loss')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f09ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, gt = infer_model(test_loader, seq2seq, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c1809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 세이브\n",
    "PATH = \"/home/augustine77/mylab/sim/sim/02_DLnote/results/LSTM_samsumg.pth\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, mean_absolute_percentage_error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim-0TBU-pA2-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
