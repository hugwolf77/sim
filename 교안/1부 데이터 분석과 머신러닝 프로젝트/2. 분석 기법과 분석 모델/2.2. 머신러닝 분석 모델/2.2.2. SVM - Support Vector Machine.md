---
categories: 글쓰기
title: 2.2.2 SVM - Support Vector Machine
created: 2025-03-30
tags:
  - 교재
  - 수업
  - SVM
  - ML
---
---
#### *2.2.2. SVM - Support Vector Machine
---

###### 초평면(Hyperplane)
- 변수 (Feature) 가 N 개 있다는 것은 N 차원으로 표현되는 dataset이라고 할 수 있다. 이를 분류하고자 할 때 N-1 차원으로 나눌 수 있는 분류 공간인 초평면(Hyperplane)을 찾는 기법. 
- 예를 들어 초평면(Hyperplane)은 2차원 공간에서는 직선(1차원), 3차원 공간에서는 평면(2차원) 그 이상의 차원에서는 초평면(3차원 이상)을 사용하여 데이터를 구분한다.
###### 마진(margin)
- 이때 초평면(Hyperplane)을 선택하는 조건은 class 사이의 분류 간격 즉, 마진(margin)을 가장 크게 확보하는 초평면을 선택한다.
- 마진(margin)을 최대화한다는 것은 일반화 성능을 높일 수 있다는 의미이다.

###### 서포트 벡터(Support Vector)
- 마진(margin)을 결정하는데 사용하는 초평면에 가장 가까운 데이터 포인트들을 의미한다.


<center>
	<img src ="https://upload.wikimedia.org/wikipedia/commons/2/2a/Svm_max_sep_hyperplane_with_margin.png" withd = 300 height =300>
</center>
- 이진 분류 데이터 정의 
$$\mathcal{D} = \{(x_{i},y_{i})|x_{i} \in \mathbb{R}^{p}, y_{i} \in \{-1,1\}\}_{i=1}^{n}$$
  
- $y_{i}$ 값에 따라 선형적으로 분리될 수 있을 때, 다음 조건을 만족하는  점 x의 집합으로 분류를 표현할 수 있다.
$$w \cdot \ x - b = 0$$
   -  " $\cdot$ " 은 내적(행렬의 곱) 연산자, w 는 초평면의 법선 벡터. 

##### Kernel Trick
- 데이터를 다른 공간 즉, (일반적으로 좀 더 높은) 다른 차원으로 맵핑 해주는 Trick. 
- 예를 들어 선형분류의 경우 2차원하게 되고 이때 데이터를 더 잘 분류할 수 있는 새로운 공간은 고차원으로 표현해주는 것이기 때문에, 저차원 (low dimensional space)에서 고차원(hight dimensional space)로 맵핑(mapping)해주는 커널트릭(kernel Trick)이라고 한다.

[이미지 출처](https://www.researchgate.net/figure/Non-linear-classifier-using-Kernel-trick-16_fig4_340610860)
<center>
<img src="https://www.researchgate.net/profile/Marouane-Hachimi/publication/340610860/figure/fig4/AS:880021191286786@1586824810950/Non-linear-classifier-using-Kernel-trick-16.ppm" withd = 300 height =300>
</center>

- 커널의 종류 (scikit-learn 제공 기준)와 파라메터
	1) 선형(linear):   $\mathcal{C}$              
		- 고차원 매핑하지 않고 원래 공간에서 분류.$$K(x, y) = xT ⋅ y$$
	2) 다항시기(polynomial) :   $\mathcal{C},\  \gamma$   
		- 다항식 형태 고차원 공간 매핑. 
		- 비선형 경계
		- 차수, 계수 등 하이퍼파라미터 필요. $$K(x, y) = (α ⋅ xT ⋅ y + c)d$$ 
	3) sigmoid      :   $\mathcal{C},\  \gamma$ 
		- 신경망의 활성화 함수와 유사한 형태로 매핑.
		- 일부 문제에서 좋은 성능이지만 RBF 커널에 비해 성능이 떨어지는 경우가 많음.$$K(x, y) = tanh(α ⋅ xT ⋅ y + c)$$
	4) rbf              :   $\mathcal{C},\  \gamma$
		- 데이터 포인트 간에 거리를 비교하고 거리가가 높을 수록 높은 유사도 값을 거리가 가까울수록 낮은 유사도 값으로 매핑하는 새로운 특성으로 추가.
		- 이를 이용하여 초공간을 찾는 커널.$$K(x, y) = exp(-γ ||x - y||^{2})$$
		$\mathcal{C}$ : decision boundary와 분류(classfying)을 학습하는 데이터 포인트의 정확도에 대한 tradeoff 관계를 조정하는 파라메터
		$\gamma$ :  분류 학습에 포함시키는 데이터 포인트의 영향력 범위


- 커절 종류에 따른 iris dataset 분류 이미지
[이미지 출처](https://editor.analyticsvidhya.com/uploads/28931svm32.png)
<center>
<img src="https://editor.analyticsvidhya.com/uploads/28931svm32.png" withd = 400 height =400>
</center>

---
[scikit-learn-SVM](https://scikit-learn.org/stable/modules/svm.html)
[SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)


 