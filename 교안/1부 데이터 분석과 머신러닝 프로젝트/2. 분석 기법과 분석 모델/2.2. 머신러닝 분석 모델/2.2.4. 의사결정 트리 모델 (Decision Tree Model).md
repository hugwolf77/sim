---
categories: 글쓰기
title: 2.2.4. 의사결정 트리 모델 (Decision Tree Model)
created: 2025-04-07
tags:
  - 교재
  - 수업
  - DecisionTree
---
---
#### *2.2.4. 의사결정 트리 모델 (Decision Tree Model)
---
### 1. 기본 개념

> 데이터의 특징(feature)들을 기준으로 **분류 (Classification)** 또는 **회귀 (Regression)** 예측을 수행하기 위해 데이터를 점진적으로 분할해 가는 모델

- 각 입력 feature 에 따른 종속 결과 값을 분리하는 과정을 모든 가능한 입력 feature의 조합에 따라 계층적 트리(tree) 구조로 분기를 만들어 가면서 분류 분석 또는 회귀분석을 진행하는 모델.
- 분기 이후 각 영역의 순도(homogeneity)가 증가하고 불순도(impurity)가 최대한 낮아지도록 학습. (Tree and Rule 구조)
- 분기 순간의 최선이라고 보이는 형태의 구조를 선택하기 때문에 탐욕적 알고리즘(greedy algorithm)이라고도 함.

1) **Node - 분기점** 
	: 특정 특징(feature)에 대한 테스트(조건)
	- **루트 노드 (Root Node):** 트리의 시작점으로, 전체 데이터셋을 포함.
	- **내부 노드 (Internal Node):** 특정 특징에 대한 테스트(조건)를 나타내며, 자식 노드를 가짐.
	- **리프 노드 (Leaf Node 또는 터미널 노드):** 더 이상 분기하지 않는 노드. 최종 예측 결과 (클래스 또는 값)를 담고 있다.

![[DecisionTreeModel.png]]

2) **가지 (Branch)** 
	: 노드에서 나오는 연결선으로, 테스트 결과에 따른 데이터의 분할 나타냄.

3) **특징 (Feature)**
	: 데이터를 설명하는 속성 또는 변수(입력되는 변수 항). 의사결정 트리는 이러한 특징들을 기준으로 분할.
	
4) **분할 규칙 (Splitting Rule)** 
	: 각 내부 노드에서 어떤 특징과 어떤 기준으로 데이터를 분할할지를 결정하는 규칙. 이 규칙은 불순도(impurity) 감소 또는 정보 이득(information gain)과 같은 기준을 사용하여 결정. 
		- 불순도 알고리즘을 통해 **Purity**(한 클래스만 존재 할수록 높음) 높아지도록 불순도(예: 엔트로피**entropy**, 지니 계수)를 가장 많이 감소시키는 특징과 분할 값을 찾아 분할. 
		- 데이터를 분할 할때 2개이상의 자식 노드로 분할 (보통 두개 노드로 분할)
		- 이러한 과정을 반복.
		- **분할 중단 조건:** 더 이상 불순도를 줄일 수 없거나, 특정 조건 (예: 노드에 속한 데이터 개수가 특정 값 이하)에 도달하면 분할을 멈추고 해당 노드를 리프 노드로 만듬.
5) **예측 (Prediction)**
	: 새로운 데이터가 주어지면, 루트 노드부터 시작하여 각 노드의 테스트 결과를 따라 트리를 순회하여 최종 리프 노드에 도달. 도달한 리프 노드에 해당하는 클래스가 예측 결과. (사다리 타기)

>[!Note] Regression Tree
>- 분류 트리와 유사하게 작동하지만, 리프 노드에는 특정 클래스 레이블 대신 해당 노드에 속한 데이터의 평균 또는 중간값과 같은 예측 값이 저장.
>- 분할 기준은 일반적으로 평균 제곱 오차(Mean Squared Error) 감소량.

6) **단점**
- **과적합 (Overfitting) 위험이 높다.** - 이를 방지하기 위해 가지치기(pruning) 등의 기법이 사용.
- **데이터에 민감함:** 작은 데이터 변화에도 트리의 구조가 크게 달라질 수 있다.
- **불안정성:** 최적의 트리를 찾는 것이 NP-완전 문제이므로, 탐욕적인(greedy) 방식으로 트리를 구성하여 항상 최적의 해를 보장하지 못할 수 있다.
- **선형 관계 표현의 어려움:** 선형적인 관계를 표현하는 데는 비효율적일 수 있다.

6) **장점**
	- 트리구조 분기로 의사결정 과정을 직관적으로 이해하기 쉽다.
	- 어떤 특징 조건에 따라 의사결정 했는지 파악이 쉽다. (해당 모델의 결정 조건 중요도 파악이 쉬움)
	- 정량(수치형), 정성(카테고리형) 데이터 타입에 구애받지 않고 적용할 수 있다.
	- 데이터 정규화를 위한 scaling에 크게 영향 받지 않는 것으로 알려져 있다.
	- 선형 분리기와 다르게 복접한 결정경계를 만들수 있다. (예: XOR 문제 해결)
	- 일부 알고리즘은 결측치를 더미(dummy)화하여 분할 규칙을 만드는 것을 지원한다.

### 2. 불순도 알고리즘

1. **Gini index**
$$ Gini\ \ Index(A) =  1 - \sum_{i=1}^{C}P_{i}^{2} \ (A=사전\ 경우\ 수, C=발생\ 경우\ 수\ )$$ 
2. **Entropy index**
 $$ Entropy\ Index(A) =  - \sum_{i=1}^{C}P_{i}log_{2}(P_{i}) \ (A=사전\ 경우\ 수, C=발생\ 경우\ 수\ )$$

3. **정보 획득 Information gain**

- 분기 이전의 불순도 계산과 분기 이후의 불순도의 차이를 정보 획득이라 표현
- 만약 Root Node 에서  불순도가 1 이 였다면 분기를 통해서 0.8인 상태로 바뀌었다면 정보를 0.2 의 정보를 획득한 것이다.
- 각 feature 에 따라서 Information gain을 계산하고 가장 순도가 높아지는 feature를 기준으로  분기 => 이를 하위 노드에서 더 이상 분기 하지 못할때 까지 반복

4. 가지치기 pruning
- Leaf Node 가 모두 최대 순도가 되게 분기하면 Full Tree가 된다. 이런 경우 각 feature 마다가 모두 반영되어 Overfitting이 발생하여 일반화된 예측이나 분류가 어렵다.
- 특정 분기 이하의 Tree 아래의 Node 분기를 진행하지 않게 하는 것 (자르는 것)
- 파라미터로 최대 깊이, 최대 leaf, Node 분할을 위한 초소 경우의 샘플 수 등을 정한다.
- 여러 분기 중 모델 적합도를 비용함수로 계산하여 모델을 선정한다.

 Err : validataion 에서의 오분류 또는 예측 오류 정도
 L : 모델의 복잡도, leaf node 의 수 등으로 모델의 크기와 복잡도
 $\alpha$ : Err 와 L의 결합 가중치 
 $$비용함수 = Err + \alpha \times L$$

> [!Note] 주요 의사결정 트리 알고리즘
> - **ID3 (Iterative Dichotomiser 3):** 범주형 특징 기반, 정보 이득(Information Gain) 사용.
> - **C4.5:** ID3 개선, 수치형 특징 처리, 결측치 처리, 가지치기 지원, 정보 이득률(Information Gain Ratio) 사용. 다지 분리
> - **CART (Classification and Regression Tree):** 분류 및 회귀 모두 가능, 지니 계수(Gini Impurity) (분류) 또는 평균 제곱 오차 감소량 (회귀) 사용, 가지치기 지원. 2지 분리
> - **CHAID** : 범주형만 가능 chi-square 사용 다지 분리. 최적모형 개발. 가지치기 암됨.

---
### 3. 앙상블과 의사결정트리 모델

#### 1) 랜덤 포레스트
- 단일 의사결정 트리의 단점인 과적합(Overfitting) 문제를 해결하고, 예측의 안정성과 정확성을 높이기 위해 앙상블 중  **배깅(Bagging)** 기법 사용.
- **특징 무작위 선택(Feature Randomness)** 을 결합하여 의사결정 트리를 구성.
- **여러 개의 의사결정 트리(Decision Tree)를 학습**시키고 이들의 예측 결과를 **집계(Aggregating)** 하여 최종 예측을 수행하는 모델. 
- "랜덤"하게 만들어진 "숲"을 활용하여 예측을 수행.

	1. **부트스트래핑 (Bootstraping)** 
		: 원본 학습 데이터셋에서 **중복을 허용하여** 여러 개의 (일반적으로 원본 데이터 크기와 동일한) 부트스트랩 샘플을 추출. 각 부트스트랩 샘플은 서로 다른 데이터를 포함.
    2. **개별 트리 학습** 
	    : 각 부트스트랩 샘플을 사용하여 **독립적인 의사결정 트리**를 학습. 
	    - 일반적인 의사결정 트리와 달리 **각 분기(node)에서 최적의 특징을 선택할 때, 모든 특징을 고려하는 것이 아니라 무작위로 선택된 일부 특징 중에서만** 최적의 특징을 찾음. 
	    - 이러한 특징 무작위 선택 과정은 생성된 트리들 간의 상관성을 줄이는 기능을 함.
    3. **예측** 
	    - **(분류(카테고리) 데이터)**: 새로운 데이터가 입력되면, 학습된 모든 의사결정 트리가 각자 예측 수행. 최종 예측은 이들의 예측 결과를 **다수결 투표(Hard Voting)** 방식으로 결정. 
		- **(회귀(연속형) 데이터)**: 새로운 데이터가 입력되면, 학습된 모든 의사결정 트리가 각자 예측 값을 출력. 최종 예측은 이들의 예측 값들을 **평균**내거나 **중앙값**을 취하여 결정 **(Soft Voting)**.
    4. 장점
		- **높은 정확도:** 여러 개의 약한 학습기(의사결정 트리)를 결합하여 강력한 예측 성능.
		- **과적합 방지:** 배깅과 특징 무작위 선택을 통해 모델의 분산(Variance)을 줄여 과적합 위험을 효과적으로 낮춤.
		- **특징 중요도 평가:** 어떤 특징이 예측에 중요한 영향을 미치는지 상대적인 중요도를 평가할 수 있음.
		- **결측치 처리 및 이상치에 강건함:** 일부 결측치를 가진 데이터에도 비교적 잘 작동하며, 이상치의 영향이 단일 결정 트리보다 적음.
		- **수치형 및 범주형 데이터 모두 처리 가능:** 다양한 형태의 데이터를 다룰 수 있음.
		- **특징 스케일링 불필요:** 데이터 정규화 scaling 영향을 적게 받음.
		- **병렬 처리 용이:** 각 트리를 독립적으로 학습시킬 수 있어 병렬 처리에 용이. 대규모 데이터셋에서도 비교적 빠르게 학습.
	5. 단점
		- **모델 해석의 어려움:** 많은 수의 트리를 분석하여 전체적인 의사결정 과정을 이해하기 어려우며 단일 처럼 직관적이지 않다.
		- **학습 시간:** 트리의 개수가 많아지면 학습 시간이 길어 진다.
		- **일부 데이터셋에서 성능 저하 가능성:** 선형 모델이나 매우 규칙적인 구조의 데이터를 가진 경우 오히려 다른 기법보다 성능이 떨어 질 수 있다.

	6. 하이퍼파라미터
		- **`n_estimators`:** 생성할 의사결정 트리의 개수. 
		- **`max_features`:** 각 노드에서 분할을 위해 고려할 특징의 최대 개수. 'sqrt', 'log2' 등의 옵션을 통해 무작위성을 조절.
		- **`max_depth`:** 각 트리의 최대 깊이를 제한하여 과적합을 방지.
		- **`min_samples_split`:** 노드를 분할하기 위한 최소한의 샘플 수.
		- **`min_samples_leaf`:** 리프 노드가 가져야 하는 최소한의 샘플 수.
		- **`bootstrap`:** 부트스트랩 샘플링 사용 여부.

[scikit-learn Randomforest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
[scikit-learn Randomforest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)

https://scikit-learn.org/stable/modules/ensemble.html

```python
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

# load data & visualize
iris = datasets.load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df["species"] = np.array([iris.target_names[i] for i in iris.target])
sns.pairplot(df, hue="species")

# data split
X_train, X_test, y_train, y_test = train_test_split(
											    df[iris.feature_names],
											    iris.target,
											    test_size=0.25,
											    stratify=iris.target,
											    random_state=123456,
											)
# model set and fit
rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)
rf.fit(X_train, y_train)

# test and accuracy
predicted = rf.predict(X_test)
accuracy = accuracy_score(y_test, predicted)

print(f"Out-of-bag score estimate: {rf.oob_score_:.3}")
print(f"Mean accuracy score: {accuracy:.3}")

# confusion matrix
cm = pd.DataFrame(
    confusion_matrix(y_test, predicted),
    columns=iris.target_names,
    index=iris.target_names,
)
sns.heatmap(cm, annot=True)
```


### 2) Boost 기반 의사결정 트리 모델


![[Boost_DecisionTree.png]]

- **부스팅(Boosting) 과정** 
	1) bootstrap 으로 여러 하위샘플을 만든다.
	2) 첫번째 모델로 하위샘플-1을 분류한다. 
	3) 하위샘플1의 분류하지 못한 샘플은 하위샘플-2로 가중치와 함게 보낸다.
	4) 하위샘플-2에서 하위햄플1에 없는 샘플에도 가중치를 준다.
	5) 다음 모델에 가중된 샘플을 넣고 분류한다.

	- 장점 : 정확도를 최대한 높일 수 있다.
	- 단점 : 이상치(outlier) 에 취약하다.

- **[AdaBoost (Adaptive Boosting):](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html)**
    - 각각의 약한 학습기(weak learner)를 순차적으로 학습시키면서, 이전 모델이 잘못 분류한 데이터에 더 큰 가중치를 부여하여 다음 모델이 해당 오류를 개선하도록 학습.(일반 boosting은 개별 모델에 동일한 가중치, adaBoost는 개별적으로 부여)
    - 각각의 학습된 모델에게 성능에 따라 가중치를 부여하여 최종 예측을 결합.
    - 비교적 간단하며 이해하기 쉬운 부스팅 알고리즘.

- **[GBM (Gradient Boosting Machine):](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)**
    - AdaBoost와 유사하게 순차적으로 모델을 학습하지만, 가중치 업데이트 대신 **잔차(residual error)** 를 학습하는 방식으로 작동.
    - 각 단계에서 이전 모델이 예측한 값과 실제 값의 차이(잔차)를 새로운 약한 학습기가 예측하도록 학습.
    - 손실 함수를 정의하고, 경사 하강법(Gradient Descent)을 사용하여 모델을 최적화.
    - AdaBoost보다 더 강력한 성능을 보이는 경우가 많지만, 과적합 위험이 있을 수 있다.

- **[XGBoost (eXtreme Gradient Boosting):](https://xgboost.readthedocs.io/en/release_3.0.0/#)**
    - GBM의 성능과 속도를 개선하기 위해 개발된 알고리즘.
    - 병렬 처리, 정규화(Regularization) 기법, 결측치 처리 등 다양한 기능을 제공하여 과적합을 방지하고 뛰어난 성능을 보임.
    - 대규모 데이터셋에서도 빠른 학습 속도를 자랑하며, 다양한 머신러닝 경진대회에서 높은 성적을 거두고 있음.

- **[LightGBM (Light Gradient Boosting Machine):](https://lightgbm.readthedocs.io/en/latest/index.html)**
    - XGBoost와 마찬가지로 GBM 기반의 부스팅 알고리즘이지만, 학습 속도와 메모리 사용량을 더욱 최적화.
    - **Leaf-wise (Leaf-wise) 트리 성장 방식**을 사용하여 균형 트리 분할 방식보다 더 빠르게 손실을 줄이고 높은 정확도를 달성.
    - 범주형 특징을 효율적으로 처리하는 기능 등을 제공.


- **[Catboost(Categorical Boosting)](https://catboost.ai/)**
	- 범주형(categorical) 변수를 처리하는 데 유용한 알고리즘.
	- 비교적 최근에 나옴.
