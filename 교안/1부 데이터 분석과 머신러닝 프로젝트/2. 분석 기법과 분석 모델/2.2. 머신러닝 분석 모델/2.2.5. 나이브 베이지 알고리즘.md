---
categories: 글쓰기
title: 2.2.4. 나이브 베이지 알고리즘
created: 2024-11-14
tags:
  - 수업
  - 교재
  - NaiveBayesAlgo
---
---
#### *나이브 베이지 알고리즘*
---


---
### *3. 나이브 베이지안 알고리즘 (Naive Bayes Algorithm) 
---

##### 1)  확률(Probability)과 조건부확률(Conditional Probility)
---
- **확률(Probability)** : 어떠한 모집단(또는 현상)에서 모든 일어날 수 있는 샘플(사건-event들)의 경우의 수 (해당 모집단 데이터에서 발생할 수 있는 카테고리의 종류) 가운데 특정 샘플(사건-event)이 일어날 가능성을 뜻함.
 $$
\begin{align}
	&P(A)\ = \frac{n(A)}{n(S)}
\end{align}
$$

- 일반적으로 모집단 또는 특정 현상에서 발생할 모든 가능한 경우의 합의 확률로 1(100%)로 나타내며, 반대로 전혀 일어나지 않는 확률을 0(0%) 나타냄.
- 이와 연계해서 우리가 Logistic function, SoftMax function 에서 0~1 사이의 공간으로 모든 결과 값을 표현하는 이유를 알 수 있다.
- 이때 모든 경우의 수(총집합 S)라는 가정에 대하여 카테고리를 나눌 수 없는 경우를 여집합( $A^{c}$)으로 표현할 수 있다는 것을 염두해 두어야 함. 즉, 우리가 어떠한 현상에 대하여 모든 경우에 수를 알 수 없어도 표현상 카테고리를 분류 할 수 있다는 뜻이다. 더 나아가 이는 데이터 분석에서 결측값(missing data)을 더미(dummy) 데이터를 사용하여 구분하여 측정하고자 하는 데이터에 대한 여집합으로 표현하는 것과 연관해서 생각해 볼 수 있음.
 $$
\begin{align}
	&A^{c} = n(S)\ - n(A) 
\end{align}
$$

- 통계-분포와 연관성에 대해서 다시 생각해 볼 수 있음. 결국 샘플 데이터로부터 모집단의 특성값을 추정하는 통계는 그 분포를 통해서 확률 정보를 구할 수 있음.
- **조건부 확률(Conditional Probility)** : 특정한 샘플(사건-event)이 발생한 경우가 다른 샘플(사건-event)의 발생했다는 전제 하에 발생할 가능성.
$$
\begin{align}
	P(A|B)\ &=\ \frac{P(A\cap B)}{P(B)}\\ \\ 
	&=\ \frac{\frac{n(A\cap B)}{n(S)}}{\frac{n(B)}{n(S)}}\ =\ \frac{n(A \cap B)}{n(B)}
\end{align}
$$
![[conditional_Prob.png|500]]

##### **2) 베이즈 정리(Baye's theorem)와 우도(Likelihood)**
---
- **베이즈 정리(Baye's theorem)** : 어떠한 사건이 서로 **독립**으로 발생할 때 이전의 경험적 확률과 현재 확인한 정보를 바탕으로 사후 조건부 확률을 추정할 수 있기 때문.
- 즉 두 확률 변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 정리. 즉, 역확률(inverse probability)문제를 구하기 위한 방법.
$$
\begin{align}
	&P(B|A)\ =\ \frac{P(A|B)P(B)}{P(A)}\ \propto\ P(B|A)\times\ P(B)\ =\ \frac{P(A \cap B)}{P(A)} \\ \\ 
	&P(A) : A의\ 사전 확률(evidence) : 일반적인\ A\ 사건이\ 발생할\ 확률로\ 일종의 관찰값에\ 대한\ 사전확률 \\
	&P(B) : B의\ 사전 확률(prior\ probability) : 일반적인\ B\ 사건이\ 발생할\ 확률 \\
	&P(A|B) :사건\ B가\ 주어졌을\ 때 \ A의 조건부 확률(likelihood) : 관찰\ 또는\ 수집,\ 실험에\ 의해\ 수집된\ 정보 \\
	&P(B|A) :사건\ A라는\ 증거에\ 대한\ 사후 확률(posterior\ probability) : 관찰\ 또는\ 수집,\ 실험에\ 의해\ 수집된\ 정보 \\
\end{align}
$$

![[Baye's_Theorem.png|500]]
$$
\begin{align}
	&P(\theta|data)\ =\ \frac{P(data|\theta)P(\theta)}{P(data)} \\ \\
	&P(\theta|data,M)\ =\ \frac{P(data|\theta,M)P(\theta,M)}{\int(P(data|\theta,M)\times P(\theta|M))d\theta} \\ \\
\end{align}
$$
- 일반적으로 evidence(=Marginal Likelihood:주변우도) 다시  말해 관찰 값의 P(data) (=데이터가 관찰될 전반적인 확률)는 가능한 모든 우도(이산이면 모든 클라스, 연속형이면 가능한 모든 분포범위)에 대해서 각각에 사전확률을 곱하여 합산(적분)

>- 우리는 $P(data|\theta)$를 알고 있고(데이터로 부터 관찰됨) 따라서 $P(data|-\theta)$ 즉, 여집합(또는 모든 관찰 결과의 확률의 가능도와 그 여집합)으로 알 수 있다.
>- 이를 통해 전체 data에 대한 가능도(likelihood)의 총합을 구할 수 있고, 가능도의 총합이 P(data)가 된다. (이는 이산 독립을 전제 한다)

- 예시
> COVID-9의 발병률이 10%로 알려져 있음. 
> COVID-9에 실제로 걸렸을 때 검진 확률은 99% 임. 
> 실제로 걸리지 않았을 때 오검진될 확률이 1% 이라고 알려져 있음.

어떤 사람이 질병이 감염된 결과가 나왔을 때, 진짜 COVID-9 감염되었을 확률은?
	D: 검진
	발병 : $P(\theta)$ = 0.1, 발병일 때 검진 확률 $P(D|\theta)$ =0.99, 발병일 때 검진 안될 확률 $P(D|- \theta)$ =0.01 
	이때 $P(D)$ 는 $\sum_{\theta} P(D|\theta)P(\theta)=0.99\times 0.1 + 0.01\times 0.9 = 0.108$ 
	.
	이를 통해서 $P(\theta|D) = 0.1 \times \frac{0.99}{0.108} \approx 0.916$ 이 된다.

>[!NOTE]
> 분류(classification) 문제와 연계시켜 이해하면
> 사전확률, 민감도(재현율Recall), 오탐율(1종-오류)를 가지고 새로운 정밀도(Precision)을 구하는 문제임
![[bayesPbr_ConfusionM.png]]

- 조건부 확률에서 발생하는 인식의 오류를 조심해야 함.  쉬운 예로 교통사고 안전벨트 예가 있다. 교통사고로 사망한 사람의 30%가 안전벨트를 메지 않았다는 확률을 이라고 하면, 이를 잘못 오해하면 70%는 안전벨트를 메고도 사망한 것이 된다. 이는 전체 운전자 중 안전벨트를 하는 확률이라는 경우가 빠져 있는 것이다. 전제 사건을 기본 전체 확률로 가정하는 오류이다. 경우의 정보가 부족한 문제이다. 즉, 정확한 확률을 구하기 위해서는 안전벨트를 메는 일반적인 확률(안전벨트 사전확률)과  교통사고에 대한 확률(사망사고의 사전확률)에 대한 정보가 필요하다.
	[위키피디아 예시](https://namu.wiki/w/%EC%A1%B0%EA%B1%B4%EB%B6%80%ED%99%95%EB%A5%A0)
- 이러한 문제로 "몬티 홀 문제", "검사의 오류prosecutor's fallacy)"  등이 있다. 
- 이러한 조건부 확률 함정을 조심해야 하는 이유는 "베이즈 정리"는 전통적인 확률 추정에서는 모집단을 변하지 않는 현상값으로 규정하지만, "베이즈 관점"에서는 모집단을 미리 확정 짓지 않고 현재의 증거의 우도(likelihood)에 의해 규정하기기 때문에 오류에 빠지기 쉽기 때문이다. 

>[!Note]
>- **왜 로그를 사용할까요?**
>- **확률의 곱셈을 덧셈으로 변환:** 여러 사건이 동시에 발생할 확률은 개별 사건의 확률을 곱해야 합니다. 로그를 취하면 곱셈이 덧셈으로 바뀌어 계산이 간편해집니다.
>- **작은 값을 다루기 쉽게:** 확률은 0과 1 사이의 값을 가지므로, 매우 작은 값을 다룰 때 로그를 취하면 상대적으로 큰 값으로 변환되어 계산 오차를 줄일 수 있습니다.
>- **정보량의 가산성:** 서로 독립적인 두 사건에 대한 정보량은 각각의 정보량을 더한 것과 같습니다. 로그를 사용하면 이러한 가산성을 만족시킬 수 있습니다.


##### 3) Naive Bayes Classifier 
---

 - 수학적 정의를  데이터에 대한 모델로 적용한다면
$$
\begin{align}
	&P(y|X)\ =\ \frac{P(X|y)\cdot P(y)}{P(X)} \\
	& X\ =\ (x_{1},x_{2},x_{3},\cdots,x_{n})
\end{align}
$$

 - y는 분류할 class 이다. X 는 입력 변수 (feature) 조건이 되고,  $p(y|X)$ y class가 X feature 에 대해서 발생할 확률이 된다.
$$
\begin{align}
	&P(y|x_{1},\cdots,x_{n})\ =\ \frac{P(x_{1}|y)P(x_{2}|y)\cdots P(x_{n}|y)\cdot P(y)}  {P(x_{1})P(x_{2})\cdots P(x_{n})} \\ \\
	& P(A,B) = P(A)P(B) \quad iid, A,B
\end{align}
$$
-  x 에 대한 일반화하면
$$
\begin{align}
	&P(y|x_{1},\cdots,x_{n})\ =\ \frac{P(y)\Pi_{i=1}^{n} P(x_{n}|y)}  {P(x_{1})P(x_{2})\cdots P(x_{n})} \\ \\
	& P(A,B) = P(A)P(B) \quad iid, A,B
\end{align}
$$
- x 데이터에 대한 관찰 값에 모든 확률에 대한 일반화로 보면
$$
\begin{align}
	& P(y|x_{1},\cdots,x_{n})\ \propto \ P(y)\Pi_{i=1}^{n} P(x_{n}|y)
\end{align}
$$
- 결국 예측하고자 하는  y class에 대해서 확률로

$$
\begin{align}
	& P(y|x_{1},\cdots,x_{n})\ argmax_{y} \ P(y)\Pi_{i=1}^{n} P(x_{n}|y) \\ \\
	& y =\ argmax_{y} \ P(y)\Pi_{i=1}^{n} P(x_{n}|y)
\end{align}
$$

- ex 01 : Weather & Play Dataset [example reference](https://www.javatpoint.com/machine-learning-naive-bayes-classifier)

| no  | Outlook  | Play |
| --- | -------- | ---- |
| 0   | Rainy    | Yes  |
| 1   | Sunny    | Yes  |
| 2   | Overcast | Yes  |
| 3   | Overcast | Yes  |
| 4   | Sunny    | No   |
| 5   | Rainy    | Yes  |
| 6   | Sunny    | Yes  |
| 7   | Overcast | Yes  |
| 8   | Rainy    | No   |
| 9   | Sunny    | No   |
| 10  | Sunny    | Yes  |
| 11  | Rainy    | No   |
| 12  | Overcast | Yes  |
| 13  | Overcast | Yes  |


- Frequency Table

| Weather  | Yes | No  |
| -------- | --- | --- |
| Overcast | 5   | 0   |
| Rainy    | 2   | 2   |
| Sunny    | 3   | 2   |
| Total    | 10  | 4   |

- Likelihood table 

| Weather  | Yes        | No        |           |
| -------- | ---------- | --------- | --------- |
| Overcast | 5          | 0         | 5/14=0.35 |
| Rainy    | 2          | 2         | 4/14=0.29 |
| Sunny    | 3          | 2         | 5/14=0.35 |
| All      | 10/14=0.71 | 4/14=0.29 |           |
|          |            |           |           |
P(Yes|Sunny) = P(Sunny|Yes)*P(Yes) / P(Sunny)
             = ( ( 3/10 ) x 0.71  ) / 0.35 = 0.60

P(No|Sunny) = P(Sunny|No)P(No) / P(Sunny)
             = ( ( 2/4 ) x 0.29  ) / 0.35 = 0.41

- anather example
![[NB_simple_example.png]]


- ex 01 : Spam Mail  [example reference](https://medium.com/analytics-vidhya/na%C3%AFve-bayes-algorithm-5bf31e9032a2)
	+ Not Spam mail : 15
	+ Spam mail : 10
	+ below table show the frequency of each word had been recorded

| word       | Not Spam | Spam |
| ---------- | -------- | ---- |
| Dear       | 8        | 3    |
| Visit      | 2        | 6    |
| Invitation | 5        | 2    |
| Link       | 2        | 7    |
| Friend     | 6        | 1    |
| Hello      | 5        | 4    |
| Discount   | 0        | 8    |
| Money      | 1        | 7    |
| Click      | 2        | 9    |
| Dinner     | 3        | 0    |
| Total      | 34       | 47   |
- P(Dear|Not Spam) = 8/34
- P(Visit|Not Spam) = 2/34
- P(Dear|Spam) = 3/47
- P(Visit|Spam) = 6/47

- "Hello friend" ??
- "Hello friend" => "Hello" , "friend"  
	: Naive assume => *the features we use to predict the target are independent*

- P("Hello friend"|Not Spam) = P("Hello"|Not Spam) x P("friend"|Not Spam) 

- P(Not Spam|"Hello friend") = P("Hello"|Not Spam) x P("friend"|Not Spam) x P(Not Spam)
						 = (5/34 x 6/34) x 15/25 = 0.0155
- P(Spam|"Hello friend") = (4/47 x 1/47) x 10/25 = 0.00072 

##### Laplace smoothing

**Zero-Frequency Problem** : feature does not appear in the dataset, so that P is zero, hence all other probabilities will have no effect

- technique for smoothing categorical data

$$
\begin{align}
	&\hat\theta\ =\ \frac{x_{i}+\alpha}{N +\alpha d} \quad (i=1,\cdots,d) \\ \\
	& \alpha : smoothing parameter \\
	& x = (x_{1},\cdots,x_{d}): observation count \\
	& N : multinominal\ distribution\ trial\ count
	
\end{align}
$$
- Not smoothing :
	 P(Spam|"dear visit dinner money money money")
			= P("dear visit dinner money money money"|Spam) x P(Spam)
			= ( 3/47 x 6/47 x 0 x $(7/47)^{3}$ ) x 10/25 = 0 
- Laplace smoothing :
	 P(Spam|"dear visit dinner money money money")
			= P("dear visit dinner money money money"|Spam) x P(Spam)
			= ( $\frac{3+1}{47+10}$ x $\frac{6+1}{47+10}$ x $\frac{0+1}{47+10}$ x $(\frac{7+1}{47+10})^{3}$ ) x 10/25 = 4.18 x $10^{-7}$  = 1.672 x $10^{-7}$  


https://scikit-learn.org/stable/modules/naive_bayes.html
https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html


1) CategoricaNB - 범주형
2) MultinomialNB - 빈도수
3) GaussianNB - 연속형


---
#### 조건부 엔트로피 (Conditional Entropy)

- [정보 엔트로피와 연계해서 생각해보자] : 조건부 엔트로피의 총합 => 그러나 만약 균등분포(독립적인 발생)의 확률이라면 각각의 확률 엔트로피의 합으로 계산 가능. 모든 확률의 동시 발생확률

$$
\begin{align}
	& P_{i}\ =\ 1/n \\ \\
	&S\ =\ -\sum_{i=1}^{n}{P_{i}lnP_{i}} = ln\ n 
\end{align}
$$


- 조건부 엔트로피 $H(Y∣X)$는 확률 변수 X가 주어진 조건 하에서 확률 변수 Y의 불확실성을 측정.
- X의 각 값 x에 대해 Y의 엔트로피 $H(Y∣X=x)$를 계산하고, X의 확률 분포에 따라 가중 평균하여 얻습니다. 
$$H(Y∣X)=\sum_{x}​p(x)H(Y∣X=x)=−\sum_{x}\sum_{y}​p(x,y)log_{2}​(p(y∣x))$$

**조건부 엔트로피의 총합 (Chain Rule for Entropy):**

- 결합 엔트로피 $H(X,Y)$는 두 확률 변수 X와 Y의 동시 불확실성을 측정.
- 결합 엔트로피는 조건부 엔트로피를 이용하여 다음과 같이 표현할 수 있다. $$H(X,Y)=H(X)+H(Y∣X)=H(Y)+H(X∣Y)$$
- 이를 확장하면 여러 확률 변수에 대해서도 마찬가지로 적용. 즉, 여러 사건의 동시 발생에 대한 불확실성은 하나의 사건의 불확실성에 나머지 사건들이 주어졌을 때의 조건부 불확실성을 더한 값으로 나타낼 수 있다.

**균등 분포 (Uniform Distribution)의 확률이라면 각각의 확률 엔트로피의 합으로 계산 "가능" (정확히는 독립적인 사건의 경우):**

- **핵심은 "균등 분포"가 아니라 "독립적인 발생"입니다.** 두 사건 X와 Y가 통계적으로 독립이라면, $P(Y∣X)=P(Y)$이고 $P(X∣Y)=P(X)$가 성립합니다.
- 이 경우, 조건부 엔트로피는 각각의 엔트로피와 같아 진다. 
$$
\begin{align}
 H(Y∣X)&=−\sum_{x}​\sum_{y}​p(x,y)log_{2}​(p(y∣x))\\ &=−\sum_{x}\sum_{y}​p(x)p(y)log_{2}​(p(y))\\ &=−\sum_{y}​p(y)log_{2}​(p(y))\sum_{x}​p(x)\\ &=H(Y)⋅1=H(Y)
 \end{align}
 $$ 마찬가지로, $H(X∣Y)=H(X)$가 된다.
- 따라서, 독립적인 사건의 결합 엔트로피는 각각의 엔트로피의 합으로 계산된다. $$H(X,Y)=H(X)+H(Y∣X)=H(X)+H(Y)$$
- **균등 분포는 독립성을 보장하지 않는다.** 예를 들어, 두 개의 주사위를 던지는 사건은 각각 균등 분포를 따르지만 독립적인 사건이다. 반대로, 특정 합을 가지는 두 숫자를 뽑는 사건은 각각 균등 분포를 따르지 않을 수 있지만 종속적인 사건이다.

**5. 모든 확률의 동시 발생 확률:**

- 결합 엔트로피 $H(X,Y)$는 확률 변수 X와 Y의 모든 가능한 동시 발생 확률 $p(x,y)$를 고려하여 계산된다. $$H(X,Y)=−\sum_{x}\sum_{y}​p(x,y)log_{2}​(p(x,y))$$
- 이는 각각의 확률 변수의 개별적인 발생 확률뿐만 아니라, 그들이 동시에 어떻게 발생하는지에 대한 정보를 포함한다. 독립적인 경우에만 $p(x,y)=p(x)p(y)$가 성립하여 결합 엔트로피가 개별 엔트로피의 합으로 간단하게 계산될 수 있는 것이다.

**결론적으로, 해당 문장은 다음과 같은 의미를 내포한다.**

- 일반적으로 여러 확률 변수의 동시 불확실성(결합 엔트로피)은 하나의 변수의 불확실성에 나머지 변수들이 주어졌을 때의 조건부 불확실성을 순차적으로 더하여 계산된다.
- 하지만 특별한 경우로, 여러 사건이 서로 독립적이라면 (각 사건의 발생이 다른 사건의 발생에 영향을 미치지 않는다면), 결합 엔트로피는 각 사건의 개별적인 엔트로피의 합으로 간단하게 계산할 수 있다. 이때 각 사건의 확률 분포가 반드시 균등 분포일 필요는 없다.
- 결합 엔트로피는 모든 가능한 사건들의 동시 발생 확률을 고려하여 불확실성을 측정합니다.

이러한 이해는 정보 이론에서 여러 확률 변수 간의 관계와 정보량을 분석하는 데 중요한 기초가 됩니다.