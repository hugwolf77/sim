---
categories: 글쓰기
title: 2.2.6. 군집화
created: 2025-04-21
tags:
  - 교재
  - 수업
---
---
#### *2.2.6. 군집분석*
---
### Label 이 없다? 그렇다면 끼리끼리 묶어 보자.

- 분류와의 차이점은 무엇일까? 정해져 있는 카테고리가 없다. (_비지도 학습_)
>[!NOTE] 
>- 분석의 목적에 따라 기준이 달라 질 수 있다.
>- 분석의 대상 또는 분석하는 현상을 바라보는 관점에 따라서 기준이 달라 질 수 있다.
>

- 새로운 상위(광의)나  하위(협의) 또는 새로운 차원의  의미를 부여하여 분류의 기준을 만드는 것
- 데이터 내에 숨어있는 별도의 그룹을 찾아서 의미를 부여

### 유사도 (similarity)

- 군집화 clustering과 분류 classification의 기반
- 가장 쉽게 생각하면 기하학적으로 얼마나 떨어져 있냐를 계산하는 것
1) Euclidean Distance (L2) :
	$$
	D(p,q)\ = \ \sqrt{\sum_{i=1}^{n}(q_{i}-p_{i})^{2}} \\
	
	$$    
2) Manhattan Distance (L1) : 
	$$D(p,q)\ = \ \sum_{i=1}^{n}|q_{i}-p_{i}|$$
3) Minkowski Distance : 
	$$D(p,q)\ = \ (\sum_{i=1}^{n}|q_{i}-p_{i}|^{p})^{1/p}$$
		: L1, L2 를 일반화한 것 p 에 따라서 달라짐 $p=\infty$ 면 Chebyshev Distance 와 동일
		
4) Cosine Similarity :  
	 $$cos(\theta) \ = \ \frac{A \cdot B}{||A||\ ||B||}$$
		: 두 벡터가 이루는 각도를 통해 유사도를 측정. 각이 작아질수록 1 각이 클수록 -1

5) Pearson Similarity : 
	$$pearson(p,q)\ = \  \frac{\sum_{i=1}^{n}(p_{i}-\mu_{p})\cdot(q_{i}-\mu_{q})}{\sqrt{\sum_{i=1}^{n}(p_{i}-\mu_{p})^{2}}\cdot\sqrt{\sum_{i=1}^{n}(q_{i}-\mu_{q})^{2}}}$$
		: 상관계수로 쓰임. 공분산 즉, 변화량의 방향성의 유사성으로 판단.

6) Mahalanobis Dstance: 
		: 확률분포를 고려하여 공분산을 이용한 두 분포간의 거리 계산
	$$
	D_{M}\ = \ \sqrt{(q_{i}-p_{i})^{r}S^{-1}(q_{i}-p_{i})} \\
	
	$$
	
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FAZUyH%2FbtsIFpCLtd7%2FWaXjsPKeMCuj1bpka5jo5k%2Fimg.png" width=300>

 7) KL(Kullback-Leibler)-divergence :
$$
	D_{KL}\ = \ (-\sum_{i=i}^{n}P(p_{i})logP(q_{i})) - (-\sum_{i=i}^{n}P(p_{i})logP(p_{i})) 
$$
		: 두 분포의 정보 Entropy 차이를 이용한 분포의 거리를 계산하는 방법, p에 대한 확률분포에 대하여 q의 확률분포의 정보 엔트로피 변화를 구하는 것. 배대칭(비교 방향에 따라 다름)의 비교 특성이 있다.

###### (유사성과 관련 되어 최근 많이 연구되는 AI 방법) Contrastive Learning :
- self-supervised representation learning 일종으로 가장 유명함.
- 유사한 데이터는 더 낮은 공간(차원)에서 서로 가깝게 
- 동시에 상이한 데이터는 서로 더욱 멀어지도록 낮은 공간(차원)에
- 새롭게 데이터를 표현해 내는 것
- Self supervised representation learning
![[self_supervised_representation_learning.png]]
[SimCLRv2 :Ting Chen et al, "Big Self-Supervised Models are Strong Semi-Supervised Learners"](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/2006.10029.pdf)

###### NT-Xent (Normalized Temperature-Scaled Cross Entropy Loss) : 
Contrastive Loss 
: 벡터 사이의 유사성을 측정하는 방법 Positive loss 와 negative loss 를 합쳐서 positive는 가깝게 negative는 Embedding 을 표현하는 손실 함수

[[Contrastive Loss와 Triplet Loss의 차이점]]

### 차원의 저주 (Curse of Dimensionality)

- 하나의 대상, 현상을 표현하는 (또는 측정하는) feature의 개수가 과도한 상태
- 목적에 맞게 또는 파악할 수 있는 정보를 벗어난 상태

- 머신러닝의 학습 : 학습 데이터의 차원의 많아지면 그만큼 학습에 필요한 데이터도 증가
	(반대로 말하면 학습데이터 량에 비하여 데이터의 차원이 많아지면 모델 성능저하)
- 데이터의 표현 : 벡터의 차원이 늘어나면 오히려 유사성 측정이 어려워짐. 즉, 구분이 모호해짐.
- 분석 모델의 성능: feature의 수가 너무 많으면 **모델의 성능 저하**, **overfit**이 될 가능성이 커짐.

### 차원 감소 또는 차원 축소(dimensionality reduction)

- 차원의 저주를 막기 위한 방법은 저 차원으로 차원을 줄이는 것이 일반적임
- 이는 feature의 감소를 의미하고 이때 더 추상적이고 함의적인 feature로 포함시켜 가는 것.

> [!Note] feature와 데이터의 차원
>- 의미 없는(설명력이 떨어지는) feature 를 약화시키거나 제외함 이를 가지치기 Prunning 이라가고 한다.
>- Rigde Regression, Rasso Regression 등 ... 이는 다시 L1, L2 로 feature의 설명력을 평가한다. 영향력의 크기 만큼 잘못된 추론에 패널티를 가한다.
>- 이때 feature를 회귀식에 입력시키는 순서와 방법도 중요한 결정 요인이 된다.
>

- _의미론적인 이해가 중요함_  측정된 데이터의 값들을 새로운 하나의 값으로 만들거나 특정 feature에 수식으로 연결 시켜야 함. 이때 기존 측정된 feature의 의미가 새로이 표현된 feature에 함의 되었는지 또는 대표하는지 분석자가 파악하고 있어야함.  

	**EDA** 측면에서 데이터에 내재되어 있는 정보를 찾는 문제. 반대로 가설에 대하여 특별한 Latant factor를 검증하는 문제일 수도 있다. 이를 통계에서는 **'탐색적 요인 분석'** 과 **'확인적 요인 분석'** 이라고 한다. 
	
	 ###### 관련 통계적 분석
	 - 관련하여 **'크롬바흐-알파'** (측정신뢰도), **'평균분산추출지수:AVE'** 를 이용한 개념 (타당도) 교차 검증 등을 시행
	 ###### 통계적인 연구 방법으로 요인분석에 관한 내용을 당겨서 수업???
	

- 그래서 일반적으로 사용되는 주요 방법 (통계적인 수치 설명력과 의미론적 설명력이 가능한 방법들)

- **Deep Learning 을 이용한 차원감소 및 데이터의 표현에 대해서도 생각해보자**


### 군집화 기법 

https://scikit-learn.org/stable/modules/clustering.html

##### 1) K-means  - 중심 기반 군집화 (Centroid-based) - 거리기반
- k개의 임의의 중심점을 설정하여 군집화 시킴 (임의의 중점을 설정하고 그로부터 데이터 feature들의 평균거리를 계산)
- 군집된 집단의 평균점으로 중심점 이동 (중점에서 가까운 데이터들을 cluster member로 소속시키는 방법)
- 소속 데이터가 다른 중심점에 더 가깝다면 해당 중심으로 소속변경
- 이를 더 이상 소속 변경이 없으면 종료
- *장점*  : 
		- 간결하고 쉬운 방법
		- 비지도학습이라 라벨이 필요 없음
		- 구현이 간단하고 대용량 데이터에 효율적
- *단점*  : 
		- feature 많을 수록 군집화 정확도 떨어짐. 
		- 균일한 확산에 적용, feature 간 관계나 편향이 있으면 성능이 떨어짐  
		- 임의 중점에 대한 설정이 어려움.
		- 초기 중심점에 민감하며, 구형이 아닌 군집 형태에 취약
	
	 - K-NN 최근접 이웃법과의 차이는 K-NN은 분류(Classification)모델로 지도학습 기법이다.
		1) 군집의 개수(k) 설정하는 방법
			(1) Rule of thumb
			(2) Elbow Method
			(3) 정보 기준 접근법 (Information Criterion Approach)
		2) 초기 중점 설정하기
			(1) Randomly select
			(2) Manually assign
			(3) K-means++
		3) 데이터를 군집에 할당(배정)하기
		4) 중심점 재설정(갱신) 하기
		5) 데이터를 군집에 할당(배정)하기

- 유사 종류
	- **K-Medoids (PAM: Partitioning Around Medoids)**
	    - K-Means의 변형으로, 중심점으로 실제 데이터 포인트 사용
	    - 이상치에 더 강건함
	    - 계산 비용이 K-Means보다 높음

##### 2) Hierarchical Clustering - 계층적 군집화 (Hierarchical)
-  계층적 군집 분석 : 각 데이터를 계층을 만들며 순차적으로 군집화 하는 기법
	- Agglomerative Hiderachical : 
		- 가장 가까운 데이터 부터 차례로 군집에 더해서 계층을 만드는 구조
		- 상향식 접근법: 각 데이터 포인트를 개별 군집으로 시작하여 병합
		- 덴드로그램(dendrogram)으로 시각화 가능
		- 다양한 거리 측정 방법 사용 가능 (단일 연결, 완전 연결, 평균 연결 등)
	- **분할형 (Divisive)**
		- 하향식 접근법: 모든 포인트를 하나의 군집으로 시작하여 분할
		- 
![[계층적군집화.png|400]]

- 중심점을 선택하지 않아도 됨.

##### 2) GMM (Gaussian Mixture Models) - 확률 분포 기반 군집화 (Distribution-based)
- 데이터들이 특정한 정규분포를 가지고 있는 집단의 혼합으로 표현되어 질 수 있다는 가정
- 정규분포의 들을 대입하여 데이터의 특성에 맞춰가는 방법
- EM(Expectation-Maximization) 알고리즘 사용
- 확률적 클러스터 할당 제공
##### 3) DBSCAN (Density-Based Spatial Clustering of Applications with Noise) - 밀도 기반 군집화 (Density-based)
- 특정한 epsilon 반경을 정하고 그 반경 안에 일정 수 이상의 point를 포함 시키는 Core Point와  그 주변에 같은 데이터를 수를 가지는 Neighbor Point를 정한다. 
- 범위 안에 들어오지만 충분한 데이터 수를 가지지 못하는 Bordor Point 를 정한다. 
- 나머지 범위 안에 Core, Neighbbor를 가지지 못하는 Noise Point 를 정해 가는 방식으로 즉, 이웃하는 데이터의 밀도를 가지고 군집해 가는 방법.

| point            | content                                                   |
| ---------------- | --------------------------------------------------------- |
| **core point**   | eps 반경 내에 minpts 이상개의 데이터 보유                              |
| **border point** | eps 반경 내에 minpts개의 데이터는 없지만, core point를 neighbor로 가짐.    |
| **noise point**  | eps 반경 내에 minpts개의 데이터도 없고, core point도 neighbor로 가지지 않음. |

- 장점:
	- 사전에 군집 수를 지정할 필요 없음
	- 임의 형태의 군집 발견 가능
	- 이상치 식별에 효과적
- 유사 종류
	- **OPTICS (Ordering Points To Identify the Clustering Structure)**
	    - DBSCAN의 확장 버전
	    - 다양한 밀도의 군집 처리 가능
	- **HDBSCAN (Hierarchical DBSCAN)**
	    - 계층적 접근 방식을 통해 DBSCAN 개선
	    - 다양한 밀도의 군집 식별에 더 효과적

##### 4) 기타 군집화

- 그리드 기반 군집화 (Grid-based)
	- **STING (STatistical INformation Grid)**
	- **CLIQUE (CLustering In QUEst)**
	- 데이터 공간을 그리드로 분할하여 처리
	- 대용량 데이터에 계산 효율적
	
- 신경망 기반 군집화
	- **Self-Organizing Maps (SOM)**
	    - 뉴런의 격자를 사용하여 입력 데이터의 위상을 보존
	- **Deep Embedded Clustering**
	    - 오토인코더와 같은 딥러닝 모델을 사용하여 특징 추출 후 군집화
	- 퍼지 군집화 (Fuzzy Clustering)
		- **Fuzzy C-Means**
		    - 데이터 포인트가 여러 군집에 부분적으로 속할 수 있음
		    - 각 데이터 포인트에 대해 모든 군집에 소속 정도를 계산


---
### 차원축소 기법 dimensionality reduction

##### 4) T-SNE(t-distributed Stochastic Neighbor Embedding)
- 높은 차원 공간에서 비슷한 데이터 구조는 낮은 차원 공간에서 가깝게 대응하며, 비슷하지 않은 데이터 구조는 멀리 떨어져 대응 시켜, 이웃 데이터 포인트에 대한 정보를 보전하려고 한다.
- 고차원 공간에서의 점들의 유사성과 그에 해당하는 저차원 공간에서의 점들의 유사성을 계산한다. 특정 데이터를 중심으로 한 정규 분포에서 확률 밀도에 비례하여 이웃을 선택, 조건부 확률을 계산 이를 저 차원 공간에 요소로 표현 할 때 조건부 확률과 비교 최소화 하는 방향으로 집단 구분을 최적화해감 이때, KL-divergence 사용 
##### 5) [SOM (Self-Orginizing Map)](https://lovit.github.io/visualization/2019/12/02/som_part1/)
- 대뇌피질 중 시각피질의 학습 과정을 모델화한 인공신경망 구조 군집화 기법,  비지도학습, 데이터가 적은 경우 Kmean과 유사하게 작동, 데이터가 커질수록 복잡한 구조를 가진다.
- 입력데이터와 동일한 차원의 가중치 벡터를 지닌, 노드와 그리드로 구성
- 공간적인 또는 위상학적인 거리를 계산하여 노드와 그리드를 변형 최적화된 map을 구성한다.
- [MiniSom]  
- [SOMPY]  
- [python_som]

##### 1) PCA(Principal Component Analysis) 주성분 분석 - 분산 기반
- 고차원의 데이터를 저차원의 데이터로 환원시키는 기법을 말한다. 이 때 서로 연관 가능성이 있는 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간(**주성분**)의 표본으로 변환하기 위해 [직교 변환](https://ko.wikipedia.org/w/index.php?title=%EC%A7%81%EA%B5%90_%EB%B3%80%ED%99%98&action=edit&redlink=1 "직교 변환 (없는 문서)")을 사용한다. 데이터를 한개의 축으로 사상시켰을 때 그 [분산]이 가장 커지는 축을 첫 번째 주성분, 두 번째로 커지는 축을 두 번째 주성분으로 놓이도록 새로운 좌표계로 데이터를 [선형 변환](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%EB%B3%80%ED%99%98 "선형 변환")한다. 이와 같이 표본의 차이를 가장 잘 나타내는 성분들로 분해함으로써 데이터 분석에 여러 가지 이점을 제공한다. 이 변환은 첫째 주성분이 가장 큰 분산을 가지고, 이후의 주성분들은 이전의 주성분들과 직교한다는 제약 아래에 가장 큰 분산을 갖고 있다는 식으로 정의되어있다. 중요한 성분들은 [공분산 행렬](https://ko.wikipedia.org/wiki/%EA%B3%B5%EB%B6%84%EC%82%B0_%ED%96%89%EB%A0%AC "공분산 행렬")의 고유 벡터이기 때문에 직교하게 된다.

![[PCA.png]]

>[!NOTE]
 결국 공분산은 상관계수와 관계 있고, 이는 관측된 변수간의 공변성을 통하여 같은 방향성을 가지는 방향성의 벡터의 표현으로 데이터를 정사형해서 표현해 주는 것과 같다.
>선형 대수적 표현으로는 공분산을 특이값분해(eigen decompose)해서 그 eigen value 와 eigen vector로 표현을 바꿔 주는 것과 같다.


---

- Data [Mall customer Segmentation Data](https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python)

```python
# 라이브러리 불러오기
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('seaborn')
sns.set_palette("hls")

from sklearn.preprocessing import OneHotEncoder, StandardScaler

import warnings
warnings.filterwarnings('ignore')
```

```python
# 데이터 불러오기
df = pd.read_csv('Mall_Customers.csv')
df.head()
del df['CustomerID']

# row 200개
# col 5개 (ID, 성별, 나이, 연간소득, 소비지수)
print(df.shape)

# gender 제외 모두 수치형
df.info()
```


```python
# gender는 one-hot encoding 해주기
df['Gender'].replace({'Male':0, 'Female':1}, inplace=True)

# sklearn 사용하여 one-hot encoding 하는 방법 
gender = np.array(df['Gender']).reshape(200,1)
enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
enc.fit(gender)
# enc.categories_
df['Gender'] = enc.transform(gender)
```

```python
# data 확인
df.head(10)
# 기초 통계
df.describe()
# 결측값 확인
df.isnull().sum()
```

```python
# 정규화
standard_scaler = StandardScaler()
scaled_df = pd.DataFrame(standard_scaler.fit_transform(df), columns=df.columns)
```

```python
# target balance 확인
text = "gender balance"
print('{:-^50}'.format(text))
print(df['Gender'].value_counts() / df.shape[0])
print(f'{"-"*len(text):-^50}')
plt.figure(figsize=(12, 5))
sns.countplot(df_raw['Gender'], palette='Set1')
```
---
### 군집분석의 평가

##### 1) Comfusion matrix : test 할 label 이 존재 한다면 사용 가능.

##### 2) Silhouette Score
- a(i) : 객체 i와 객체 i가 속한 cluster 내 다른 객체들간의 거리 평균
- b(i) : 객체 i와 다른 군집에 속한 객체들간의 거리평균 중, 가장 최소값을 가지는 군집과의 거리 평균

$$
\begin{align}
	&S_{i} = \frac{b_{i}-a_{i}}{max(a_{i},b_{i})} \\ \\
&a_{i} : \ mean \ of \ distance \ in \ index \ i's \ group \\
&b_{i} : minimum\ mean\ distance\ of \ index \ i \ point \ from \ other\ cluster \ groups\\
\end{align}
$$

```python 
from sklearn.metrics import silhoutte_score
sil_score = silhoutte_score(X, lables)
```

 - **범위:** -1 (나쁜 군집화) ~ +1 (좋은 군집화)
 - **장점:**
	- **직관적인 해석:** 각 데이터 포인트가 얼마나 자신의 군집에 잘 속해 있는지, 그리고 다른 군집과 얼마나 잘 분리되어 있는지 시각적으로 이해.
	- **군집 개수 추정 가능:** 각 군집의 실루엣 점수를 통해 특정 군집이 너무 좁거나 넓은지, 혹은 다른 군집과 겹치는지 등을 파악하여 적절한 군집 개수를 추정하는 데 도움.
	- **군집 모양에 대한 가정이 적음:** 군집의 모양이 볼록하지 않아도 적용 가능.
- **단점:**
	- **밀도 기반 군집에 취약:** 밀도가 다른 군집이나 복잡한 모양의 군집에서는 성능 평가 약함.
	- **거리 측정 방식에 민감:** 사용되는 거리 측정 방식에 따라 결과가 크게 달라질 수 있음.
	- **대규모 데이터셋에 대한 계산 비용:** 모든 데이터 포인트 간의 거리를 계산해야 하므로 데이터셋 크기가 커질수록 계산 비용이 증가.

##### 3) Clinski-Harabasz Index

- 분자는 군 간 변동(Variation Between Cluster)
- 분모는 국 내 변동(Variation Withn Cluster)
- 빠라서 CH 가 클수록 군집화가 잘된 결과이다.

$$
\begin{align}
	&CH = \Big[ \frac{\sum_{k=1}^{K}n_{k}||c_{k}-c||_{2}^{2}}{K-1} \Big] / \Big[ \frac{\sum_{k=1}^{K}\sum_{i=1}^{n_{k}}||x_{i}^{k}-c_{k}||_{2}^{2}}{n-K} \Big]\\ \\
	&K : \ total numbers\ of \ cluster \\
	&c_{k} : a\ core\ vector\ cluster\ k \\
	&n_{k} : elements\ number\ in\ cluster\ k \\
	&c = \sum_{i=1}^{n} x_{i}/n : core\ vector\\
	
\end{align}
$$
```python
def get_CH(x, cluster):
	cluster_label = np.unique(cluster)
	K = len(cluster_label)
	n = x.shape[0]
	c = n.mean(x, axis=0)
	
	num_sum = 0
	denom_sum = 0

	for cl in cluster_label:
		sub_x = x[np.where(cluster==cl)[0], :]
		c_k = np.mean(sub_x, axis=0)
		n_k = sub_x.shape[0]
		num_sum += n_k*np.sum(np.square(c_k - c))
		denom_sum += np.sum(np.square(sub_x - c_k))
	calinski_harabasz_index = (num_sum/(K-1))(denom_sum/(n-K))
	return calinski_harabasz_index
```
- **범위:** 값이 클수록 좋음 (하한 없음)

- **장점:**
	- 분산 개념의 직관적인 클러스터링 평가지표
	- 클러스터링 알고리즘 간 비교가 가능
	- 클러스티링 개수를 모르는 경우 클러스터 개수 선택에 활용 가능
	- **계산 효율성:** 실루엣 점수에 비해 계산 비용이 저렴하여 대규모 데이터셋에 적용하기 용이.
	- **군집 개수에 대한 정보 반영:** 군집 개수 k가 점수 계산에 직접적으로 반영.

**단점:**
- **볼록한 군집 형태 가정:** 군집이 대략적으로 볼록한 형태를 가진다고 가정하며, 복잡한 모양의 군집에서는 성능 평가가 부정확할 수 있음.
- 이상치에 민감
- DBSCAN 같은 밀도 기반 클러스터링 알고리즘의 평가 척도로 적절하지 않음.
		(Kmeans, GMM 등에 사용)
- **점수 해석의 어려움:** 실루엣 점수와 달리 절대적인 좋은 값의 기준이 명확하지 않아 상대적인 비교에 주로 사용.

##### (3) Davies-Bouldin Index

- 군집에 대해 해당 군집 내 데이터 포인트들의 평균 거리(si​)와 가장 유사한 다른 군집과의 분리 정도(dij​)를 고려하여 유사도 척도 Rij​를 계산합니다. 데이비스-볼딘 지수는 모든 군집에 대한 Rij​의 최댓값의 평균

![[Davies-Bouldin-Score.png]]
$||a||_{p} = L_{p}-Norm$

$DB=\frac{1}{k}\sum_{i=1}^{k} max_{j\ne i}​(\frac{​s_{i}​+s_{j}}{d_{ij}}​​)$
- si​: i번째 군집 내 각 데이터 포인트와 군집 중심 간 거리의 평균 (응집도).
- dij​: i번째 군집과 j번째 군집 중심 간의 거리 (분리도).

- **범위:** 0 (최고 성능)에 가까울수록 좋음 (하한 0)

- **장점:**
	- **직관적인 해석:** 각 군집이 얼마나 작고, 군집 간 거리가 얼마나 먼지를 기반으로 평가.
	- **군집 모양에 대한 가정이 적음:** 실루엣 점수와 마찬가지로 군집의 모양에 대한 강한 가정을 하지 않음.

- **단점:**
	- **볼록한 군집 형태 선호 경향:** 군집 중심을 기준으로 응집도를 측정하므로, 비볼록하거나 복잡한 모양의 군집에서는 성능이 제대로 반영되지 않을 수 있음.
	- **거리 측정 방식에 민감:** 사용되는 거리 측정 방식에 따라 결과가 크게 달라질 수 있음.
	- **대규모 데이터셋에 대한 계산 비용:** 모든 군집 쌍에 대한 비교를 수행해야 하므로 군집 수가 많아질수록 계산 비용이 증가.

---
## <관심 있는 분만 - 정성적 데이터로 접근>
## 다차원척도법 (Multi-Dimensional Scaling, MDS)

- 객체간 근접성(Proximity)를 시각화하는 기법
- 군집분석과 유사하게 변수들을 측정 후 개체들의 유사성 또는 비유사성을 측정하여 2차원 공간에 관계도를 표현하는 분석법
- 위상적인 데이터 감의 pattern과 구조를 찾는다.

- 수치형이 아닌 범주형  명목척도 데이터에 대하여서도 관계성을 파악하는데 도움이 된다.
-  대표적으로 상응분석 Correspondence Analysis 


---
## 머신러닝 연관성 분석

```
1) prince
2) apriori
3) FP-tree
```

## 통계적 연관성 분석

- 범주형, 명목형, 순위형 데이터에 대한 기술 통계
**measure of association** : 카이제곱 통계량에 기초한 분석


---

