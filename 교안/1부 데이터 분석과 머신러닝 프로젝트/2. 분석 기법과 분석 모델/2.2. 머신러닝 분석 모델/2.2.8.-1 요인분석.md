---
categories: 
title: 2.2.8. 요인분석
created: 2025-05-12
tags:
---
---
#### *2.2.8. 요인분석*
---
##### 1. 개념
 - 수많은 변수들 간의 상호 관계를 분석하여, 이 변수들의 기저에 놓여 있는 더 적은 수의 잠재적인 요인(factor)을 찾아내는 통계적 방법
 - 겉으로 보이는 여러 복잡한 현상들을 몇 가지 숨겨진 공통된 원인으로 설명하려는 분석 기법.
 - 예) 설문지 문항을 통해서 알고자하는 특징, 현상 등
##### 2. 목적
- **데이터 축소 (Data Reduction):** 많은 변수들을 더 적은 수의 요인으로 요약.
- **변수 구조 파악 (Structure Discovery):** 변수들 간의 숨겨진 관계나 패턴을 발견하고, 변수들이 어떤 공통된 요인에 의해 영향을 받는지 파악.
- **불필요한 변수 제거 (Variable Elimination):** 다른 변수들과 묶이지 않거나 설명력이 낮은 변수를 식별하여 제거함으로써 분석의 효율성을 높임.
- **측정 도구의 타당성 평가 (Instrument Validation):** 설문지나 검사와 같은 측정 도구가 실제로 측정하고자 하는 개념을 제대로 측정하는지(구인 타당성)를 평가하는 데 사용.
- **새로운 변수 생성 및 활용:** 요인분석을 통해 추출된 요인을 새로운 변수로 활용하여 후속 분석(예: 회귀분석, 판별분석)에 사용.

##### 3. 기본적인 가정
1) **변수 간의 상관관계 존재:** 분석 대상 변수들 간에는 어느 정도 이상의 상관관계가 존재해야 함. (너무 높아도 안됨)
2) **잠재 요인의 존재:** 관측된 변수들의 변화는 그 이면에 있는 잠재적인 요인(Latant Factor)들에 의해 설명될 수 있다고 가정.
- **선형 관계:** 관측 변수와 잠재 요인 간의 관계는 선형적이라고 가정.
- **고유 요인과 오차의 독립성:** 각 변수의 고유 요인(공통 요인으로 설명되지 않는 부분)과 측정 오차는 서로 독립적이라고 가정.

##### 4. **요인분석의 종류**
1) **탐색적 요인분석 (Exploratory Factor Analysis, EFA):** 
	- 사전 이론이나 가설 없이 데이터의 내재된 구조를 탐색하고 새로운 요인을 발견하는 데 사용. 
	- 요인의 수나 각 요인에 어떤 변수가 속할지에 대한 사전 지식 없이 분석을 통해 이를 밝히고자 함. (역으로 해석적으로 사용하기도 함)
2) **확인적 요인분석 (Confirmatory Factor Analysis, CFA):** 
	- 기존의 이론이나 연구를 바탕으로 요인의 수와 각 요인에 속할 변수 간의 관계를 미리 설정.
	- 수집된 데이터가 이러한 모형에 부합하는지 검증하는 데 사용. 
	- 주로 측정 도구의 타당성을 평가하는 데 활용.
3) **공통 요인 분석 (Common Factor Analysis):** 변수들의 공통 분산만을 이용하여 잠재된 공통 요인을 추출하는 데 초점. 변수에는 공통 요인 외에도 고유 요인이 존재한다고 가.

##### 5. 요인분석의 주요 단계

 1) **변수 선택:** 분석하고자 하는 변수들을 선정.
 2) **상관 행렬 계산:** 변수들 간의 상관 계수를 계산하여 상관 행렬을 생성.
 3) **요인 추출:** 상관 행렬을 바탕으로 잠재 요인을 추출. (PCA 등)
 4) **요인 수 결정:** 추출된 요인 중 몇 개를 최종적으로 선택할 것인지 결정. (고유값 기준, 스크리 그림(elbow), 설명된 분산 비율 등)
 5) **요인 회전:** 추출된 요인들의 의미를 더 명확하게 해석하기 위해 요인 축을 회전(선형변화을 이용). (회전법의 종류: 직각 회전(Varimax, Quartimax, Equamax)과 사각 회전(Direct Oblimin, Promax) 등)
 6) **요인 해석 및 명명:** 회전된 요인들의 요인 적재량(각 변수가 해당 요인과 얼마나 관련이 있는지 나타내는 값)을 바탕으로 각 요인이 무엇을 의미하는지 해석하고 적절한 이름을 부여.
 7)  **결과 보고:** 요인 분석 과정과 결과를 종합적으로 정리.


![[Factor_elements.png]]

- 자기 자신 보다 낮은 함축적 차원의 특징(feature)으로 원본 데이터를 표현하는 과정으로 설명할 수 있음.
$$
X_{i} - \mu_{i} = l_{i,1}f_{1}+l_{i,2}f_{2}+\cdots+l_{i,m}f_{m}+ \epsilon_{i}\ , \quad i=1,\dots , p 
$$
>	$X_{i}$ = i 번째 변수
	 $\mu_{i}$ = i 번째 변수의 평균
	 $l_{i,k}$ = i 번째 변수의 $k$ 번째 요인의 적재(설명력) : Factor Loading
	 $f_{k}$ = $k$ 번째 공통인자 : Common Factor
	 $\epsilon_{i}$ = i 번째 변수만이 가진 특징 또는 특성 정보로 공통인자로 설명하지 못하는 부분 : Specific Factor

$$
\begin{align}
	&X - \mu = Lf+\epsilon \\ \\
	&L = \begin{bmatrix} 
			l_{11}\ l_{12}\ \cdots\ l_{1m} \\
			l_{21}\ l_{22}\ \cdots\ l_{2m} \\
			\vdots\quad \vdots\quad \ddots\quad \vdots\\
			l_{p1}\ l_{p2}\ \cdots\ l_{pm} \\
		 \end{bmatrix} \\ \\
	&\Sigma = E(X-\mu)\cdot E(X-\mu)^{T} = LL^{T} + \psi \\ \\
	&\psi = \begin{bmatrix} 
		\psi_{11}\ \psi_{12}\ \cdots\ \psi_{1m} \\
		\psi_{21}\ \psi_{22}\ \cdots\ \psi_{2m} \\
		\vdots\quad \vdots\quad \ddots\quad \vdots\\
		\psi_{p1}\ \psi_{p2}\ \cdots\ \psi_{pm} \\
	 \end{bmatrix} \\ \\
	 &Cov(X,f) = E[(X-\mu)f^{T}]=E[(Lf+\epsilon)f^{T}] =L \\ \\
	 &Var(X_{i}) = l_{i1}^{2}+l_{i2}^{2}+\cdots+l_{im}^{2}+\psi=\sum_{j=1}^{m}l_{ij}^{2}+\psi \\ \\
\end{align}
$$

```python
class FactorAnalysis:
    def __init__(self):
        self.loadings = None ## 인자 적재 행렬
        self.rotation_matrix = None ## 회전 행렬
        self.rotated_loadings = None ## 회전시킨 인자 적재 행렬
        self.factor_score = None ## 인자 점수
        self.factor_score_coefficent = None ## 인자 점수 계수
        self.cov_mat = None ## 표본 공분산 행렬
        self.explained_variance = None ## 분산 설명 비율
        
    def fit(self, X, num_factor=2, rotation_type='varimax', k=4, max_iter=1000, eps=1e-05):
        assert rotation_type in ['varimax', 'promax']
        
        self.get_loadings(X, num_factor) ## 인자 적재 행렬
        L = self.loadings
        ## 인자 회전
        self.get_rotated_loadings(L, rotation_type=rotation_type, 
                                  max_iter=max_iter, eps=eps, k=k) 
        
        self.get_factor_score(X)
        return self
    
    def get_rotated_loadings(self, L, rotation_type, k, max_iter, eps):
        if rotation_type == 'varimax':
            R, rotated_L = self._varimax(L, max_iter=max_iter, eps=eps)
            self.rotation_matrix = R
            self.rotated_loadings = rotated_L
        else:
            R, rotated_L = self._promax(L, k=k, max_iter=max_iter, eps=eps)
            self.rotation_matrix = R
            self.rotated_loadings = rotated_L
            
    def _varimax(self, L, max_iter=1000, eps=1e-05): ## varimax 회전
        R = np.eye(L.shape[1])
        d_old = 0 
        p = L.shape[0]
        for i in range(max_iter):
            L_star = L @ R
            Q = L.T @ (L_star ** 3 - (L_star @ np.diag(np.diag(L_star.T @ L_star)))/p)
            U, S, V_t = np.linalg.svd(Q)
            R = U @ V_t
            d_new = np.sum(S)
            if d_new < d_old * (1 + eps):
                break
            else:
                d_old = d_new
        return R, L_star        
        
    def _promax(self, L, k=4, max_iter=1000, eps=1e-5): ## promax
        _, L_star = self._varimax(L, max_iter=max_iter, eps=eps)
        P = L_star * (np.abs(L_star) ** (k-1))
        L_inv = np.linalg.inv(L_star.T @ L_star)
        R = L_inv @ L_star.T @ P
        R = R/np.sqrt(np.sum(np.square(R), axis=0))
        L_final = L_star @ R
        return R, L_final
        
    def get_loadings(self, X, num_factor): ## 인자 적재 행렬
        cov_mat = (X - np.mean(X, axis=0)).T @ (X - np.mean(X, axis=0))/(X.shape[0]-1)
        self.cov_mat = cov_mat
        eigen_value, eigen_vector = np.linalg.eig(cov_mat)
        eigen_value = eigen_value[:num_factor]
        eigen_vector = eigen_vector[:, :num_factor]
        loadings = np.sqrt(eigen_value)*eigen_vector
        self.loadings = loadings
        self.explained_variance = np.sum(eigen_value)/X.shape[1]
        
    def get_factor_score(self, X):
        L = self.rotated_loadings
        S_inv = np.linalg.inv(self.cov_mat)
        factor_score = L.T @ S_inv @ (X-np.mean(X, axis=0)).T
        self.factor_score_coefficent = (L.T @ S_inv).T
        self.factor_score = factor_score.T
```


---

##### 요인분석 Python Package

1) factor_analyzer

```python
# Install the factor_analyzer package
# !pip install factor_analyzer

import pandas as pd
from factor_analyzer import FactorAnalyzer
import matplotlib.pyplot as plt

# Load data
data = pd.read_csv('your_data.csv')

# Apply Bartlett's test
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
chi_square_value, p_value = calculate_bartlett_sphericity(data)
print(f'Chi-square value: {chi_square_value}\nP-value: {p_value}')

# Apply KMO test
from factor_analyzer.factor_analyzer import calculate_kmo
kmo_all, kmo_model = calculate_kmo(data)
print(f'KMO Model: {kmo_model}')

# Create factor analysis object and perform factor analysis
fa = FactorAnalyzer(rotation="varimax")
fa.fit(data)

# Check Eigenvalues
eigen_values, vectors = fa.get_eigenvalues()
plt.scatter(range(1, data.shape[1]+1), eigen_values)
plt.plot(range(1, data.shape[1]+1), eigen_values)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()

# Perform factor analysis with the determined number of factors
fa = FactorAnalyzer(n_factors=3, rotation="varimax")
fa.fit(data)

# Get factor loadings
loadings = fa.loadings_
print(loadings)

# Get variance of each factor
fa.get_factor_variance()

# Get factor scores
factor_scores = fa.transform(data)
print(factor_scores)
```

2) [scikit-learn : factor-analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html)
: 
[Example-1](https://www.geo.fu-berlin.de/en/v/soga-py/Advanced-statistics/Multivariate-Approaches/Factor-Analysis/A-Simple-Example-of-Factor-Analysis-in-Python/index.html)
[Example-2](https://scikit-learn.org/stable/auto_examples/decomposition/plot_varimax_fa.html)


3) [statsmodels.multivariate.factor](https://www.statsmodels.org/dev/generated/statsmodels.multivariate.factor.Factor.html)

- semopy

> [R "lavaan" 사용]
> R 설치 후 `install.packages("lavaan")`, Python에서 `pip install rpy2`


---
