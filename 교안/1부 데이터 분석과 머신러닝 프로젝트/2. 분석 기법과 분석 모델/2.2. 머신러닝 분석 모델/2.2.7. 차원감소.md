---
categories: 
title: 2.2.7. 차원감소
created: 2025-05-12
tags:
---
---
#### *2.2.7. 차원감소*
---

### 차원감소 or 차원축소(dimensionality reduction)

#### 1. 의미
- 단순 의미 : 고차원 데이터로부터, 저차원의 데이터로 변환하는 방법.
- 의미 : 원본 데이터의 일부 의미 있는 속성을 선택하거나 이상적으로는 본질적인 차원에 정보에 가깝게 유지하도록 고차원 공간의 데이터를 저차원 공간으로 변환하여 표현하는 것.
- 모델 추론 과정 해석과도 일부 관계가 있음.
#### 2. 필요
- **계산 효율성 향상**: 데이터의 차원이 줄어들면 모델 학습 및 예측에 필요한 계산 자원을 줄일 수 있어 자원효율 성과 응답속도를 높일 수 있음. 
	 `최적화` : 모든 알고리즘과 분석은 프로젝트 목적에 맞는 효율성과 최적화가 필수이다.
- **모델 성능 향상**: 고차원 데이터에는 불필요하거나 노이즈가 많은 특징(feature)들이 포함될 수 있으며, 이는 모델의 과적합(overfitting)을 유발하고 일반화 성능을 저하시킬 수 있다. 차원 축소는 이러한 문제를 완화할 수 있다. 
	 `Noise filtering` : 모든 개별적 Sample은 고유 정보 또는 각 상황과 상태에 따른 정보를 가지고 있다. 그러나 분석 또는 추론 목적과 직접적으로 모두 상관 있는 정보이지는 않다. 
- **데이터 시각화 용이**: 2차원 또는 3차원으로 축소된 데이터는 시각화하여 데이터의 패턴, 군집 구조, 이상치 등을 쉽게 파악할 수 있다. 
	 `직관적 해석 가능성` : 고차원으로는 이해가 어려운 관계를 직관적으로 파악할 수 있게 해준다.
- **특징 해석 용이**: 중요한 특징들을 추출하거나 새로운 저차원 특징들을 생성함으로써 데이터의 구조를 더 잘 이해할 수 있다. 
	 `요인 분석 가능성` : 함축된 추상화 상위 개념으로 변환. 또는 역으로 가설이나 개념의 검증 분석

#### 3. 방법론적 분류
###### **A. 특징 선택 (Feature Selection)** => 가지치기적 접근
- 원래의 특징 집합(feature or columns)에서 예측 또는 분류에 가장 관련성이 높은 부분 집합을 선택하 관점. (일반화 또는 목적에 불필요한 정보를 제공하는 특징을 제거한다는 관점)

1) **필터 방법 (Filter Methods)**: 
	- 통계적 측정 기준(분산, 상관 계수, 카이 제곱 검정, 정보 이득)을 사용하여 각 특징의 중요도를 평가. 미리 정의된 기준에 따라 특징을 선택. (데이터의 통계적 속성에만 의존하며 모델 학습과는 독립적으로 과정)
    - **분산 임계값 (Variance Thresholding)**: 분산이 낮은 특징들을 제거.
    - **상관 분석 (Correlation Analysis)**: 목표 변수와의 상관 관계가 높은 특징들을 선택하거나, 특징 간의 높은 상관 관계를 보이는 특징 중 하나를 제거. (너무 낮으면 의미 없는 관계일 수 있고, 너무 높으면 타겟 특징과 동일한 특징을 가능성이 높음)
    - **카이 제곱 검정 (Chi-squared Test)**: 범주형 특징과 범주형 목표 변수 간의 독립성을 검정하여 관련성이 높은 특징을 선택.
    - **정보 이득 (Information Gain)**: 특징이 목표 변수의 불확실성을 얼마나 감소시키는지 측정하여 특징을 선택. (의사결정 트리 나무의 Infomation Entropy)

2) **래퍼 방법 (Wrapper Methods)**: 선택된 특징의 성능을 특정 머신러닝 모델을 사용하여 평가하고, 최적의 특징 부분 집합을 탐색. 
    - **전진 선택 (Forward Selection)**: 특징을 하나씩 추가(여러 경우의 수 발생하여 계산량은 증가함)하여 모델에 최적화된 특징을 선택.
    - **후진 제거 (Backward Elimination)**: 모든 특징을 포함한 상태에서 시작하여 성능에 가장 적은 영향을 미치는 특징을 하나씩 제거.
    - **재귀적 특징 제거 (Recursive Feature Elimination, RFE)**: 모델을 반복적으로 학습하고 가장 중요하지 않은 특징을 제거하는 과정을 반복.
    
3) **임베디드 방법 (Embedded Methods)**: 모델 학습 과정에 특징 선택 기능이 내장된 방법입니다. 모델 학습과 동시에 특징의 중요도를 학습.
    - **L1 규제 (Lasso)**: 선형 모델에 L1 노름 페널티를 추가하여 일부 특징의 계수를 0으로 만들어 특징 선택 효과.
    - **트리 기반 모델 (Tree-based Models)**: 의사 결정 트리, 랜덤 포레스트, 그래디언트 부스팅 등의 모델은 특징 중요도를 평가하는 기능을 제공하며, 이를 기반으로 중요한 특징을 선택. (랜덤포레스트의 특징 랜덤 선택을 이용한 모델 생성 등)

###### **B. 특징 추출 (Feature Extraction)** => 데이터 변환적 접근
- 원래의 고차원 특징들을 변환하여 더 낮은 차원의 새로운 특징 집합을 생성하는 방법. 
- 이 새로운 특징들은 원래 데이터의 중요한 정보를 최대한 보존하도록하는 방법 선택.

 1) **주성분 분석 (Principal Component Analysis, PCA)**: 
	 - 데이터의 분산을 최대한 보존하는 직교하는 주성분(principal components)들을 찾아내어 데이터를 이 주성분들의 선형 결합으로 표현. 
	 - 분산이 큰 주성분일수록 데이터의 정보를 더 많이 담고 있다고 가정. 
	 - 서로 연관 가능성이 있는 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간(**주성분**)의 표본으로 변환하기 위해 [직교 변환](https://ko.wikipedia.org/w/index.php?title=%EC%A7%81%EA%B5%90_%EB%B3%80%ED%99%98&action=edit&redlink=1 "직교 변환 (없는 문서)")을 사용
	 - 데이터를 한개의 축으로 사상시켰을 때 그 [분산]이 가장 커지는 축을 첫 번째 주성분, 두 번째로 커지는 축을 두 번째 주성분으로 놓이도록 새로운 좌표계로 데이터를 [선형 변환](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%EB%B3%80%ED%99%98 "선형 변환")한다.
	 - 주성분들은 이전의 주성분들과 직교. (선형관계 독립)
	 - 중요한 성분들은 [공분산 행렬](https://ko.wikipedia.org/wiki/%EA%B3%B5%EB%B6%84%EC%82%B0_%ED%96%89%EB%A0%AC "공분산 행렬")의 고유 벡터이기 때문에 직교. (일종의 공분산 행렬의 고유값 분해(Eigen-Value Decomposition) 

![[PCA.png]]

 2) **선형 판별 분석 (Linear Discriminant Analysis, LDA)**: 클래스 간의 분산은 최대화하고 클래스 내의 분산은 최소화하는 선형 변환을 찾아내어 데이터를 저차원 공간으로 투영. (분류 알고리즘의 정의역으로 투영)

> 다음은 **매우 중요** 하지만 선형대수에 대한 선행 학습 필요.
> 1) 선형변환(선형사상) : 가산성과 동차성
> 2) 행렬 연산과 주요 개념
> 3) 고유값과 고유벡터 (eigen-value, eigen-vector) - 고유값 분해 (eigen vector decomposition)

 3) **특이값 분해 (Singular Value Decomposition, SVD)**: 
	 - 행렬을 세 개의 행렬의 곱으로 분해하는 방법. 
	 - 고유값 분해를 일반적인 형태로 확장한(정방행열에서 직각행열 형태의 데이터로 확장) 방법으로 PCA의 수학적 기반. 
	 - 이미지 압축, 추천 시스템, 노이즈 제거 등 다양한 분야에 활용. 

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Singular_value_decomposition_visualisation.svg/500px-Singular_value_decomposition_visualisation.svg.png" width=300>

$$A=U \Sigma V^{T}$$

- $U$ = m × m 크기 : **left singlar vector** 
	- $AA^{T}$ 을 고유값-분해 ($AA^{T}=U(\Sigma\Sigma ^{T})U^{T}$)
	- 열 공간(column space)에 대한 정규직교 기저(orthonormal basis)를 형성. A의 결과 벡터들이 놓이는 공간의 주요 방향.
- $\Sigma$ = m × n 크기 : **Singular Value Matrix** 
	- 대각선상에 있는 원소의 값은 음수가 아니며 나머지 원소의 값이 모두 0인 대각행렬. $σ_{1}​≥σ_{2}​≥⋯≥σ_{r}​>0$ (여기서 $r$은 행렬 A의 랭크)으로 이루어져 있으며, 나머지 성분은 모두 0 이다.
	- A에 의한 선형 변환이 i번째 좌특이 벡터 방향으로 얼마나 많은 "에너지" 또는 "중요도"를 갖는지를 나타냄. 
	- 특이값이 클수록 해당 방향으로의 데이터 분산이 크고, 데이터의 중요한 정보를 많이 담고 있다고 해석할 수 있음.
	- 특이값의 크기는 해당 특이 벡터 쌍(ui​와 vi​)이 A를 통해 데이터를 변환할 때의 "확대/축소" 정도를 나타냄.
- $V^{T}$ = n x n : **Transpose of Right Singular VectorsV** 
	- 켤레전치 행렬 ($A^{T}A=V(\Sigma ^{T}\Sigma)V^{T}$)
	- A의 **행 공간(row space)** 또는 AT의 열 공간에 대한 정규직교 기저를 형성
	- 원본 데이터의 특징들이 놓이는 공간의 주요 방향들을 나타냄.
- $U$ 와 $V$ 는 직교 행열 (Orthogonal Matrix)
	- $U^{T}U = UU^{T} = I$ ($I$: 단위행렬)
	- $V^{T}V = VV^{T} = I$ ($I$: 단위행렬)

>- $V^{T}$: 원본 데이터의 특징 공간을 회전시켜 데이터의 주요 분산 방향을 새로운 축으로 정렬.
>- $\Sigma$: 정렬된 각 축 방향으로 데이터를 특이값에 비례하여 스케일링(확대 또는 축소). 큰 특이값에 해당하는 방향으로는 데이터가 크게 늘어나고, 작은 특이값에 해당하는 방향으로는 작게 줄어듬.
>- $U$ : 스케일링된 데이터를 최종 결과 공간으로 다시 회전. 

 4) **독립 성분 분석 (Independent Component Analysis, ICA)**: 
	 - 다변량 신호를 통계적으로 독립적인 성분들의 선형 결합으로 분리. 
	 - 다양한 연속 데이터 또는 데이터 표현을 층(layer)으로 분해

 5) **t-분산 확률적 이웃 임베딩 (t-distributed Stochastic Neighbor Embedding, t-SNE)**: 
	- 고차원 데이터 점들 간의 유사성을 저차원 공간에서 보존하는 비선형 차원 축소 기법. 
	- 주로 데이터 시각화에 사용.

 6) **균일 다양체 근사 및 투영 (Uniform Manifold Approximation and Projection, UMAP)**: 
	 - 고차원 데이터의 국소적 및 전역적 구조를 모두 보존하는 것을 목표로 하는 비선형 차원 축소 기법. 
	 - t-SNE보다 계산 효율성이 높고 대규모 데이터셋에 잘 작동.

 7) **자동 인코더 (Autoencoder)**: 
	 - 신경망 기반의 비선형 차원 축소 기법. 
	 - 입력 데이터를 저차원 표현(bottleneck)으로 인코딩한 후, 다시 원래 차원으로 디코딩하는 네트워크를 학습. 
	 - 학습된 저차원 표현이 특징 추출의 결과로 사용될 수 있다. 

---

[scikit-learn : PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)


```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

# 1. 데이터 로드 및 준비
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names

# 데이터 표준화 (PCA 적용 전 필수)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 2. PCA 모델 생성 및 학습
# n_components는 유지할 주성분 개수
# 여기서는 2개의 주성분으로 차원을 축소합니다.
n_components = 2
pca = PCA(n_components=n_components)
pca.fit(X_scaled)

# 3. 주성분 분석 결과 확인
print("고유값 (설명된 분산):", pca.explained_variance_)
print("설명된 분산 비율:", pca.explained_variance_ratio_)
print("주성분 (고유 벡터):")
for i, component in enumerate(pca.components_):
    print(f"  주성분 {i+1}: {component}")
    for
```

[scikit-learn : fastICA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import FastICA
from scipy.io import wavfile
import os

# 1. 혼합된 신호 생성 또는 로드
# 간단한 예시로 두 개의 독립적인 사인파를 생성하여 혼합합니다.
np.random.seed(0)
n_samples = 2000
time = np.linspace(0, 8, n_samples)

# 독립적인 신호 1: 5Hz 사인파
signal1 = np.sin(5 * time)

# 독립적인 신호 2: 12Hz 사인파
signal2 = np.sin(12 * time)

# 독립적인 신호들을 하나의 행렬로 쌓습니다. 각 행이 하나의 신호입니다.
S = np.c_[signal1, signal2]
S += 0.2 * np.random.normal(size=S.shape)  # 약간의 노이즈 추가
S /= S.std(axis=0)  # 각 신호의 표준편차를 1로 정규화

# 혼합 행렬 A 생성 (임의의 혼합)
A = np.array([[1, 0.8], [0.3, 1.5]])

# 혼합된 신호 X 생성
X = np.dot(S, A.T)

# 시각화
plt.figure(figsize=(12, 6))

plt.subplot(3, 1, 1)
plt.title('원본 독립 신호')
plt.plot(time, S[:, 0], label='신호 1')
plt.plot(time, S[:, 1], label='신호 2')
plt.xlabel('시간')
plt.ylabel('진폭')
plt.legend()

plt.subplot(3, 1, 2)
plt.title('혼합된 신호')
plt.plot(time, X[:, 0], label='혼합 1')
plt.plot(time, X[:, 1], label='혼합 2')
plt.xlabel('시간')
plt.ylabel('진폭')
plt.legend()

# 2. ICA 모델 생성 및 적용
n_components = 2  # 찾고자 하는 독립 성분 개수 (원본 신호 개수와 동일하게 설정)
ica = FastICA(n_components=n_components, random_state=0)
S_recovered = ica.fit_transform(X)  # 혼합된 신호 X로부터 독립 성분 S_recovered 추정

# 3. ICA 결과 시각화
plt.subplot(3, 1, 3)
plt.title('ICA로 복원된 독립 신호')
plt.plot(time, S_recovered[:, 0], label='복원된 신호 1')
plt.plot(time, S_recovered[:, 1], label='복원된 신호 2')
plt.xlabel('시간')
plt.ylabel('진폭')
plt.legend()

plt.tight_layout()
plt.show()

# 4. (선택 사항) 음성 데이터에 ICA 적용 예시
# (주의: 오디오 파일 경로를 실제 파일 경로로 변경해야 합니다.)
if os.path.exists('mixed_audio_1.wav') and os.path.exists('mixed_audio_2.wav'):
    try:
        # 두 개의 혼합된 오디오 파일 로드
        samplerate1, mixed_signal1 = wavfile.read('mixed_audio_1.wav')
        samplerate2, mixed_signal2 = wavfile.read('mixed_audio_2.wav')

        if samplerate1 != samplerate2:
            raise ValueError("샘플링 속도가 다릅니다.")

        mixed_audio = np.c_[mixed_signal1, mixed_signal2]

        # ICA 적용
        n_components_audio = 2
        ica_audio = FastICA(n_components=n_components_audio, random_state=0)
        separated_audio = ica_audio.fit_transform(mixed_audio)

        print("ICA를 통해 분리된 오디오 신호 형태:", separated_audio.shape)

        # (선택 사항) 분리된 신호 저장
        wavfile.write('separated_audio_1.wav', samplerate1, separated_audio[:, 0].astype(np.int16))
        wavfile.write('separated_audio_2.wav', samplerate1, separated_audio[:, 1].astype(np.int16))

        print("분리된 오디오 신호가 separated_audio_1.wav 및 separated_audio_2.wav로 저장되었습니다.")

    except FileNotFoundError:
        print("오디오 파일을 찾을 수 없습니다. 'mixed_audio_1.wav' 및 'mixed_audio_2.wav' 파일이 현재 디렉토리에 있는지 확인해주세요.")
    except ValueError as e:
        print(f"오류: {e}")
else:
    print("오디오 파일 예제를 실행하려면 'mixed_audio_1.wav' 및 'mixed_audio_2.wav' 파일이 필요합니다.")
    print("이 파일들은 예시를 위해 직접 생성하거나 준비해야 합니다.")
```


- [T-sne](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.datasets import load_digits
from sklearn.preprocessing import StandardScaler

# 1. 데이터 로드 및 준비
# 손글씨 숫자 데이터셋 (8x8 이미지, 총 1797개 샘플, 10개 클래스)
digits = load_digits()
X = digits.data
y = digits.target
n_samples, n_features = X.shape

# 데이터 표준화 (t-SNE 적용 전 권장)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 2. t-SNE 모델 생성 및 학습
# n_components: 임베딩할 저차원 공간의 차원 (일반적으로 2 또는 3)
# perplexity: 지역 이웃의 수에 영향을 미치는 파라미터 (일반적으로 5 ~ 50 사이의 값)
# n_iter: 최적화를 위한 반복 횟수
X_scaled = scaler.fit_transform(X)
# 3. t-SNE 결과 시각화 (2차원으로 임베딩했을 경우)
if n_components == 2:
    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, cmap='Spectral')
    plt.xlabel('t-SNE Component 1')
    plt.ylabel('t-SNE Component 2')
    plt.title(f't-SNE visualization of digits dataset (perplexity={perplexity}, n_iter={n_iter})')
    plt.colorbar(scatter, label='Digit')
    plt.grid(True)
    plt.show()

# 4. t-SNE 모델 생성 및 학습 (3차원으로 임베딩하는 예시)
if n_features > 3:  # 원본 데이터가 최소 4차원 이상이어야 3차원 임베딩 가능
    n_components_3d = 3
    tsne_3d = TSNE(n_components=n_components_3d, perplexity=perplexity, n_iter=n_iter, random_state=42)
    X_embedded_3d = tsne_3d.fit_transform(X_scaled)

    print("\nt-SNE 3차원 임베딩 후 데이터 형태:", X_embedded_3d.shape)

    # 5. t-SNE 결과 시각화 (3차원으로 임베딩했을 경우)
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')
    scatter = ax.scatter(X_embedded_3d[:, 0], X_embedded_3d[:, 1], X_embedded_3d[:, 2], c=y, cmap='Spectral')
    ax.set_xlabel('t-SNE Component 1')
    ax.set_ylabel('t-SNE Component 2')
    ax.set_zlabel('t-SNE Component 3')
    ax.set_title(f't-SNE 3D visualization of digits dataset (perplexity={perplexity}, n_iter={n_iter})')
    fig.colorbar(scatter, label='Digit')
    plt.show()
else:
    print("\n원본 데이터가 3차원 미만이므로 3차원 t-SNE 임베딩 예제를 건너뜁니다.")
```

---

#### Manifold

- 수학 및 데이터 분석에서는 다양체, 즉 국소적으로는 유클리드 공간과 유사하지만 전체적으로는 독특한 구조를 가진 위상 공간을 의미


![[manifold_roll_data.png]]

![[manifold_transdata.png]]

![[Reasonable_distance_manifold.png]]


[Manifold-Learning](https://scikit-learn.org/stable/auto_examples/manifold/index.html)
