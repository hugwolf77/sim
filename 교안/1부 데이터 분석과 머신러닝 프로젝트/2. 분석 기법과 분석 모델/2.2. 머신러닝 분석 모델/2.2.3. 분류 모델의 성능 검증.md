---
categories: 글쓰기
title: 2.2.3.  분류 모델의 성능 검증
tags:
  - 수업
  - 교재
  - model_matrix
created: 2024-09-19
---

---
## 2.2.3. 분류 모델의 성능 검증
---


- 모델의 성능 : confusion metrix 

<center>
	<img src="https://miro.medium.com/v2/resize:fit:720/format:webp/0*TsXuwwoeyoRWQZZG.jpg">
</center>

- **TP (True Positive):** 실제 양성(Positive)을 양성으로 정확하게 예측한 경우
- **TN (True Negative):** 실제 음성(Negative)을 음성으로 정확하게 예측한 경우
- **FP (False Positive):** 실제 음성을 양성으로 잘못 예측한 경우 (Type 1 오류)
- **FN (False Negative):** 실제 양성을 음성으로 잘못 예측한 경우 (Type 2 오류)

**주요 성능 지표 및 계산식**

1) **정확도(Accuracy)**
        - 설명: 전체 예측 중 정확하게 예측한 비율
    - 계산식: $$\frac{(TP + TN)}{(TP + TN + FP + FN)}$$
    - 특징: 데이터의 클래스 분포가 균등할 때 유용하지만, 불균형 데이터에서는 성능을 잘못 평가할 수 있음.

2) **정밀도(Precision)**
    - 설명: 양성으로 예측한 것 중에서 실제 양성인 비율
    - 계산식: $$\frac{TP}{(TP + FP)}$$
    - 특징: FP를 줄이는 것이 중요한 경우에 유용 (예: 스팸 메일 분류)

3) **재현율(Recall) 또는 민감도(Sensitivity)**
    - 설명: 실제 양성 중에서 양성으로 정확하게 예측한 비율
    - 계산식: $$\frac{TP}{(TP + FN)}$$
    - 특징: FN을 줄이는 것이 중요한 경우에 유용 (예: 암 진단)

4) **F1 점수(F1 Score)**
    - 설명: 정밀도와 재현율의 조화 평균
    - 계산식: $$\frac{2 \times (Precision * Recall)}{(Precision + Recall)}$$
    - 특징: 정밀도와 재현율의 균형을 평가할 때 유용

5) **특이도(Specificity)**
    - 설명: 실제 음성 중에서 음성으로 정확하게 예측한 비율
    - 계산식: $$\frac{TN}{(TN + FP)}$$
    - 특징: 음성을 정확히 예측하는 것이 중요한 경우에 유용

6) **1종 오류 (Type I Error - $\alpha$ Error)**
- 귀무가설이 실제로 참인데도 불구하고 이를 기각하는 오류.
- 즉, 실제로 효과가 없는데 효과가 있다고 잘못 판단하는 경우.(**FP**)
- 1종 오류를 범할 확률은 유의 수준(α)으로 나타내며, 일반적으로 0.05 또는 0.01로 설정.
- 예시: 새로운 약이 실제로 효과가 없는데, 임상 시험에서 효과가 있다고 잘못 결론짓는 경우

7) **2종 오류 (Type II Error - $\beta$ Error)**
- 귀무가설이 실제로 거짓인데도 불구하고 이를 기각하지 못하는 오류.
- 즉, 실제로 효과가 있는데 효과가 없다고 잘못 판단하는 경우.
- 2종 오류를 범할 확률은 β로 나타내며, 검정력(1-β)은 귀무가설이 거짓일 때 이를 올바르게 기각할 확률을 의미.
- 예시: 실제로 효과가 있는 새로운 약을, 임상 시험에서 효과가 없다고 잘못 결론짓는 경우

![[1형2형오류.png]]

### AUC and ROC (Reciever Operating Characteristic curve and Area Under the Curve)

##### **1) ROC curve (Reciever Operating Characteristics, 수신자 조작 특성 곡선)**

![[ROC_AUC 1.png]]

- 기준값(threshold)이 달라짐에 따라 분류모델의 성능이 어떻게 변하는지를 나타내기 위해 그리는 곡선. ROC 곡선은 분류 모델의 성능을 시각화하는 도구이며, 1종 및 2종 오류와 밀접한 관련 있음.

- **참 양성 비율 (TPR):**
    - ROC 곡선의 y축은 참 양성 비율(TPR) 또는 민감도를 나타냄.
    - TPR은 실제 양성을 양성으로 올바르게 예측한 비율이며, 2종 오류와 관련됨.
    - TPR이 낮을수록 2종 오류를 범할 확률이 높아짐.
$$참양성율(재현율, 민감도)\ \ TPR = \frac{TP}{TP+FN}$$
- **거짓 양성 비율 (FPR):**
    - ROC 곡선의 x축은 거짓 양성 비율(FPR)
    - FPR은 실제 음성을 양성으로 잘못 예측한 비율이며, 1종 오류와 관련됨.
    - FPR이 높을수록 1종 오류를 범할 확률이 높짐.

$$거짓양성률(1-특이도)\ \ FPR=\frac{FP}{FP+TN}$$
- **임계값:**
    - 분류 모델은 예측 확률을 기반으로 양성 또는 음성을 결정하는 기준 설정.
    - 이때 사용되는 기준값을 임계값이라 함.
    - 임계값을 변경하면 FPR과 TPR이 모두 변경 됨.
    - ROC 곡선은 다양한 임계값에서 FPR과 TPR의 변화를 보여주는 그래프임.

##### 2) AUC (Area Under the Curve)

-  ROC 곡선 아래 영역, 가능한 모든 분류 임곗값에서 집계된 성능 측정값을 제공

![[ROC_AUC 1.png]]


####### [**confusion-matrix extension**](https://www.ml-science.com/confusion-matrix)


----
##### scikit-learn
###### [Confusion Matrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)

###### [ROC display](https://scikit-learn.org/stable/auto_examples/model_selection/plot_det.html#sphx-glr-auto-examples-model-selection-plot-det-py)
```python
import matplotlib.pyplot as plt
from sklearn.metrics import DetCurveDisplay, RocCurveDisplay
from sklearn.metrics import ConfusionMatrixDisplay

fig, [ax_roc, ax_det] = plt.subplots(1, 2, figsize=(11, 5))

for name, clf in classifiers.items():
    clf.fit(X_train, y_train)
    RocCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax_roc, name=name)
    DetCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax_det, name=name)

ax_roc.set_title("Receiver Operating Characteristic (ROC) curves")
ax_det.set_title("Detection Error Tradeoff (DET) curves")

ax_roc.grid(linestyle="--")
ax_det.grid(linestyle="--")

plt.legend()
plt.show()
```

####### [Calibration Curve](https://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html)
