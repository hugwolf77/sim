---
categories: 글쓰기
title: 2.1.3. 선형 회귀분석 모델(Linear Regression Model)
created: 2024-09-22
tags:
  - 교재
  - 수업
  - 선형회귀모델
---
---
### *선형 회귀분석 모델(Linear Regression Model)*
---
##### 인과관계 (Causality)
- 한 사건이나 변수가 다른 사건이나 변수의 **직접적인 원인**이 되는 관계를 의미
- 시간적 선후 관계
- 인위적인 개입
- 매개변수
##### 상관관계 (Correlation)
- 두 가지 변수가 함께 변화하는 정도를 나타내는 관계를 의미
- 같이 변화하는 정도 인과관계의 존재를 꼭 의미하지는 않음 (가능성이 높음)
##### 인과관계를 정의하는 연구 모형에서 
- 현상을 설명하기 위한 연구적 설계와 정의가 있어야 회귀모델을 통한 인과관계 설명이 가능하다. 회귀식 만으로는 인과관계로 확정지을 수는 없다.
- **영향을 주는 변수 : 독립 변수 (independent variable) or 설명 변수**
- **영향을 받은 변수 : 종속 변수 (dependent variable) or 반응 변수**
$$
	\begin{align}
		&Y_{i}\ =\ \alpha\ +\ \beta X_{i}\ +\ \epsilon_{i} \\
		& i = 1,2,3,\dots,n \quad \\
		& n: number\ sample
	\end{align}
$$
![[Uni_reg.png|400]]
- 상수(constant) $\alpha$  
- 회귀계수(regression coefficient) $\beta$
- 오차(error term) $\epsilon$  

- 독립변수와 종속변수하 하나뿐인 가장 간단한 단변량 단순 회귀분석

- 상관계수와 달리 X와 Y 간에 입력(영향을 주는 X)과 출력(영햘을 받는 Y)가 존재.
- 오차항을 최소로하는 직선의 방정식을 찾는 과정
- 그렇다면 다변량은?
	- 입력 변수들의 독립성 보장 (선형독립성) => 다중공선성 분석 (입력 변수들 간에 영향력 없을 것)
	- 다중 선형 회귀분석으로 다차원 공간의 선형 모델을 찾는 것
- $\beta$  회귀계수가 곧 입력변수가 종속변수에 주는 영향력의 크기 

---
##### 모델를 통한 성능
- 입력변수와 모델을 통해 데이터 변수 간의 관계를 설명한 정도

![[선형관계의강도.png]]

- SST  =  Total Sum of Squares : 총제곱합
$$
	\begin{align}
		&SST\ =\ \sum_{i=1}^{n}\ (y_{i}- \bar y)^{2} \\ \\
		&SST\ =\ SSR\ +\ SSE
	\end{align}
$$
	- 실제 관측값에서 관측값의 평균을 뺀 결과의 총합
	- 설명하고자 하는 종속 변수의 대표값(평균)으로 부터 해당 관측샘플의 변동성 크기. 즉, 평균으로 설명하지 못한 해당 샘플의 정보
	 
- SSR  =  Sum of Squares due to Regression : 회귀제곱합
$$
	\begin{align}
		&SSR\ =\ \sum_{i=1}^{n}\ (\hat y_{i}- \bar y)^{2}
	\end{align}
$$
	- ESS: Explaned Sum of Squares
	- 예측값에서 관측값의 평균을 뺀 결과의 총합.
	- 입력변수(독립변수)와 모델을 도입하여 추가로 설명할 수 있었던 종속 변수의 변동량.

- SSE  =  Sum of Squares Residual of Error: 잔차제곱합
	- RSS : Residual Sum of Squares 
	- SSR : Sum of Squared Residual
$$
	\begin{align}
		&SSE\ =\ \sum_{i=1}^{n}\ (y_{i}- \hat y_{i})^{2}
	\end{align}
$$
	- 실제 관측값과 모델의 예측값 사이의 차이인 잔차(Residual)의 총합을 뜻함.
	- 독리변수를 모델에 도입하고도 설명하지 못한 종속변수의 변동량 정보.

#### R-Squared: 결정계수 $R^{2}$

- 모델이 종속 변수의 분산을 얼마나 잘 설명하는지 나타내는 지표. 즉, 독립변수가 종속 변수를 얼마나 잘 예측하는지 나타내는 지표.

- $R^{2}$ 는 0과 1 사이에 값을 가짐. 1에 가까울수록 모델의 설명력이 높음.
$$
	\begin{align}
		&R^{2}\ =\ \frac{SSR}{SST}\ =\ 1\ -\ (\frac{SSE}{SST}) 
	\end{align}
$$

-  Adjusted R-Squre (조정된 결정계수)
$$
	\begin{align}
		&Adjusted\ R^{2}\ =\ \ 1\ -\ \frac{SSE\ \div\ (n-k-1) }{SST\ \div\ (n-1)} 
	\end{align}
$$
- 평균과 분산의 분포는 샘플의 수에 영향을 받습니다. 그중에 자유도 k (독립변수에 결정되는 분)에 대한 부분을 뺀  표본 크기의 비중으로 결정 계수를 조정해 줍니다.
- 결국 k 가 커지게 되면 나눠주는 샘플 크기의 영향이 커지고 결정계수는 작아지게 되어 독립변수의 크기가 많아 질수록 모델의 복잡도 (그 만큼 많은 설명 변수를 사용하여 복잡해짐)가 높아지는데 대한 패널티를 주는 것임.
- 따라서 항상 결정계수보다 수정결정계수는 작을 수 밖에 없음. (수정결정계수는 음수도 가능)

###### 모델의 적정성 : 생성된 모델의 설명 유의성 판별

- F 값을 사용
$$
	\begin{align}
		&F\ =\ \frac{MSR:mean\ SSR}{MSE:mean\ SSE} \\ \\ &=\ \frac{회귀식으로 \quad 설명가능한 \quad 변량의 \quad 평균}{회귀식으로 \quad 설명하지 \quad 못하는 \quad 변량의 \quad 평균} \\ \\
		&MSR\ =\ \frac{SSR}{k-1} \\
		&MSE\ =\ \frac{SSE}{k-1}
	\end{align}
$$

#### 머신러닝 관점 (학습 또는 최적화 시킬 수 있는) 에서 데이터로 최적화 시키기 : Least Square method  최소자승법 다변량식

$$
	\begin{align}
		&Y\ =\ \beta_{0}\ +\ \beta_{1}X_{1} +\ \cdots\ +\ \beta_{p}X_{p}\ +\ \epsilon \\ \\
		&SSE\ =\ \sum_{i=1}^{n}(y_{i}\ -\ \beta_{0}\ -\ \sum_{j=1}^{p}\beta_{j}x_{ij})^{2}
	\end{align}
$$
- 쉬운 설명으로는 결국 SSE을 최소로 만드는 상수항과 회귀계수항을 찾는 것이다.
- 방법은 대수적방법, 해석학적방법, 비선형적방법 등이 있다. (미분 같은 방법들)

#### 입력 변수가 증가하고 복잡도가 높아져서 변수의 선택과 입력 순서 등을 최적화 하는 방법 필요해짐.
: 이를 통해서 복잡도를 낮추고, 설명력을 높이며, 모델의 일반성을 높이고자함.
(변수의 입력 순서, 변수의 가지치기(purunning))

#### 대표적 선형회귀식의 정규화(regularization) 방법 : Shrinkage methods
##### 1) Ridge Regression model : 능형 회귀 모델   - *L2 정규화* 

$$
	\begin{align}
		&SSE\ +\ L_{2} =\ \sum_{i=1}^{n}(y_{i}\ -\ \beta_{0}\ -\ \sum_{j=1}^{p}\beta_{j}x_{ij})^{2}\ +\ \lambda\sum_{j=1}^{p}\beta_{j}^{2}
	\end{align}
$$

 - 계수의 제곱 (변동량의 크기의 제곱)의 합에 하이퍼파라메터를 곱하여 페널티를 준다.
 - 특징 : 
	 1) $\lambda$  하이퍼파라메터에 의해서 페널티의 영향력이 달라진다. 람다의 크기가 커질 수록 계수의 크기가 작아지게 된다. (더 많은 페널티)
	 2) 페널티가 커지더라도 입력 변수의 계수가 0 (zero)가 되어 해당 입력변수의 영향력이 없어지지는 않는다.

##### 2) Lasso Regression model : 올가미 회귀 모델 - *L1 정규화* 

$$
	\begin{align}
		&SSE\ +\ L_{1} =\ \sum_{i=1}^{n}(y_{i}\ -\ \beta_{0}\ -\ \sum_{j=1}^{p}\beta_{j}x_{ij})^{2}\ +\ \lambda\sum_{j=1}^{p}|\beta_{j}|
	\end{align}
$$


 - 계수의 절대값(변동량의 크기의 절대값)의 합에 하이퍼파라메터를 곱하여 페널티를 준다.
 - 특징 : 
	 1) $\lambda$  하이퍼파라메터에 의해서 페널티의 영향력이 달라진다. 람다의 크기가 커질 수록 계수의 크기가 작아지게 된다. (더 많은 페널티)
	 2) 페널티가 커지면 입력 변수의 계수가 0 (zero)가 되어 해당 입력변수를 제거하는 subset select 즉, 가지치기 효과가 발생한다.

![[L1_L2.png]]


- L2 가 입력 변수들의 상관관계가 높은 상황에서 좋은 성능이 나오는 경향이 있음.
- L2 크기가 큰 변수를 우선적으로 줄이는 경향이 있음.
- L1 은 모델의 공역 범위의 변화가 예측 오차에 민감하게 변함
- L2 는 상대적으로 안정적으로 모델이 변함

### 분석 연습 
라이브러리를 사용한 방법 (직접 수식을 사용하는 방법 제외)

```python
#라이브러리 불러오기  
# !pip install -U statsmodels  
￼￼L2 정규화￼￼ 
import statsmodels.api as sm  
import statsmodels.formula.api as smf  
  
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import statsmodels.api as sm

def make_data(w=0.5, b=0.8, size=50, noise=1.0):
    x = np.arange(size)
    y = w * x + b
    noise = np.random.uniform(-abs(noise), abs(noise), size=y.shape)
    yy = y + noise  # 노이즈 추가

    return x, yy

def OLS(x, y):
    df = pd.DataFrame(x, columns=["x"])
    df['y'] = y
    df['intercept'] = 1
    model = sm.OLS(df['y'], df[['intercept', 'x']])
    results = model.fit()
    print(results.summary())

    res = results.params['x']*x + results.params['intercept']
    plt.figure(figsize=(10, 7))
    plt.plot(res, color='red')
    plt.scatter(x, y)
    plt.suptitle("OLS")
    plt.title('y = ' + str(round(results.params['x'], 3))+'*x+' + str(round(results.params['intercept'], 3)))
    plt.show()

    return results.params['x'], results.params['intercept']

beta = 0.8
alpha = 2

x, y = make_data(size=100, w=beta, b=alpha, noise=6)
OLS(x, y)

# w: 0.82
# b: 0.19
```

```python
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression

def make_data(w=0.5, b=0.8, size=50, noise=1.0):
    x = np.arange(size)
    y = w * x + b
    noise = np.random.uniform(-abs(noise), abs(noise), size=y.shape)
    yy = y + noise  # 노이즈 추가

    return x, yy

def LR(x, y):
    model = LinearRegression()
    model.fit(x.reshape(-1, 1), y)
    res = model.coef_[0]*x + model.intercept_
    print('w: {:.2f}, b: {:.2f}'.format(model.coef_[0], model.intercept_))

    plt.figure(figsize=(10, 7))
    plt.plot(res, color='red')
    plt.scatter(x, y)
    plt.suptitle("LR")
    plt.title('y = ' + str(round(model.coef_[0], 3))+'*x+' + str(round(model.intercept_, 3)))
    plt.show()
    return model.coef_[0], model.intercept_

beta = 0.8
alpha = 2

x, y = make_data(size=100, w=beta, b=alpha, noise=6)
LR(x, y)

# w: 0.82
# b: 0.19
```

https://todayisbetterthanyesterday.tistory.com/14
#### Lasso
```python
import numpy as np 
import pandas as pd 
from sklearn.model_selection 
import train_test_split 
from sklearn import metrics 
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve 
from sklearn.linear_model import Ridge, Lasso, ElasticNet 
import statsmodels.api as sm 
import matplotlin.pyplot as plt 
import itertools import time 

ploan = pd.read_csv("./Personal Loan.csv") 
ploan_processed = ploan.dropna().drop(['ID','ZIP Code'],axis=1,inplace = False) 

feature_columns = list(ploan_processed.columns.difference(["Personal Loan"]) 
X = ploan_processed[feature_columns] 
y = ploan_processed['Personal Loan'] # 대출여부 1 or 0 
					   
train_x, test_x, train_y, test_y = train_test_split(X,y,stratify = y, train_size =0.7, test_size =0.3, random_state = 42)

print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)
```

```python
# Lasso 적합 
ll = Lasso(alpha = 0.01 ) # alpha = Lambda 
ll.fit(train_x,train_y)
```

```python
##### Lasso와 Ridge의 예측 정확도를 측정하는데 사용할 것이다. 
# 0/1 cut-off(임계값) 함수 
def cut_off(y, threshold): 
	Y = y.copy() 
	Y[Y>threshold] = 1 
	Y[Y<=threshold] = 0 
	return (Y.astype(int)) 
# 정확도 acc 함수 
def acc(cfmat): 
	acc = (cfmat[0,0] + cfmat[1,1]) / np.sum(cfmat) 
	return acc
```


```python
pred_Y_lasso = ll.predict(test_x) 
pred_Y_lasso = cut_off(pred_Y_lasso, 0.5) 
cfmat = confusion_matrix(test_y,pred_Y_lasso) 
print(acc(cfmat))
```

```python
fpr,tpr, thresholds = metrix.roc_curve(test_y,pred_Y_lasso,pos_table = 1) 
# print ROC curve 
plt.plot(fpr,tpr) 
# print AUC 
auc = np.trapz(tpr, fpr) 
print("AUC :",auc)
```


#### Ridge
```python
# Lasso 적합 
rr = Ridge(alpha = 0.01 ) # alpha = Lambda 
rr.fit(train_x,train_y)
## ridge result 
print(rr.coef_)

## ridge y 예측, confusion matrix, acc 계산 
## 예측, confusion matrix, acc 계산 

pred_Y_ridge = rr.predict(test_x) 
pred_Y_ridge = cut_off(pred_Y_ridge,0.5) 
cfmat = confusion_matrix(test_y,pred_Y_ridge) 
print(acc(cfmat))

## Ridge AUC, ROC Curve 
fpr, tpr, thresholds = metrics.roc_curve(test_y,pred_Y_ridge, pos_label=1) 
# print ROC curve 
plt.plot(fpr,tpr) 
# print AUC 
auc = np.trapz(tpr,fpr) 
print("AUC :",auc)

```

```python
# lambda값 지정 
# 0.001 <= lambda <= 10 
alpha = np.logspace(-3,1,5) 
print(alpha)

data = [] 
acc_table = [] 

for i, a in enumerate(alpha): 
	lasso = Lasso(alpha=a).fit(train_x, train_y)
	data.append(pd.Series(np.hstack([lasso.intercept_, lasso.coef_]))) 
	pred_y = lasso.predict(test_x) # full model 
	pred_y = cut_off(pred_y, 0.5) 
	cfmat = confusion_matrix(test_y,pred_y) 
	acc_table.append((acc(cfmat))) 
	
df_lasso = pd.DataFrame(data,index = alpha).T 
acc_table_lasso = pd.DataFrame(acc_table, index = alpha).T

print(df_lasso)
print(acc_table_lasso)
```

```python
data = [] 
acc_table = [] 

for i, a in enumerate(alpha): 
	ridge = ridge(alpha=a).fit(train_x, train_y)
	data.append(pd.Series(np.hstack([ridge.intercept_, ridge.coef_]))) 
	pred_y = ridge.predict(test_x) # full model 
	pred_y = cut_off(pred_y, 0.5) 
	cfmat = confusion_matrix(test_y,pred_y) 
	acc_table.append((acc(cfmat))) 

df_ridge = pd.DataFrame(data,index = alpha).T 
acc_table_ridge = pd.DataFrame(acc_table, index = alpha).T

print(df_ridge)
print(acc_table_ridge)

```

```python
import matplotlib.pyplot as plt 

ax1 = plt.subplot(121) 
plt.semilogx(df_ridge.T) 
plt.xticks(alpha) 
plt.title("Ridge") 

ax2 = plt.subplot(122) 
plt.semilogx(df_lasso.T) 
plt.xticks(alpha) 
plt.title("Lasso") 
plt.show()
```


- 모델의 단순성
https://aliencoder.tistory.com/24