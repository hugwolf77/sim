---
categories: 수업
title: 2.1.5. 요인 분석(Factor Analysis)
created: 2024-09-22
tags:
  - 수업
  - 교재
  - 요인분석
---
---
#### 2.1.5. 요인 분석(Factor Analysis)
---

#### 데이터의 정규화(normalization) 또는 표준화(standardization)
##### 필요성
1) 이상치 처리
2) 변수 간 scale 차이 해결
3) 모델의 수렴 속도 향상
4) 고차원 데이터의 문제 해결

##### 방법
1) 표준화 - standarization
2) 정규화 - normalizer
3) 표준화 - min-max scaler

```python
from sklearn.preprocessing import StanardScaler
std = StandardScaler()
std_data = std.fit_transform(data)

from sklearn.preprocessing import Normalizer
nor = Normalizer()
nor_Data = nor.fit_transform(data)

from skliarn.preprocessing import MinMaxScaler
mm = MinMaxScaler()
mm_data = mm.fit_transform(data)
```

#### 요인 분석
###### 일반적인 설명
 측정 된 데이터의 변수(또는 특성:feature)의 수가 많아 데이터의 표현이 너무 높은 차원일 때, 이들을 상위의 개념(메타개념) 또는 함축적인 요인(Factor or Latant Factor)로 묶어 적은 수의 차원으로 정보를 추출, 압축, 분석하는 방법을 말한다.

- 일반적으로 "차원 감소" 방법을 사용하여 다량의 데이터 차원에서 함축적인 개념의 데이터 차원으로 데이터가 가진 정보를 함축한 변수(또는 요인)으로 변환하고 그 과정에서 데이터를 분석하는 방법

- 통계적 연구 방법에서는 **탐색적 요인분석(EFA: Exploratory Factor Analysis)** 과 **확인적 요인분석(CFA: Confirmatory Factor Analysis)** 두 가지 존재한다. 일반적으로 요인분석이라고 하면 탐색적 요인분석을 뜻한다. 확인적 요인분석은 이미 연구가설에 의한 모델의 "함수식"이 존재하고 이에 대하여 해당 데이터의 변수(특성:feature)들이 그 "함수식"에 적정하게 요인으로 입력되었는지 검증하는 분석이다. (연구가설 모델에 사용된 데이터의 적정성 검증)

###### 요인 추출

#### 요인 분석의 용도 (탐색적 분석을 기준으로)
1) 데이터 특성 변수들의 차원을 줄여 정보를 요약(함축)하는데 사용
2) 데이터의 특성 변수들 내부에 존재하는 구조를 파악
3) 요인으로 묶어지지 않는 변수 중 중요도가 낮은 데이터의 특성 변수를 제거 할 수 있음.
4) 상위 개념을 측정하고자 여러 관점 또는 척도로 측정된 데이터가 적정하게 해당 개념을 측정하였는지 검증.
5) 요인분석을 통하여 얻어진 요인을 사용하여 분석 모델의 효율성과 성능을 높이고자 할 때. 

#### 요인 분석 방법
##### 주성분 분석(PCA: Principle component analysis)
 - 선형대수적 데이터의 정보 반영 정도(분산의 설명력) 정도를 기준으로 독립적인(직교) 차원의 요인으로 정사형(projection)하여 차원을 감소 시키는 방법
```python
from sklearn.decomposition import PCA, FactorAnalysis
pca = PCA(n_componets= k)
df_pca = pca.fit_transform(data)
```

##### 최대우도법(MLE: Maximum Liklihood Estimatation)
- 데이터의 확률분포를 이용하여 데이터가 표현하는 정보의 분포가 가장 높게 나타나는 데이터 분포 차원으로 데이터를 표현해 주는 방법

**-> 관련 분석법에 더해서 별도로 다뤄야 할 것으로 보임. 

> [!요인 분석과 차원 감소]
>  !!! PCA 방법을 사용하여 요인 분석을 하지만 PCA 와 요인 분석은 다르다. PCA 는 데이터의 정보를 최대로 나타내고자 한다면, 요인 분석은 공분산의 정보를 최대로 나타내고자 한다.
> 1) 빅-데이터 분석이 요구되고 있는 최근의 데이터 분석 환경에서 너무 많은 특성 변수는 분석을 어렵게 한다. 타라서 요인분석의 필요성이 더욱 필요로 해지고 있다.
> 2) 또한 컴퓨터 자원사용에 있어서도 고차원의 특성 데이터는 메모리와 고차원 행렬계산의 어려움으로 분석모델의 성능을 떨어뜨리는 주요한 원인이된다.
>  


- https://scikit-learn.org/stable/auto_examples/decomposition/plot_varimax_fa.html
```python
# Authors: Jona Sassenhagen
# License: BSD 3 clause

import matplotlib.pyplot as plt
import numpy as np

from sklearn.datasets import load_iris
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.preprocessing import StandardScaler

data = load_iris()
X = StandardScaler().fit_transform(data["data"])
feature_names = data["feature_names"]

ax = plt.axes()
im = ax.imshow(np.corrcoef(X.T), cmap="RdBu_r", vmin=-1, vmax=1)
ax.set_xticks([0, 1, 2, 3])
ax.set_xticklabels(list(feature_names), rotation=90)
ax.set_yticks([0, 1, 2, 3])
ax.set_yticklabels(list(feature_names))
plt.colorbar(im).ax.set_ylabel("$r$", rotation=0)
ax.set_title("Iris feature correlation matrix")
plt.tight_layout()
```

```python
n_comps = 2

methods = [
    ("PCA", PCA()),
    ("Unrotated FA", FactorAnalysis()),
    ("Varimax FA", FactorAnalysis(rotation="varimax")),
]
fig, axes = plt.subplots(ncols=len(methods), figsize=(10, 8), sharey=True)

for ax, (method, fa) in zip(axes, methods):
    fa.set_params(n_components=n_comps)
    fa.fit(X)

    components = fa.components_.T
    print("\n\n %s :\n" % method)
    print(components)

    vmax = np.abs(components).max()
    ax.imshow(components, cmap="RdBu_r", vmax=vmax, vmin=-vmax)
    ax.set_yticks(np.arange(len(feature_names)))
    ax.set_yticklabels(feature_names)
    ax.set_title(str(method))
    ax.set_xticks([0, 1])
    ax.set_xticklabels(["Comp. 1", "Comp. 2"])
fig.suptitle("Factors")
plt.tight_layout()
plt.show()
```


##### 요인 적재값(factor loading)
- 요인 추출을 방법을 사용하여 감소한 차원에서 원 데이터가 가지는 정보량

##### 요인 회전(rotation)
- 추출된 요인이 더욱 잘 들어나도록 선형 변환을 이용하여 함축되는 차원을 회전시키는 해석방법

	1) 직교 회전(orthogonal rotation) : 추출될 요인 간의 상호 독립성을 전제하는 회전법
		- VARIMAX : 인자 적재값 제곱의 분산이 최대가 되도록 회전
		- QUARTIMAX : 인자 적재값이 차지하는 비중(상대적 크기)의 제곱에 대한 분산

	2) 사각 회전(oblique rotaton) : 추출될 요인 간의 상호 관계성을 염두해둔 회전법
		- Direct Oblimin 
		- PROMAX
<center>
	<img src="https://postfiles.pstatic.net/MjAyMDAyMTBfMTM5/MDAxNTgxMjg2NDk3MzI0.t-20_HcZr6Lorm-EchjhkjpjH9mWQuqxD_X8H0jYANQg.QR9gBVlmQkmkkjivye9pNNmP517Te8-BQn8xe2RLHr8g.PNG.shoutjoy/image.png?type=w773" width = 400 height =200>
</center>
출처 : [자유자재 paper](https://blog.naver.com/PostView.naver?blogId=shoutjoy&logNo=221802826087)  -  좋은 블로그 인지는 아직 살펴보지 않아서 모르겠음.

##### 요인 점수(factor score)
-  요인 적재값에 요인회전을 적용하였을 때, 추출되는 요인의 점수 

#### 요인 분석 이후
- 추출된 요인을 사용하여 상관계수 등을 분석하여 새롭게 추출된 요인의 상관 관계를 확인한다.
- 분석하고자 하는 목표 특성(현상)에 대한 분석 모델의 입력 변수로 사용 가능한지 확인한다.


#### 요인 분석 python 사용

##### scikit-learn decomposition PCA

##### factor_analyzer FactorAnalyzer
 data : [vincentarelbundock.github.io/Rdatasets/datasets.html](https://vincentarelbundock.github.io/Rdatasets/datasets.html)
```python
!pip install factor-analyzer 

import pandas as pd 
from factor_analyzer import FactorAnalyzer 
import matplotlib.pyplot as plt 

df = pd.read_csv('bfi.csv', index_col=0) 
df.columns df.drop(['gender', 'education', 'age'],axis=1,inplace=True) df.dropna(inplace=True) df.info()
```

- Bartlett 검정 : 귀무가설 "상관관계 행렬이 단위행렬" 을 기각 시켜야함. (대립가설 채택)
- KMO(Kaiser-Meyer-Olkin) 검정: 일반 통계 프로그램에서는 KMO 검정을 같이 사용함. 변수들 간의 상관관계가 요인분석에 적정한지 봄. 일반적으로 KMO 점수가 최소 0.6 이상이어야 한다.)
```python 
from factor_analyzer.factor_analyzer import alculate_bartlett_sphericity chi_square_value,p_value=calculate_bartlett_sphericity(df) print(chi_square_value)
print(p_value)

from factor_analyzer.factor_analyzer import calculate_kmo kmo_all,kmo_model=calculate_kmo(df) 
print(kmo_model)
```

- 요인 수 선택
```python 
fa = FactorAnalyzer(n_factors=25,rotation=None)
fa.fit(df)

ev,v = fa.get_eigenvalues()
print(ev)

plt.scatter(range(1,df.shape[1]+1),ev) plt.plot(range(1,df.shape[1]+1),ev) plt.title('Scree Plot') plt.xlabel('Factors') plt.ylabel('Eigenvalue') plt.grid() plt.show()
```

```python
fa = FactorAnalyzer(n_factors=6, rotation="varimax") #ml : 최대우도 방법
fa.fit(df) 
efa_result= pd.DataFrame(fa.loadings_, index=df.columns) 
efa_result
```

```python
plt.figure(figsize=(6,10)) sns.heatmap(efa_result, cmap="Blues", annot=True, fmt='.2f')
```


###### 신뢰계수 크로바흐 알파 계산 : 내적 일관성 신뢰도
```python
# 신롸도 계수를 구하는 함수 
def CronbachAlpha(itemscores): 
	itemscores = np.asarray(itemscores) 
	itemvars = itemscores.var(axis=0, ddof=1) 
	tscores = itemscores.sum(axis=1) 
	nitems = itemscores.shape[1] 
	return (nitems/(nitems-1))*(1-(itemvars.sum()/scores.var(ddof=1)))
```

```python
factors = ['A', 'C', 'E', 'N', 'O'] 
factors_items_dict = {} 
for factor in factors: 
	factors_items_dict[factor]=[x for x in df.columns if x[0] == factor] 
	factors_items_dict
for key, value in factors\_items\_dict.items(): 
	print(key) 
	print(CronbachAlpha(df\[value\])) 
	print()
```

```python
import pingouin as pg
pg.cronbach_alpha(data=data)
```



---


- 아주 많고 복잡한 내용도 기억의 궁전에 배치를 상호관계적으로 배치하면 모두 기억할 수 있다. <미드 : mentalist>
<center>
<img src="https://i.namu.wiki/i/ZOaenGmicNgPLmWxmFs9FEJz3Hzg2jdVdc6OAIOkCD8ZVyZ_02RkO8n8xRClWPe4e5QoE0egXkFwy-2pBo1L8jklyJE3grC2bb2UhJI_ffPmaEgAzJUlk8g-POJD9SPLPYAJS_nvGm1awA7WQ5EMgg.webp" width = 150>
</center>

- 공감각적(인간이 가진 오감과 그 상호작용)을 이용하여 기억의 실마리들의 복잡한 네트워크를 만드는 것

---
##  데이터 변수: Variance, Feature 에서 부터 평균과 분산에서 확률분포까지

#### 독립적 존재, 개념. 특성을 측정해 모와 놓은 것.

- 독립이라는 존재의 독립된 정보의 순수성이란? 그리고 그 범위란?

- 완전히 독립된 무차별한 독립체 범위 내의 정보는 정규분포 형태이며 무질서 하다.

- 무작위한 움직임, 가장 무작위한것이 가장 포편적인 움직임을 보인다. 

<center>
<img src="https://velog.velcdn.com/images%2Fyunyoseob%2Fpost%2F17e26465-c7d9-4707-a596-458ce7462b46%2Fimage.png" width = 200>
</center>

- 경우의 수 => 하나의 측정의 1의 독립성 (정수) => 팩토리얼 (descrete 빈도 경우: Sigma $\sum : 경우의 합 $)=> gamma 함수 (continuos 전 실수로 확대: $ integration \int : 적분$) 

#### 독립된 개별 정보의 범위 => 순도 (또는 그 특성에 대한 보편적인, 진리의)

- 일반적인 평균은 descrete 한 측정 샘플의 구간별 빈도의 산술적 평균을 의미한다.

- 확률분포함수 또는 확률분포(가우시안 => 정규분포)는 연속적인 형태를 나타낸다.

- 지금까지 말한 것은 모두 의미론적 변동량의 안정된 유의미한 시공간에서의 측정을 의미한다.

- 따라서 평균이란 것은 측정하고자 하는 정보 또는 개념, 특성의 의미론적 해석이 중요하다. 평균 소득과 소득의 분포. 시계열 데이터의 디스카운트 팩터 등...

- 특성의, 정보의 순수성, 순도 다른 뜻으로는 대표성을 나타낸다.

- 클라스의 구분과 정보를 전달하는데 필요한 노력 => 정보 Entropy

- 독립적으로 각 클래스의 확률이 같다면 Entropy 증가. 순수정도가 같은 독자적인 개별 정보이기 때문에 혼돈 증가

- 각각의 개별 샘플 클라스의 경우에서 범쥐 또는 분포로 생각해 보자.

- 순도가 높다는 것은 독립된 개념, 특성이라는 뜻. 이를 분리해 낼때 정보를 획득하고 불확실성 Entropy 감소

- 여집합의 개념 (이항의 개념 : 이항 확률 분포) => combination(permutation: 조합) 조건적 (클라스 존재) 경우의 수 => beta 함수 (전 실수로 확대)

- 여집합을 하나의 집합으로 놓으면 전체는 두개의 집합이 된다. (두개의 집합 이외의 고려한 여집합은??) => 베이지안과 연계해서 생각해보면??

- 연계해서 confusion matrix를 생각해 볼수도 있다. (1형 오류와 2형 오류) 측정의 descrete 한 빈도의 경우의 수 확률 => 연속형 ROC => 면적으로 수치화 AUC

- 따라서 독립된 개념을 측정하였다면, 특별한 작용을 부여한 개념이나 정보가 아닌 이상은 정규분포 형태를 따른다. 정규분포의 특징은 좌우 대칭에 가우시안 분포 모양이다.

- 그렇지 않은 경우는 크게 2가지이다.

- 첫째, 독립된 하나의 개념이 아니다. (내부에 새로운 영향 요인이 존재 한다. 순도가 떨어짐)

- 둘째, 충분한 양의 샘플 측정이되어 지지 않아서 분포를 찾지 못하였다.

- 일반적으로 우리는 충분한 샘플을 확보하지 못하거나, 충분히 무작위적인 샘플링을 (어떠한 영향을 받은 상태에서의 샘플을 취하기 (selective bias) 때문이다.) 하지 못하기 때문이다.

- 역으로 생각할 수도 있다. 우리가 어떠한 현상을 구분하거나 예측하고자 할때 종합적인 영향관계를 반영하고자 할때, 하나의 개념 또는 특성에 다른 특성을 섞어서 새로운 혼합된 특성을 측정하기도 한다. (베이즈 정리를 생각해 보자)

#### 독립적인 범위 내에서의 분포적 특성

- 일정한 표준적 순도의 범위를 분산이라고 생각해 보자. 평균과 같이 분산도 지극히 의미론적인 개념이다.

$$\mu = \frac{1}{N}\sum_{i}^{N}x_{i} \quad \sigma^{2} = \frac{\sum(x_{i}-\mu)^{2}}{n}$$

$$\bar x = \frac{1}{N}\sum_{i}^{N}x_{i} \quad s^{2} = \frac{\sum(x_{i}-\bar x)^{2}}{n-1}$$

- 개념, 특성에 대해서 얼마나 흩어져 있느냐의 일반적 표준적 거리를 의미

- 이러한 특성(개념)의 일반화와 표준적 수치화를 통해서 정규화(Normalize)를 한다.

- 먼저 표준적이란 무엇인가 생각해 보자. Norm을 기억하는가 각 벡터 방향의 변화량 단위 또는 기저라고 할 수 있다. 이처럼 데이터의 측정 단위 즉, 변화단위를 일반화된 표준으로 나타는 작업이기 대문에 scaling 이라고도 하였다.

- 이때 기준점을 정규분포와 같이 0점과 측정단위에서 정규분포 단위인 1~0 사이의 값 즉, 1~0 사이 한 단위의 기준으로 변환하는 것을 뜻한다.

- 이때 조심해야 하는 것은 모델화 할 때 원 변수(특성)이 가진 측정의 단위(scale)을 잃어 버린다는 것이다. (정성적 정보가 사라짐.)

- 따라서 사실 진정한 예측에서는 상수항을 살리는 관계식으로 예측하여야(상수항의 의미가 살아 있으려면 스케일 단위가 살아 있어야 한다. 의미적 결합에서) 한다. 그러나 제어계측적 관점에서는 측정과 통제를 위한 조정의 관점에서는 새로운 단위로의 변화도 의미가 있다.  단 필요 용도에 따라서 맞는 (label 또는 통제 단위) 단위로 변환하여야 한다.

---
##  연구의 요인 분석 과정

- 연구의 과정에서의 변수와 요인

- 연구배경(연구개기) -> 문헌연구 -> 개념정의 -> 선행연구 분석 (개념 간의 관계) -> 조작적 정의 (측정할 수 있는 개념화) -> 측정 척도 설계 -> 측정  

- 측정된 데이터에 대해서 원하는 요인이 측정되었는지 확인하는 법

- 측정에 대한 판정 1) 신뢰성 2) 타당성

- 신뢰성 측정이 샘플에 대해서 일관성 있게 측정되는가.

- 타당성 측정하고자 하는 개념을 측정하였는가.


##### 신뢰도 측정

- 신뢰성을 측정하는 개념과 방법은 여러가지가 있다.  (반복적 재조사법, 복수 양식법, 같은 구릅을 반씩 조사하는 반분법 등) 또한 실험설계와 연구방법이나 조사방법 설계에 따라서도 달라진다. 가장 일반적으로 이미 있는 데이터를 평가하는 경우 내적 일관성을  평가하는 크롬바흐 알파를 기본으로 널리 사용한다.

- 크롬바흐 알파 => cross validation을 떠올려 보라

- 하나의 요소를 측정하기 우해 측정한 여러 척도가 하나의 요소를 일관되게 측정하였는지 보는 것, 즉, 측정 도구가 일관되는지 보는 것이다.

$$\alpha = \frac{K}{K-1}(1-\frac{\sum_{i=1}{\sigma_{x_{i}}^{2}}}{\sigma_{T}^{2}})$$

##### 타당도 측정 

- 타당도의 종류

(1) 내용 타당도(content validity) : 측정항목이 측정하고자 하는 영역의 내용에 관한 적절한 샘플인지 나타냄.

(2) 준거 타당도(criterion-related validity) : 측정항목이 샘플이 범위를 넘어 얼마나 일반적인 모수에 대하여도 의미가 있는지

(3) 구성 타당도 (construct validity) : 측정하고자 하는 요인에 대한 측정 항목으로 구성되었으며 정확하게 모델화하여 측정하였는가를 의미함 (모델의 적정성을 나타내는 model fit과는 다른 개념이다.). 구성타당도는 다시 

    a. 수렴 타당도(convergent validity) : 이론적으로 관계있는 구성개념과의 상관성
    b. 변별 타당도(discriminant validity) : 이론적으로 관계 없는 구성개념과의 상관성 

 구성 타당도는 주로 확인적 요인 분석에서 확

### 요인분석 : 측정 항목들로 부터 소수의 의미 있는 잠재변수 (latent variable)를 발견해내거나 만들어 내는 통계적 과정


##### 확인적 요인분석(Comfirmatory Factor Analysis:CFA) 

- 구조방정식모형을 사용하여 탐색적 요인분석(EFA; exploratory factor analysis)
으로 확인된 잠재요인을 사용하였을 때 탐색적 요인분석이 가진 약점 즉, 연구자의 가설을 구조화한 구조방정식모델에 정말 추출된 요인이 적정했는가를 판단하는 분석 방법이다. 
- 이에 사용되는 지표로 먼저
- 다음으로평균분산추출지수 AVE(average variable extraction) 구하여 (측정 항목 적재값과 측정 에러의 일정 비율로 계산)를 구하여 잠재요인에 설명력이 일정 기준 이상을 나와야 하며, 이 AVE 의 제곱값을 다른 잠재요인 간의 상관관계 보다 높게 측정되어 자기 자신을 설명하는 측정 항목들이 다른 항목들에 의해서 설명되는 자신보다 높게 평가 되어 "판별 타당도"를 측정하게 된다.
- 또한 구조모형에서는 크롬바흐 알파와 다른 신뢰도인 개념 신뢰도(Composite/Construct reliability)도 측정한다.

> 탐색적 요인분석(EFA; exploratory factor analysis)의 한계
>
> 1. 잠재 요인으로 추출된 요인들 사이에 대해서 모두 상관관계(causality: 인과성)가 있는 것으로 가정하거나, 또는 없는 것으로 가정하고 분석해야 한다. (사각회적(구조적 상관관계 모두 존재, p), 직각회적(구조적으로 요인간 독립관계라 가정))
> 2. 잠재요인에 측정된 항목은 모두 적재된것으로 가정하여야 함. (실제로는 문제 있을이 신뢰도와 연구목적에 문제되지 않는 다면 제거하기도 함)
> .

그러나 머신러닝 등의 기계학습에서는 확인적 요인분석을 사용하는 경우는 적으며, 탐색적 요인분석을 주로 진행한다. 특희 구성타당도를 위주로 본다.

### 탐색적 요인 분석 (EFA; exploratory factor analysis) 주요한 개념 

| 개념               | 내용                                                                                                                                                                                                   |
| ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 샘플(데이터)의 수       | 최소 200 정도 이상 300 이상 (정규성 확보)                                                                                                                                                                         |
| 정규성 검정           | 수집된 데이터가 정규성을 보이는가 왜도 < 0.2, 첨도 <=7                                                                                                                                                                  |
| 요인분석 적절성 지표      | KMO(Kaiser-Meyer-Olkin test) 0.7 이상이 보통, Bartlett p<0.05 (상관행열이 단위행렬이 아님)                                                                                                                            |
| 공통성(Communality) | 전체 데이터에 대해서 측정항목이 가지는 설명력 비율 (보통 0.4 이상. 절대는 아님)                                                                                                                                                     |
| 요인의 추출법          | PCA(주성분분석법)와 MLE(최대우도법)이 주로 많이 사용됨.                                                                                                                                                                  |
| 요인의 판별           | Kaiser 규칙으로 고유값(eigan-value) 1 이상의 요인을 추출된 요인으로 보지만 정확한 기준은 아니다(1이 넘어도 의미 없을 수 있음), scree 도표 사용. 총분산(총부하량) 잠재요인으로 추출되었다고 판단되는 요인의 설명력 분산을 합하여 0.5 이상 되어야 함                                           |
| 교차 부하량           | 요인들에 로딩된 각 측정 항목의 부하의 차이가 0.3 이상을 좋다고 봄. 선행 가정에서 개념적으로 소속된 측정 항목이 교차부하로 뚜렷하게 구분되는 것을 교차타당성으로 보기도 한다                                                                                                  |
| 요인회전             | 새로운 좌표계로 측정항목들을 배열하기 위하여, 행렬곱을 이용한 회전을 의미한다. 이때 각 잠재요인의 구조적 관계성이 있다고 보아야 하면 사각회전(oblique rotation 예: 직접 오블리민)을 사용하며, 임의적 또는 개념적으로 완전히 잠재요인 간에 독립적이라고 가정하면 직교회전(orthogonal rotation: 예 varimax)을 실시한다 |


## PCA

#####  공분산 (Covariance)

- x, y 두 변수의 공분산 식은 다음과 같다
$$Cov(x,y) = \frac{\sum_{i=1}^{n}(x_{i}-\bar x)(y_{i}-\bar y)}{n}$$
$$\rho(x,y)=\frac{Cov(x,y)}{\sigma_{x}\cdot\sigma_{y}}$$
- 공분산은 각 변수의 변화량의 상호작용의 평균이라면 상관계수는 이를 각 변수의 표준편차의 곱으로 나눠서 정규화 한것이라고 볼 수 있다.
- 다시 말해서 변수들의 함께하는 변동성의 크기의 평균을 공분산은 나타내고 상관계수는 이 크기를 정규화하여 -1 과 1 사이의 값으로 나타낸 것이다.

#####  공분산 행렬 (Covariance matrix)
-  대각은 각 변수 자신의 분산을 나타내며 교차하는 지점은 해당 요인 간의 공분산을 나타내는 행열을 의미한다. 즉, 변수들의 자신과 다른 변수들 사이에서의 변화량을 나타내는 행렬이 된다.

$$
\sum = 
\begin{pmatrix}
     var(x) & Cov(x,y) \\
     Cov(x,y)& var(y) 
\end{pmatrix}
$$


##### 고유값 분해(Eigen-value Decompose)
- 먼저 수식적으로 살펴 보면, 정방행렬 A 에 대하여 다음의 식을 만족하는 벡터 $v$를 고유벡터(eigen-vector)라하고 $\lambda$를 고유값(eigen-value)이라고 하며, 이 둘을 찾는 작업을 고유값 분해라고 한다.
$$Av\ = \lambda v$$
$$
\begin{pmatrix}
     a_{11} &\dots & a_{1n} \\
     \vdots &\ddots& \vdots \\
     a_(n1) &\dots & a_{nn} 
\end{pmatrix}
\begin{pmatrix}
     v_{1}  \\
     \vdots  \\
     v_{n}  
\end{pmatrix}
= \ \lambda \begin{pmatrix}
     v_{1}  \\
     \vdots  \\
     v_{n}  
\end{pmatrix}
$$
- 아래를 만족하는 $v(\neq0)$ 가 있기 위해서는 $det(A-\lambda I) = 0$ 가 만족되어야 한다. 이를 특성 방정식이라 한다.
$$
Av - \lambda v = 0\ (0:영행렬) \\
(A - \lambda I)v = 0\ (I:단위행렬)
$$
- 선행대수의 행렬의 곱은 선형변환이다 즉, 기존의 값을 새로운 위치로 변환 시킨단 뜻이다.
- 고유벡터(eigen-vector)는 행렬 A에 의해서 선형변환되어도 그 벡터의 방향이 변하지 않는 벡터를 의미한다.
- 고유값(eigen-value)는 A에 의해서 선형변환된 벡터의 크기의 변화를 비율을 나타내는 값이다.
- 즉, 고유벡터에 대해서 A 행렬로 선형 변환하고 고유값 만큼 크기를 변환을 한 것과 같다.

- 직사각형 행렬에 대한 분해인 특이값 분해도 알아보길 바란다.

### 공분산 행렬에 대한 eigen-value decompose 실행한다면?

- 위의 x,y 두 변수에 대한 고유값 분해를 나타내보면 다음과 같다.
$$
\begin{pmatrix}
     var(x) & Cov(x,y) \\
     Cov(x,y)& var(y) 
\end{pmatrix}
v\ =\ \lambda\ v
$$

- 앞에서 살펴본 고유벡터(eigen-vector)의 성질에 의해서 고유벡터의 방향은 변하지 않는다. $var$가 변수 자신의 분산이라면 $Cov$는 변수 간에 함께 변한 변화량이다 이러한 변화를 선형변환이라고 보았을 때 그 방향이 바뀌지 않으면서 단지 고유값(eigen-value) 비율의 만큼 크기가 변하는 벡터인 것이다.
- 다시 말해서 고유벡터를 구할 수 있다는 것은 변수의 변화량과 변수들 간의 공통변화량을 나타내는 벡터를 찾을 수 있다는 것이다. 
- 고유값 분해에서는 $Acv=\lambda cv$와 같이 이때 고유벡터는 하나가 아니라 여러개가 될 수 있으나 단위 1로 정규화한 단위벡터를 고유벡터로 많이 사용하며, 분산의 형태인 공분산 행렬에서도 마찬가지 이다.
$$\frac{v}{||v||}$$
- 따라서 $\lambda$ 고유값은 변동량 (분산)의 정규화된 단위벡터의 비례이기 때문에 고유벡터로의 변화된 각 변수의 상대적 비율이 된다. 
- 이를 가지고 전체 데이터에 대한 설명력과 대표성을 표시할 수 있고 의미 없는 비율의 요소는 제거할 수 있게 된다.


- 좀더 시각적으로 PCA -  요인 결정 방법을 살펴보면 다음 그림과 같다.

<center>
<img src="https://miro.medium.com/v2/format:webp/1*D87pCZmnWKuwkNw5a2qe5w.jpeg" width = 600>
</center>

- 정사영(projection)을 통해서 각 측정 항목들이 가장 많은 분산을 표현할 수 있는 축을 찾아 낸다. 이를 1 component 로 설정한다.

- 이후 직교성 (Orthogonality)을 가지는 즉, 벡터의 내적이 '0' 인 방향으로 새로운 component 찾게 된다.

- 이때 지교랑 1 원소와의 상관관계성이 없는 독립적인 요소를 찾기 위해서이다. 이는 축소되어 함축된 새로운 요인들 간의 독립성을 확보하기 위해서 이다.


### 일반적인 요인분석 (Factor Analysis)와 PCA의 차이점

<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F261C1033554108B810" width = 600 >
</center>

- 계산적 차이는 PCA는 공분산을 이용하여 측정 항목들 간의 상관관계의 공통의 분산을 통해 요인을 계산한다.
- 요인 분석은 각 측정 항목의 고유 분산을 사용하여 인과적 영향력이 요인에 얼마나 되는지 보는 것이다.


---
## Fourier Transform 

[Reference: GongbroDesk](https://www.youtube.com/watch?v=wpHWGuof2nE)

[참고: Veritasium](https://www.youtube.com/watch?v=eKSmEPAEr2U&t=1260s)

  푸리에 변환 

<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcysI8F%2FbtracO3QmLV%2Fod6lhTlCWlKrl9QiLEtPH0%2Fimg.png" width = 500>
</center>


 $$\frac{Time}{Cycle}=Period[s] \quad \frac{Cycle}{Time}=\frac{Cycle}{1 second} = Hz$$

$$
    \hat f(\omega) = \int_{-\infty}^{\infty} f_{t}e^{+i\omega t}dt
$$

---
- 푸리에 급수 Fourier Series

    : 어떤 함수에 대하여 (주기성 함수) 정현파의 무한한 합으로 근사할 수 있다.
$$
    \hat f(x) = \frac{a_{0}}{2}+\sum_{n=1}^{\infty}a_{n}cos(\frac{2\pi nx}{T})+\sum_{n=1}^{\infty}b_{n}sin(\frac{2\pi nx}{T})
$$
---
- 오일러 공식 Euler Equation

    : 푸리에 변환 공식에 있는 오일러 함수는 결국 코사인과 사인파의 합을 하나로 합쳐 지수함수 형식으로 나타낸 것이다. (복소수 평면에)
$$
    e^{+i\omega t} = cos(\omega t)+ i\ sin (\omega t)
$$
 
$$f(t) = A\times e^{i2\pi f\times (t-\varphi)}$$

$f(t)$ 주기성 함수의 진폭 A 주파수 f  위상 $\varphi$ 으로 표현

또한 지수함수의 특성으로 적분과 미분의 결과의 형태가 같다.

즉, 오일러 공식을 통하여 푸리에 급수는 정현파를 하나의 형태로 합해서 표현할 수 있게 되었다.

---
- 적분 Integration

$$
    \lim_{dx\rightarrow 0} \sum_{n=1}^{N}f(x_{n})\times dx = \int_{a}^{b}f(x)dx = 0
$$
 sin과 cos 의 주기 면적의 합은 0 이 된다.

---
- 직교성 Orthogonality

내적은 두 벡터의 상관관계 Correlation를 나타낸다.
$$v_{a}\cdot v_{b}$$

정사영(projection)의 의미가 결국 정사영 시키는 벡터의 관계성을 반영한 정사영 받는 벡터의 값의 표현임을 알 수 있다.

벡터의 내적이 0 이라는 것은 아무런 연관성이 없는 독립되어 있음을 알고 있다.

두 주기 함수의 내적을 표현하면
$$\hat f \cdot \hat g = \int f(t)g(t) dt = 0$$

즉, sin과 cos 은 직교성을 가지므로 항상 두 함수의 내적은 0이며 따라서 두함수의 합으로 표현되어지는 어떠한 주기성 함수도 표현되어 질 수 있다. (상관관계로 인한 왜곡이 없기 때문에 입력되는 데이터를 그대로 표현해 준다)

---
실제로는 시간 데이터에서 수집되는 데이터는 주기 데이터 형식과 다르게 유한하고 일정 주기마다 Descrete 하게 수집된다.

따라서 실제 수집되는 데이터에 대한 푸리에 변환 계산은 모두 DFT(Descrete Fourier Transform) 이며 따라서 연산의 량은 샘플의 크기에 따라 늘어난다. 

Big-O Complexity Chart
<center>
<img src="https://blog.chulgil.me/content/images/2019/02/Screen-Shot-2019-02-07-at-2.31.54-PM-1.png" width = 600>
</center>

$$ DFT = O(n^{2})  \quad FFT = O(n\ log\ n)$$

이를 정현파의 위상이 같은 지점을 공유한다는 점에 착아하여 샘플의 연산 수를 줄여 준것이 FFT(Fast Fourier Transform)이다.


<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$']]},
    messageStyle: "none",
    "HTML-CSS": { availableFonts: "TeX", preferredFont: "TeX" },
  });
</script>