---
categories: 글쓰기
title: 2.1.4. 군집분석
created: 2025-03-26
tags:
  - 교재
  - 수업
  - 기초통계량
  - ML
---
---
#### *군집분석*
---


---
# 2.1.4. 군집분석
---
### Label 이 없다?

- 분류와의 차이점은 무엇일까? 정해져 있는 카테고리가 없다. (_비지도 학습_)
>[!NOTE] 
>- 분석의 목적에 따라 기준이 달라 질 수 있다.
>- 분석의 대상 또는 분석하는 현상을 바라보는 관점에 따라서 기준이 달라 질 수 있다.
>

- 새로운 상위(광의)나  하위(협의) 또는 새로운 차원의  의미를 부여하여 분류의 기준을 만드는 것
- 데이터 내에 숨어있는 별도의 그룹을 찾아서 의미를 부여

### 유사도 (similarity)

- 군집화 clustering과 분류 classification의 기반
- 가장 쉽게 생각하면 기하학적으로 얼마나 떨어져 있냐를 계산하는 것
1) Euclidean Distance (L2) :
	$$
	D(p,q)\ = \ \sqrt{\sum_{i=1}^{n}(q_{i}-p_{i})^{2}}
	$$    
2) Manhattan Distance (L1) : 
	$$D(p,q)\ = \ \sum_{i=1}^{n}|q_{i}-p_{i}|$$
3) Minkowski Distance : 
	$$D(p,q)\ = \ (\sum_{i=1}^{n}|q_{i}-p_{i}|^{p})^{1/p}$$
		: L1, L2 를 일반화한 것 p 에 따라서 달라짐 $p=\infty$ 면 Chebyshev Distance 와 동일
4) Cosine Similarity :  
	 $$cos(\theta) \ = \ \frac{A \cdot B}{||A||\ ||B||}$$
		: 두 벡터가 이루는 각도를 통해 유사도를 측정. 각이 작아질수록 1 각이 클수록 -1
5) Pearson Similarity : 
	$$pearson(p,q)\ = \  \frac{\sum_{i=1}^{n}(p_{i}-\mu_{p})\cdot(q_{i}-\mu_{q})}{\sqrt{\sum_{i=1}^{n}(p_{i}-\mu_{p})^{2}}\cdot\sqrt{\sum_{i=1}^{n}(q_{i}-\mu_{q})^{2}}}$$
		: 상관계수로 쓰임. 공분산 즉, 변화량의 방향성의 유사성으로 판단.

6) Mahalanobis Dstance: 
		: 확률분포를 고려하여 공분산을 이용한 두 분포간의 거리 계산

 7) KL(Kullback-Leibler)-divergence :
$$
	D_{KL}\ = \ (-\sum_{i=i}^{n}P(p_{i})logP(q_{i})) - (-\sum_{i=i}^{n}P(p_{i})logP(p_{i})) 
$$
		: 두 분포의 정보 Entropy 차이를 이용한 분포의 거리를 계산하는 방법, p에 대한 확률분포에 대하여 q의 확률분포의 정보 엔트로피 변화를 구하는 것. 배대칭(비교 방향에 따라 다름)의 비교 특성이 있다.

###### 평가지표
- 다음 시간에 다시 AI 시간에 잠깐 다뤘던  Comfusion Matrix 를 살펴본다.
- 위에 유사도 역시 손실함수 이면서 동시에 평가 지표가 될 수 있다.

###### (유사성과 관련 되어 최근 많이 연구되는 AI 방법) Contrastive Learning :
- self-supervised representation learning 일종으로 가장 유명함.
- 유사한 데이터는 더 낮은 공간(차원)에서 서로 가깝게 
- 동시에 상이한 데이터는 서로 더욱 멀어지도록 낮은 공간(차원)에
- 새롭게 데이터를 표현해 내는 것
- Self supervised representation learning
![[self_supervised_representation_learning.png]]
[SimCLRv2 :Ting Chen et al, "Big Self-Supervised Models are Strong Semi-Supervised Learners"](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/2006.10029.pdf)
###### NT-Xent (Normalized Temperature-Scaled Cross Entropy Loss) : Contrastive Loss 
: 벡터 사이의 유사성을 측정하는 방법 Positive loss 와 negative loss 를 합쳐서 positive는 가깝게 negative는 Embedding 을 표현하는 손실 함수

### 차원의 저주 (Curse of Dimensionality)

- 하나의 대상, 현상을 표현하는 (또는 측정하는) feature의 개수가 과도한 상태
- 목적에 맞게 또는 파악할 수 있는 정보를 벗어난 상태

- 머신러닝의 학습 : 학습 데이터의 차원의 많아지면 그만큼 학습에 필요한 데이터도 증가
	(반대로 말하면 학습데이터 량에 비하여 데이터의 차원이 많아지면 모델 성능저하)
- 데이터의 표현 : 벡터의 차원이 늘어나면 오히려 유사성 측정이 어려워짐. 즉, 구분이 모호해짐.
- 분석 모델의 성능: feature의 수가 너무 많으면 **모델의 성능 저하**, **overfit**이 될 가능성이 커짐.

### 차원 감소 또는 차원 축소(dimensionality reduction)

- 차원의 저주를 막기 위한 방법은 저 차원으로 차원을 줄이는 것이 일반적임

- 이는 feature의 감소를 의미하고 이때 더 추상적이고 함의적인 feature로 포함시켜 가는 것.

###### 이후 회귀모델 방법에서는... 
- 의미 없는(설명력이 떨어지는) feature 를 약화시키거나 제외함 이를 가지치기 Prunning 이라가고 한다.
- Rigde Regression, Rasso Regression 등 ... 이는 다시 L1, L2 로 feature의 설명력을 평가한다.
- 이때 feature를 회귀식에 입력시키는 방법도 중요한 결정 요인이 된다.

- _의미론적인 이해가 중요함_  측정된 데이터의 값들을 새로운 하나의 값으로 만들거나 특정 feature에 수식으로 연결 시켜야 함. 이때 기존 측정된 feature의 의미가 새로이 표현된 feature에 함의 되었는지 또는 대표하는지 분석자가 파악하고 있어야함.  

	**EDA** 측면에서 데이터에 내재되어 있는 정보를 찾는 문제. 반대로 가설에 대하여 특별한 Latant factor를 검증하는 문제일 수도 있다. 이를 통계에서는 **'탐색적 요인 분석'** 과 **'확인적 요인 분석'** 이라고 한다. 
	
	 ###### 관련 통계적 분석
	 - 관련하여 **'크롬바흐-알파'** (측정신뢰도), **'평균분산추출지수:AVE'** 를 이용한 개념 (타당도) 교차 검증 등을 시행
	 ###### 통계적인 연구 방법으로 요인분석에 관한 내용을 당겨서 수업???
	

- 그래서 일반적으로 사용되는 주요 방법 (통계적인 수치 설명력과 의미론적 설명력이 가능한 방법들)

- **Deep Learning 을 이용한 차원감소 및 데이터의 표현에 대해서도 생각해보자**


### 군집화 기법 

https://scikit-learn.org/stable/modules/clustering.html

##### 1) K-means  - 거리기반
-  k개의 임의의 중심점을 설정하여 군집화 시킴
- 군집된 집단의 평균점으로 중심점 이동
- 소속 데이터가 다른 중심점에 더 가깜다면 해당 중심으로 소속변경
- 이를 더 이상 소속 변경이 없으면 종료
- *장점*  : 간결하고 쉬운 방법
- *단점*  : - feature 많을 수록 군집화 정확도 떨어짐. 
		- 균일한 확산에 적용, feature 간 관계나 편향이 있으면 성능이 떨어짐  
		- 임의 중점에 대한 설정이 어려움.
##### 2) Hierarchical Clustering - 거리기반
-  계층적 군집 분석 : 각 데이터를 계층을 만들며 순차적으로 군집화 하는 기법
- Agglomerative Hiderachical : 가장 가까운 데이터 부터 차례로 군집에 더해서 계층을 만드는 구조
![[계층적군집화.png]]

- 중심점을 선택하지 않아도 됨.

##### 2) GMM (Gaussian Mixture Models) - 확률기반
- 데이터들이 특정한 정규분포를 가지고 있는 집단의 혼합으로 표현되어 질 수 있다는 가정으로 특정 정규분포의 들을 대입하여 데이터의 특성에 맞춰가는 방법
##### 3) DBSCAN (Density-Based Spatial Clustering of Applications with Noise) - 밀도 기반
- 특정한 epsilon 반경을 정하고 그 반경 안에 일정 수 이상의 point를 포함 시키는 Core Point와  그 주변에 같은 데이터를 수를 가지는 Neighbor Point를 정한다. 범위 안에 들어오지만 충분한 데이터 수를 가지지 못하는 Bordor Point 를 정한다. 나머지 범위 안에 Core, Neighbbor를 가지지 못하는 Noise Point 를 정해 가는 방식으로 즉, 이웃하는 데이터의 밀도를 가지고 군집해 가는 방법

### 차원축소 기법 dimensionality reduction

##### 4) T-SNE(t-distributed Stochastic Neighbor Embedding)
- 높은 차원 공간에서 비슷한 데이터 구조는 낮은 차원 공간에서 가깝게 대응하며, 비슷하지 않은 데이터 구조는 멀리 떨어져 대응 시켜, 이웃 데이터 포인트에 대한 정보를 보전하려고 한다.
- 고차원 공간에서의 점들의 유사성과 그에 해당하는 저차원 공간에서의 점들의 유사성을 계산한다. 특정 데이터를 중심으로 한 정규 분포에서 확률 밀도에 비례하여 이웃을 선택, 조건부 확률을 계산 이를 저 차원 공간에 요소로 표현 할 때 조건부 확률과 비교 최소화 하는 방향으로 집단 구분을 최적화해감 이때, KL-divergence 사용 
##### 5) SOM (Self-Orginizing Map)
- 대뇌피질 중 시각피질의 학습 과정을 모델화한 인공신경망 구조 군집화 기법,  비지도학습, 데이터가 적은 경우 Kmean과 유사하게 작동, 데이터가 커질수록 복잡한 구조를 가진다.
- 입력데이터와 동일한 차원의 가중치 벡터를 지닌, 노드와 그리드로 구성
- 공간적인 또는 위상학적인 거리를 계산하여 노드와 그리드를 변형 최적화된 map을 구성한다.

##### 1) PCA(Principal Component Analysis) 주성분 분석 - 분산 기반
- 고차원의 데이터를 저차원의 데이터로 환원시키는 기법을 말한다. 이 때 서로 연관 가능성이 있는 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간(**주성분**)의 표본으로 변환하기 위해 [직교 변환](https://ko.wikipedia.org/w/index.php?title=%EC%A7%81%EA%B5%90_%EB%B3%80%ED%99%98&action=edit&redlink=1 "직교 변환 (없는 문서)")을 사용한다. 데이터를 한개의 축으로 사상시켰을 때 그 [분산]이 가장 커지는 축을 첫 번째 주성분, 두 번째로 커지는 축을 두 번째 주성분으로 놓이도록 새로운 좌표계로 데이터를 [선형 변환](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%EB%B3%80%ED%99%98 "선형 변환")한다. 이와 같이 표본의 차이를 가장 잘 나타내는 성분들로 분해함으로써 데이터 분석에 여러 가지 이점을 제공한다. 이 변환은 첫째 주성분이 가장 큰 분산을 가지고, 이후의 주성분들은 이전의 주성분들과 직교한다는 제약 아래에 가장 큰 분산을 갖고 있다는 식으로 정의되어있다. 중요한 성분들은 [공분산 행렬](https://ko.wikipedia.org/wiki/%EA%B3%B5%EB%B6%84%EC%82%B0_%ED%96%89%EB%A0%AC "공분산 행렬")의 고유 벡터이기 때문에 직교하게 된다.

![[PCA.png]]

>[!NOTE]
 결국 공분산은 상관계수와 관계 있고, 이는 관측된 변수간의 공변성을 통하여 같은 방향성을 가지는 방향성의 벡터의 표현으로 데이터를 정사형해서 표현해 주는 것과 같다.
>선형 대수적 표현으로는 공분산을 특이값분해(eigen decompose)해서 그 eigen value 와 eigen vector로 표현을 바꿔 주는 것과 같다.


##### 잘 분석해 놓은 blog 예제 살펴 보세요.
https://tobigs.gitbook.io/tobigs/data-analysis/undefined-3/python-2-1


### 군집분석의 평가

##### Comfusion matrix

##### Silhouette Score

$$
\begin{align}
	&S_{i} = \frac{b_{i}-a_{i}}{max(a_{i},b_{i})} \\ \\
&a_{i} : \ mean \ of \ distance \ in \ index \ i's \ group \\
&b_{i} : minimum\ mean\ distance\ of \ index \ i \ point \ from \ other\ cluster \ groups\\
\end{align}
$$

```python 
from sklearn.metrics import silhoutte_score
sil_score = silhoutte_score(X, lables)
```

## <관심 있는 분만 - 정성적 데이터로 접근>
## 다차원척도법 (Multi-Dimensional Scaling, MDS)


- 객체간 근접성(Proximity)를 시각화하는 기법
- 군집분석과 유사하게 변수들을 측정 후 개체들의 유사성 또는 비유사성을 측정하여 2차원 공간에 관계도를 표현하는 분석법
- 위상적인 데이터 감의 pattern과 구조를 찾는다.

- 수치형이 아닌 범주형  명목척도 데이터에 대하여서도 관계성을 파악하는데 도움이 된다.
-  대표적으로 상응분석 Correspondence Analysis 

## 머신러닝 연관성 분석

```
1) prince
2) apriori
3) FP-tree
```

## 통계적 연관성 분석

- 범주형, 명목형, 순위형 데이터에 대한 기술 통계
**measure of association** : 카이제곱 통계량에 기초한 분석

## 네트워크 분석



# 요인분석과 관련 신뢰성 타당성 지표에 관한 통계 기법

- 일반적 회귀분석 통계 논문의 통계분석 과정 
1) 주제 설정
2) 관련 문헌 조사 연구
3) 문제 정의, 관련 요인 설정, 개념 정의
4) 선행 연구 논의
5) 가설설정 및 연구모형 설정
6) 개념에 대한 조작적 정의 및 척도 설계(ex) 리커드 척도)
7) 척도에 따른 설문지와 같은 측정 도구 선택. 측정 또는 수집
8) 데이터 입력, 검증
9) 기초 기술 통계 분석 (빈도, 평균, 분산, 구릅) => 기술 비교 분석 (분산분석:ANOVA 등을 이용)
10) **탐색석 요인 분석 : 설계된 측정 문항, 또는 측정 변수들로부터 요인 추출**
		- 측정된 문항에서 개념적 요인을 추출하는 과정이 차원감소 과정이다.
		- 일반적으로  PCA나 Maximum Likehood 방법을 많이 사용한다.
		- 추가적으로 요인 회전 (선형변환)을 통해서 최대로 추출된 요인이 원천 데이터의 정보를 또는 연구의 가설 전제에 맞게 추출되도록 조작한다.
		- PCA 경우 Verimax, MLE 경우 Oblimin 등
	- 신뢰도 검증 : 크롬바흐 알파
	- 교차 타당성 검증 
	- screen 도표
	- 분산 검증,
	- 기타 통계 논문적 일반 검정 기분 확인.
	- **다루는 이유** : EDA 에서 기본적인 분석 이후 심도 있는 분석과 관계 있기 때문.
11) 상관관계 분석:
12) 가설에 따른 회귀분석
13) 분석결과로 가설검증
14) 분석결과 정리 및 결과 해석
15) 표현

