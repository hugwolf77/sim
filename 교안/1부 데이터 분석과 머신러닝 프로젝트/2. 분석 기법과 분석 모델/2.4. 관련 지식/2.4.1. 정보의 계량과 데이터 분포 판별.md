---
categories: 
title: 2.4.1. 정보의 계량과 데이터 분포 판별
created: 2025-02-24
tags:
---
---
#### 2.4.1. 정보의 계량과 데이터 분포 판별
---
 
---
##### 1. #정보_Entropy information entropy
---

1) 데이터의 정보량을 어떻게 측정할 것인가? (Information Measure)

> 데이터량과 데이터가 가진 정보량은 즉, 그 데이터가 가진 가치는 다르다. 데이터로 부터 우리가 얻을 수 있는 정보량은 데이터량과 같지 않다. 따라서 데이터를 저장하거나 통신하는데에는 따라서 이러한 정보량에 따라서 그 효율성과 비용이 결정된다. 무조건적으로 데이터를 많이 보내는 것이 아니라 정보량으로 측정할 수 있어야 기술적으로 효율성과 성능을 높일 수 있다. 이러한 의미에서 나온 이론이 정보이론이다. 
	정보이론 : 최대한 많은 데이터를 매체에 저장하거나 채널을 통해 통신하기 위해 데이터를 정량화하는 응용 수학의 한 분야.

> 정보이론 : "정보"라는 것에 대한 "수학적인 이론"

- 정보량에 대한 직관전 표현은 여러가지가 있다. 필자는 하나의 현상에서 수집 된 데이터들로부터 발견할 수 있는 정보의 희소성의 정도 또는 가치 있는 정보를 찾는데 소모되는 노력의 반비례라고 설명한다.

	(출발) 어떤 내용을 표현하기 위해 필요한 최소한의 질문 개수 (정보량)

- 이러한 정보량에 대한 하틀리는 정보이론의 기초가 되는 정보량 단위에 관한 표현을 고안했다. 
- 알파벳 7개에 대한 내용을 0, 1 로 정보를 전송할 때 몇 개의 표현으로 전달하여야 할까?
- 알파벳은 총 26개이고 정보의 표현은 0, 1 이므로 가장 효율적으로 표현한는 방법은 한번의 정보를 보낼때 대상이 가능한 경우의 반씩 확인하는 식으로 보내는 것이다. 알파벳의 26개 중 반절의 13개에 속했는지 안했는지 그리고 다시 그 반절정도에 속했는지를 확인하는 방법이 1,0으로 표현 할때 가정 적은 비용으로 정보를 전송하는 방법이다.
$$
\begin{align}
	& \ 2^{질문개수} = 26\ ,  \quad 질문의\ 개수 = log_{2}26 \\ \\
	& \ 질문의\ 개수\ = 4.7 (4.700439718141092)
\end{align}
$$
- 총 보내야 하는 알파벳 정보는 7개 이므로 
$$
\begin{align}
	& \ 7\ \times \ 4.7 = \ 32.9 \\ \\
	& \ 질문의\ 개수\ = log_{2}(가능한\ 결과의\ 수)
\end{align}
$$
 **Ralph Vinton Lyon Hartley(1888~1970)**=> 정보 H
 ![[Hartley.png]]
 1928년 논문 "Transmission of Information"에서 정보량을 'H'로 처음 표기
 $$
\begin{align}
	  H\ &= \ n\ log\ (s) \\ 
	     &= \ log\ (s^{n})
\end{align}
$$
- n 은 data의 개수, s는 가능한 class의 수 이다. 즉, $s^{n}$은 가능한 데이터 순서의 경우의 수가 된다. 


(2) 정보량의 크기는 데이터 세트마다 다르다.

- 앞의 방법은 그렇다 , 아니다의 두가지 정보표현을 가지고 정보를 전송하는 경우를 보았다. 이는 다시말하면 하나의 알파벳을 표현하는데 그렇다 그렇지 않다는 표현 방법이 두가지라는 것이다. 즉, 이항분포 확률과 관련이 있다.
---
$$
\begin{align}
	&Binominal :\ P(x) \ = _{n}C_{x}P^{x}(1-P)^{n-x} \\ \\
\end{align}
$$
<center>
<img src="https://mathworld.wolfram.com/images/eps-svg/GaltonBoard_1000.svg" width =100, height =200>
</center>
---

- 이런 경우를 생각해 보자.
-  A, B, C, D 4가지 class 의 결과를 분류하는 모델이 2개 있다.
- 첫번째 T 모델은 4개 class를 구분할 확률이 모두 0.25로 같다. 앞의 방법으로 생각할 때 알파 모델이 class를 구분하는 노력인 정보량 H는 2 가 된다. 
- 그런데 만약 두번재 Q 모델의 각각의 class 로 분류될 확률이 다음과 같이 각각 다르다면 어떨가
	+ P(A) = 0.5
	+ P(B) = 0.125
	+ P(C) = 0.125
	+ P(D) = 0.25
- A의 확률이 가장 높음으로 먼저 A 를 분류해 내는 것이 답을 찾을 가능성이 높다.
- 다음으로 나머지 확률 0.5 중 D 를 분류해 내는 것이 0.25 로 가장 크다.
- 마지막으로 나머지 B, C 를 구분하는 분류를 하는 것이 효율적이다.
- 이를 다시 표현해 보면 A를 분류하는데 1번, D를 분류하는데 2번, 나머지 B, C를 분류하는데 3번째의 노력이 필요하다. 즉, Q 모델의 드려야하는 노력 정보량은 1.75가 된다.
$$ P(A) \times 1 +\ P(B) \times 3 +\ P(C) \times 3 + P(D) \times 2\ =\ 1.75 $$
- 1.75의 노력의 분류를 통해서 4가지 class 구분해 낼 수 있다.
- 이처럼 한 데이터 셋에서 특정한 데이터의 종류를 구하는 확률이 달라지면 데이터를 전달하는 노력의 정도가 달라진다. 즉, 같은 개수의 클라스 분류를 가지고 있더라도 한 데이터 세트의 정보량의 희소성, 도는 정보를 전달하는데 드는 노력의 크기는 달라 질 수 있다.
---
 
 (3) 정보량의 크기를 불순도로 표현하다.
 
 ##### **Claude Elwood Shannon => Entropy**

![[클로드섀넌.png|200]] 
정보이론의 아버지 : 클로드 엘우드 섀넌  Claude Elwood Shannon (1916~2001) <정보통신의 수학적 이론>

- 섀넌은 이러한 정보량의 차이를 확률과 연관시켜 각각의 클래스의 발생확률이 균일할수록 즉, 균일하게 섞여 그 혼합정도가 높을 수록 복잡도가 높아지고 이를 마치 높은 순도에서 낮은 순도로, 질서정연한 상태에서 혼잡한 상태로 구분하는 Entropy로 표현하였다. 
- 또한 이러한 정보량을 컴퓨터의 bit로 표현하는 일반적인 데이터의 표현으로 정보량을 계산하는 정보이론학 논문을 내놓으면서 종보이론학이 발전하게 된다.  
 - Decision Tree model 에서 불확정성을 Entropy로 표현한 이유.
	+ 불확정성이 높으면 순도가 낮아지고 정보량이 많아 진다. 
	+ 반면 불확정성이 낮아지면 순도도 높아지고 정보량이 적어진다. 

<center>
<img src="https://upload.wikimedia.org/wikipedia/ko/thumb/7/71/Entropy_kor.jpg/330px-Entropy_kor.jpg" width =300, height =300>
</center>
- 일반화 하면 어떤 결과가 발생 가능한 확률이 작아지면 정보량은 커지고, 발생 확률이 높아 질수록 정보량은 작아진다.
- 모두 같은 확률로 구분되어 진다. 하나 하나의 높은 확률이 없어진다. 분류가 많아질 수록 확률은 더 낮아 진다. 정보량이 증가한다.
- 각각 확률이 다르다. 특정한 분류가 더 많은 정보를 가지고 있다.

- 이를 각 클래스들의 빈도의 분포 즉, 이산확률분포로 넓혀서 생각하면 다음과 같습니다.
- 경우의 수가 고르게 일어날 확률이 높을 수록 분포는 넓게 평평해 지고 그에 해당하는 정보를 전달하는데도 더 많은 노력이 필요합니다.
- 반면 불균형 할 수록 정보를 전달해야하는 수고는 줄어 듭니다.
![[Pasted image 20250409124658.png]]
[Pattern Recognition and Machine Learning, C.M. Bishop]

<center>
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Binary_entropy_plot.svg/300px-Binary_entropy_plot.svg.png" width =200, height =200>
</center>
[C.E. Shannon, A Mathematical Theory of Communication, 1948]

---
- 이는 각 class의 확률과 관계가 있다는 뜻이다. 
	+ P(A) = 0.5 = 1/2         => $log_{2}(\frac{1}{1/2}) = 1$
	+ P(B) = 0.125 = 1/8      => $log_{2}(\frac{1}{1/8}) = 3$
	+ P(C) = 0.125 = 1/8      => $log_{2}(\frac{1}{1/8}) = 3$
	+ P(D) = 0.25 = 1/4       => $log_{2}(\frac{1}{1/4}) = 2$
+ 즉, 가능한 경우의 수는 사건 발생 확률의 역수가 된다. 

- 이를 통하여 섀넌은 데이터의 정보량을 Information Entropy라고 하여 다음과 같은 식을 세우게 된다. 
$$
\begin{align}
	H \ &= \sum(class확률) \times log_{2}(\frac{1}{class확률})  \\ \\
	    &= \sum_{i} P_{i}\ log_{2}(\frac{1}{P_{i}}) \\ \\
	    &= -\sum_{i} P_{i}\ log_{2}(P_{i})
\end{align}
$$

- 즉, 이산확률분포를 통해 계산된 정보량(정보를 보내거나 얻기 위해 들어가야 하는 노력)에 대한 기대값 (질문의 개수)이다.


---
##### 2. #Cross_Entropy
---
- 다른 관점에서 생각해보자
- 앞에서 예로든 두개의 모델 T와 Q의 성능 차이를 구해야 한다고 생각해보자. 
- 우리는 이럴때 Information Entropy를 사용하여 정보량의 차이를 사용할 수 있음을 알 수 있다.
- Log에 역수로 표현된 확률을 모델이 출력해내(또는 추론하는)는 class에 대한 확률분포($Q$)라고 생각해보자. 그리고 $\sum$ 에 해당하는 확률분포를 class가 가진 원래의 현상의 확률분포($P$)라고 생각해보자.
- 그러면 위에 Information Entropy는 모델이 추론으로 만든 확률분포를 통한 정보량의 계산으로 실제 확률분포를 계산한 기대값이 되므로 일종의 확률분포의 차이를 계산하는 식으로 사용할 수 있게 된다.
- 사실을 이런 차이 계산은 로지스틱 함수의 손실함수(Binary Cross Entropy(BCE) loss)와 같습니다.
$$
\begin{align}
	\hat y=\frac{1}{1+e^{-(\hat\beta_{0}+\hat\beta_{1}x_{1}+\dots + \hat\beta_{d}x_{d})}} \\ \\
	L(y,\hat y) = -[y\ log(\hat y) + (1-y)\log(1-\hat y)]
\end{align}
$$

- 즉, 두 가지 확률 분포 사이의 차이를 측정하는 지표로 머신러닝의 손실함수로 사용될 수 있는 것이다.
	- 예측 모델의 성능평가
	- 손실 함수
	- 확률분포 비교

- 차이를 구한다는 의미에서 손실함수 관점에서 생각해 보자.
- 이번에는 Q, P 두 개의 확률 분포가 있다고 생각해 보자 
- Q 는 모델의 예측값의 확률 분포이다.
- T 는 데이터의 라벨의 실제값의 확률 분포이다.
- Q라는 모델은 T라는 확률분포를 최대한 유사한 확률분포를 추론해 내야한다.
- 모델의 성능이 높아서 효율성이 좋아서 적은 정보로 분류를 해낼 수 있다면 즉, Entropy 가 낮을 것이고 이는 정보량도 낮을 것이다.
$$
\begin{align}
	H(P,Q) \ &= \sum_{i=1}^{k} P_{i} \times log_{2}(\frac{1}{Q_{i}})  \\ 
\end{align}
$$
- Cross Entropy를 loss 로 사용하여 어떤 분류일지 유사하게 추론 경우 즉, 예측값으로 나온 확률분포가 실제와 같까워 질수록 Cross Entrypy 는 작아진다.

>[!NOTE]
>- 이산형이 아니라 연속형 확률분포라면?
> - sigma 가 integral 이 되어야 한다. 각 경우의 합이 아니라 연속적인 확률 분포의 적분이 된다. 
> - 이를 미분 엔트로피라한다.
$$
	\begin{align}
		& H(x)\ =\ - \int_{S}f(x)log f(x) dx \\
	\end{align}
$$
$$
\begin{align}
	& \int_{i=1}^{k} P(x) \times log\frac{1}{Q(x)}  \\ \\
	& - \int_{i=1}^{k} P(x)\times logQ(x)  \\
\end{align}
$$

- 이진(class)분류(0,1) 분류로 생각해보면, logistic regression의 목적함수 즉, cost function (loss)로 표현하면
$$
\begin{align}
	& -P(x)\ log\ Q(x) - (1-P(x))log(1-Q(x))  \\
	=\ &-y\ log\ \hat y - (1-y)log(1-\hat y)
\end{align}
$$
- Cross Entropy를 최소화 하는 것은 Log-Likelihood를 최대화하는 것과 같다.
$$
\begin{align}
	\ & if \quad y=1 \quad \Longrightarrow  P(y|x)=\hat y \\
	\ & if \quad y=0 \quad \Longrightarrow  P(y|x)= 1 -\hat y \\ \\
	\ &P(y|x) = \hat y^{y}(1-\hat y)^{(1-\hat y)} \\ \\
\end{align}
$$

- 위식에 log를 취하면
$$
\begin{align}
	\ &P(y|x) = \hat y^{y}(1-\hat y)^{(1-\hat y)} \\ \\
	&log(P(y|x)) = log(\hat y^{y}(1-\hat y)^{(1-\hat y)}) \\ \\
	&=\ y\ log\ \hat{y} + (1-y)log(1-\hat{y})
\end{align}
$$

- y = 1 인 경우는 $\hat y$ 를 최대화해야함, y=0 인 경우 $(1-\hat y)$ 을 최대화 해야함.
$$
\begin{align}
	\ &maximize\ y\ log\ \hat y + (1-y)log(1-\hat y) \\ 
	&= minimize\ -y\ log\ \hat y - (1-y)log(1-\hat y)
\end{align}
$$
---
#### 3.  #Kullback-Leibler_divergence
---
> **쿨백-라이블러 발산**(Kullback–Leibler divergence, **KLD**)은 두 [확률분포](https://ko.wikipedia.org/wiki/%ED%99%95%EB%A5%A0%EB%B6%84%ED%8F%AC)의 차이를 계산하는 데에 사용하는 함수로, 어떤 이상적인 분포에 대해, 그 분포를 근사하는 다른 분포를 사용해 샘플링을 한다면 발생할 수 있는 [정보 엔트로피](https://ko.wikipedia.org/wiki/%EC%A0%95%EB%B3%B4_%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC) 차이를 계산한다. 상대 엔트로피(relative entropy), 정보 획득량(information gain), 인포메이션 다이버전스(information divergence)라고도 한다.
> <위키피디아>
__
- 사실 Cross Entropy 에는 실제값에 대한 정보량이 이미 숨어 있다.

$$
\begin{align}
	H(P,Q) \ &= -\sum_{i=1}^{k} P_{i} \times log_{2}Q_{i}  \\
			 &= -\sum_{i=1}^{k} P_{i} \times log_{2}Q_{i} 
			    -\sum_{i=1}^{k} P_{i} \times log_{2}P_{i}  
			    +\sum_{i=1}^{k} P_{i} \times log_{2}P_{i}  \\ \\
		  & 위\ 식중\ H(P) = -\sum_{i=1}^{k} P_{i} \times log_{2}P_{i}  \\ \\
		     &= H(P) -\sum_{i=1}^{k} P_{i} \times log_{2}Q_{i} 
			    +\sum_{i=1}^{k} P_{i} \times log_{2}P_{i}  \\ \\
		     &= H(P) +\sum_{i=1}^{k} P_{i} \times log_{2}\frac{P_{i}}{Q_{i}}  \\ \\
\end{align}
$$

- 결국 H(P) : P 분포의 Entropy 에 $\sum_{i=1}^{k} P_{i} \times log_{2}\frac{P_{i}}{Q_{i}}$ 를 더한 것이 Cross Entropy 이다.
- 그리고 $\sum_{i=1}^{k} P_{i} \times log_{2}\frac{P_{i}}{Q_{i}}$ 이 KL-divergence 가 된다.
-  이것은  P, Q 두 분포 간의 정보량을 계산하는데서 P 분포의 정보량을 빼서 P, Q 두 분포 간의 정보량 차이만을 남긴 것이다.

- 간소화 해서 Cross Entropy ~ KL-divergence 정리하면 다음과 같다.
$$
\begin{align}
	H(P,Q) \ &= KL(P||Q) + H(P)\\ \\
	KL(P||Q) \ &= H(P,Q) - H(P)\\ \\
\end{align}
$$
---
##### 4. Conditional Entropy
---

- 확률 분포 X, Y 두 개에 대한 Joint Entropy (결합 정보량)은
$$
\begin{align}
	& H_{p}(X,Y) \ = -\sum_{x,y}P(x,y) \times log_{2}P(x,y)  \\ \\
	& if\ X,\ Y\ 서로\ 독립이면 \ \ H_{p}(X,Y) = H(X)\ +\ H(Y)
\end{align}
$$

- Conditional Entropy (조건부 정보량)
$$
\begin{align}
	H_{p}(X|Y) \ &= -\sum_{x,y}P(x,y) \times log_{2}P(x|y)  \\ \\
				 &= -\sum_{x,y}P(x,y) \times log_{2}\frac{P(x,y)}{P(y)}  \\ \\
				 &= -\sum_{x,y}P(x,y) \times \{log_{2}P(x,y) - log_{2}P(y)\}  \\ \\  
				 &= -\sum_{x,y}P(x,y)log_{2}P(x,y) + \sum_{x,y}P(x,y)log_{2}P(y) \\ \\
				 &= -\sum_{x,y}P(x,y)log_{2}P(x,y) + \sum_{y}P(y)log_{2}P(y) \\ \\
				 &= H_{p}(X,Y) - H_{p}(Y)
\end{align}
$$

##### 5. Mutual Information
---
 
$p(x), p(y)$ 두 확률 변수가 공유하는 정보를 측정하는 척도.

$$
\begin{align}
	H_{m}(X;Y) \ &= \sum_{(x,y)\in (\mathcal{X},\mathcal{Y})}P(x,y) \times log \frac{p(x,y)}{p(x)p(y)}  \\ \\
\end{align}
$$
---
#### etc. 열역학의 엔트로피

열역학적 엔트로피는 분자들의 배열이 나타낼 수 있는 상태의 총합에 로그함수를 취한 것으로 정의

$$
\begin{align}
	& S = -\mathcal k_{b} \sum P_{i}\ ln\ p_{i}  \\
\end{align}
$$
- 1J K-1=13.06175ZB(=11.06373ZiB)