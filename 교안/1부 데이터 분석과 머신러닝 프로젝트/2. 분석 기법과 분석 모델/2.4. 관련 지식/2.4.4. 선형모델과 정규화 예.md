---
categories: 
title: 선형모델과 정규화
created: 2025-03-22
tags:
---
---
#### *선형모델과 정규화*
---
```python
from sklearn.linear_model import LogisticRegression

logistic_l1 = LR(penalty='l1', C=1.0)
logistic_l1.fit(x, y_true)
```

- Nonnegative Matrix Factorization 
	 데이터 X를 representation W 와 dictionary H 로 분해하되, W,H 의 components 가 모두 non-negative 하게 만드는 factorization 방법
```python
from sklearn.decomposition import NMF

nmf = NMF(n_components=200, l1_ratio=1.0)
z = nmf.fit_transform(x)
```

### Lasso


- 데이터 만들기
```python
import numpy as np

n_data = 1000
n_features = 3000
noise = 0.01

probs = np.linspace(0.1, 0.005, num=n_features)

def fill(prob):
    if np.random.random() >= prob:
        return 0
    return np.random.randint(1, 2)

def make_sparse_data(n_data, n_features, probs):
    x = np.zeros((2 * n_data, 2 * n_features))
    y = np.asarray([0] * n_data + [1] * n_data)
    for i in range(n_data):
        for j in range(n_features):
            x[i,j] = fill(probs[j])
            x[i,j + n_features] = fill(max(probs[j] * 10 * noise, noise))
            x[i+n_data,j+n_features] = fill(probs[j])
            x[i+n_data,j] = fill(max(probs[j] * 10 * noise, noise))
    return x, y

x, y_true = make_sparse_data(n_data, n_features, probs)
```


- L2로 잘 분류되는 데이터
```python
from sklearn.linear_model import LogisticRegression as LR

logistic_l2 = LR()
logistic_l2.fit(x, y_true)
y_pred = logistic_l2.predict(x)

def accuracy(y_true, y_pred):
    return np.where(y_true == y_pred)[0].shape[0] / y_true.shape[0]

accuracy(y_true, y_pred) # 1.0
```


```python
from sklearn.linear_model import LogisticRegression

for c in [100, 10, 5, 2, 1, 0.5, 0.2, 0.1, 0.07, 0.03, 0.01]:
    logistic_l1 = LR(penalty='l1', C=c)
    logistic_l1.fit(x, y_true)
    y_pred = logistic_l1.predict(x)
    train_accuracy = accuracy(y_true, y_pred)
    nnz = np.where(abs(logistic_l1.coef_[0]) > 0)[0].shape[0]
    print('c = {}, accuracy = {}, nnz = {}'.format(c, train_accuracy, nnz))
```


- torch로 구현

```python
import torch.optim as optim

optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)
```

```python
import torch.nn as nn

class LogisticRegression(nn.Module):
    def __init__(self, input_dim, n_classes, bias=True):
        super().__init__()
        self.fc = nn.Linear(input_dim, n_classes, bias=bias)

    def forward(self, X):
        out = self.fc(X)
        return out
```

```python
def train(model, loss_func, optimizer, x, y, epochs,
    c=1.0, penalty='l1', sparse_threshold=0.0005, use_gpu=False):

    # for training error compute
    y_true = y.numpy()

    # regularity
    c = torch.FloatTensor([c])
    p = 1 if penalty == 'l1' else 2

    # cuda
    use_cuda = use_gpu and torch.cuda.is_available()
    if use_cuda:
        print('CUDA is {}available'.format('' if use_cuda else 'not '))
        model = model.cuda()
        x = x.cuda()
        y = y.cuda()
        c = c.cuda()

    # Loop over all epochs
    for epoch in range(epochs):

        # clean gradient of previous epoch
        optimizer.zero_grad()

        # predict
        y_pred = model(x)

        # defining cost
        loss = loss_func(y_pred, y)
        regularity =  torch.norm(model.fc.weight, p=p)
        cost = loss + c * regularity

        # back-propagation
        cost.backward()
        optimizer.step()

        if epoch % 100 == 0 or epoch <= 10:
            # training error
            if use_cuda:
                y_pred = y_pred.cpu()
            y_pred = torch.argmax(y_pred, dim=1).numpy()            
            train_accuracy = accuracy(y_true, y_pred)

            # informations
            l1_norm = torch.norm(model.fc.weight, p=1)
            l2_norm = torch.norm(model.fc.weight, p=2)
            parameters = model.fc.weight.data.cpu().numpy().reshape(-1)
            t = abs(parameters).max() * sparse_threshold
            nz = np.where(abs(parameters) < t)[0].shape[0]

            print('epoch = {}, training accuracy = {:.3}, l1={:.5}, l2={:.3}, nz={}'.format(
                epoch, train_accuracy, l1_norm, l2_norm, nnz))

    if use_cuda:
        model = model.cpu()

    return model
```


```python
model = LogisticRegression(input_dim = 2 * n_features, n_classes = 2)

# loss function & optimizer
# Mean Squared Error loss
loss_func = nn.CrossEntropyLoss()

# Stochastic Gradient Descent optimizer
optimizer = optim.SGD(model.parameters(),lr=0.01, weight_decay=0.0)

model = train(
    model,
    loss_func,
    optimizer,
    torch.FloatTensor(x),
    torch.LongTensor(y_true),
    epochs=10000,
    use_gpu=True,
    c = 0.01
)
```