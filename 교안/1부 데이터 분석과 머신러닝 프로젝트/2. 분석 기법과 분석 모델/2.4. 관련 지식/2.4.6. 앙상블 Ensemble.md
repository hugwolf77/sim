---
categories: 
title: 앙상블 Ensemble
created: 2025-04-07
tags:
---
---
#### 앙상블 Ensemble
---

- 여러 개의 개별 모델을 조합하여 최적의 모델로 일반화하는 방법.
- weak classifier 들을 결합하여 strong classifier 를 만드는 방법.
- decision tree 에서 overfitting 되는 문제를 감소 시킨다는 장점.

### 1) 보팅 Voting

- 각각 다른 알고리즘을 이용한 분류기를 결합하는 방식으로 최종 예측 값을 투표하는 방식
	(1) Hard Voting
		: 각 모델의 예측 결과 (클래스 레이블)를 기반으로 다수결 투표를 하여 최종 클래스를 결정.
	(2) soft Voting 
		: 각 모델의 클래스별 확률 예측값을 평균내어 가장 높은 확률을 가진 클래스를 최종 클래스로 결정. 일반적으로 소프트 보팅이 성능이 더 좋다.
		
![[Hard_Soft_voting.png|600]]

### 2) Bagging - Bootstrap Aggregating

- 하나의 알고리즘을 사용하여, **부트스트래핑 (Bootstraping)** 이라는 샘플링 기법으로 원본 데이터에서 여러 개의 중복을 허용한 표본(subset)을 추출합니다. 각 표본으로 개별 모델을 학습시킨 후, 이들의 예측 결과를 **집계 (Aggregating)** 하여 최종 예측.
- 분산(Variance)을 줄여 과적합(Overfitting)을 방지하고 모델의 안정성을 높인다.
- **대표적인 알고리즘 랜덤 포레스트 (Random Forest)** - 결정 트리 앙상블에 배깅과 특징 무작위 선택을 결합한 기법

	![[Bagging.png]]
	
	(1) Bootstraping - 부츠 뒤꿈치의 가죽 손잡이
		: Bootstrap resampling이라고도 한다. 원본 데이터로 부터 하위 데이터 집합(subset) 표본을 무작위 복원 추출을 통하여 생성한다. 새로운 데이터 수집 없이 데이터의 모수의 통계치를 유추하고자 할 때 사용한다.
	(2) Aggregating 
		: categorical data 에 대한 classifier 분류기 모델에서는 최종 집계를 voting으로 한다. 반변 특정 값을 예측해야하는 continuous data 에 대해서는 평균을 사용하여 집계한다.
	(3) Voting과 Bagging의 차이점
		: Voting은 서로 다른 알고리즘을 사용한 모델들로 같은 데이터를 학습한 결과를 가지고 집계한다. 반면 Bagging은 같은 알고리즘을 사용하여 다른 데이터 (하위 샘플링 데이터)로 학습한다.
		
	![[Voing_Bagging.png|500]]


### 3) boosting 

- 배깅에서 각각의 모델이 독립적으로 학습하는 반면, 부스팅은 이전 모델의 학습이 다음 모델의 학습에 영향을 준다. 
- 여러 개의 약한 학습기(weak learner)를 **순차적으로** 학습시키면서, 이전 모델이 잘못 예측한 데이터에 더 큰 **가중치**를 부여하여 오류를 개선해 나가는 방식이다. 
- 각 모델은 이전 모델의 약점을 보완하는 방향으로 학습한다.
- 편향(Bias)을 줄여 예측 정확도를 향상시키는데 집중한다.
- **대표적인 알고리즘:**
    - AdaBoost (Adaptive Boosting)
    - GBM (Gradient Boosting Machine)
    - XGBoost (eXtreme Gradient Boosting)
    - LightGBM (Light Gradient Boosting Machine)
    - CatBoost
![[boosting.png|550]]


### 4) Stacking

- 여러 개의 서로 다른 모델(base learner)의 예측 결과를 새로운 메타 모델(meta learner 또는 blender)의 학습 데이터로 사용하여 최종 예측을 수행.
- 보통 교차 검증(Cross-Validation) 방법과 연계하여 사용.
- Base Learner 단계에서 원본 데이터 하나로 학습하면 overfitting 되기 쉽다.
- 다양한 모델의 강점을 결합하여 최고의 예측 성능을 얻는 것을 추구.

	(1) **Base Learner 학습 단계:** 여러 다양한 모델을 원본 학습 데이터로 학습.
	(2) **Meta Learner 학습 단계:** 각 base learner의 예측 결과들을 새로운 특징(feature)으로 사용하여, 최종 예측을 수행하는 메타 모델을 학습. 이때, 검증 데이터셋이나 교차 검증(Cross-Validation)을 통해 얻은 예측값들이 메타 모델 학습에 사용.

![[CV_stacking.png]]