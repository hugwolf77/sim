---
categories: 글쓰기
title: 2.4.8. Likehood
created: 2024-10-30
tags:
  - 교재
  - 수업
  - Likehood
---
---
### 2.4.8. Likehood
---

- 특정 모수(parameter) 값이 주어졌을 때, 관찰된 데이터가 나타날 가능성(plausibility)을 나타내는 함수

	- **확률 (Probability):** 모수가 이미 알려져 있을 때, 특정 데이터가 관찰될 가능성(Parameter -> Data)
	- **우도 (Likelihood):** 관찰된 데이터가 주어져 있을 때, 특정 모수 값이 참일 가능성 (Data -> Parameter)

#### example

- **확률(Probability)** 
	: 만약 앞면이 나올 확률 θ=0.5라고 알려져 있다면, 10번 던져서 7번 앞면이 나올 확률은 이항 분포를 이용하여 계산할 수 있습니다. 
$$P(X=7∣θ=0.5)= \begin{pmatrix}10\\ 7\end{pmatrix}(0.5)^{7}(0.5)^{3} $$
$$
_{n}C_{r}=\frac{_{n}p_{r}}{r!}=\frac{n!}{r!(n-r)!}$$
여기서 $\theta$는 고정된 값이고, $X=7$ 이라는 특정 데이터가 나올 확률을 계산.
    
- **우도(Likelihood)** 함수
	: 동전 던지기 결과를 관찰한 후, 원래 동전의 앞면이 나올 확률 $\theta$를 추정하고자 할때. 
	10번 던져서 7번 앞면이 나왔다는 데이터가 주어졌다고 하면 다양한 $\theta$ 값에 대해 이 데이터가 얼마나 "가능성이 있는지"를 나타내는 함수. 
	$$L(\theta∣X=7)=P(X=7∣\theta)=\begin{pmatrix}10\\7\end{pmatrix} \theta^{7}(1−\theta)^{3}$$
	 여기서 관찰된 데이터 X=7은 고정되어 있고, θ는 변수. 우도 함수 $L(\theta∣X=7)$는 각 $\theta$ 값에 대해 10번 던져서 7번 앞면이 나올 "가능성".

![[Likelihood.png]]

### 최대우도법 (Maximum Likehood Method:MLE)

**최대가능도방법** (最大可能度方法, : maximum likelihood method) 또는 **최대우도법**(最大尤度法)은 어떤 확률변수에서 [표집] 한 값들을 토대로 그 확률변수의 [모수]를 구하는 방법이다. 어떤 모수가 주어졌을 때, 원하는 값들이 나올 [가능도]를 최대로 만드는 모수를 선택하는 방법이다. [점추정] 방식에 속한다.

### Bayes' Theorem

$$
	\begin{align}
		P(A|B) &= \frac{P(B|A)P(A)}{P(B)} \propto \mathcal L(A|B)\ P(A) \\
	\end{align}
$$

- $\mathcal L$ 은 우도(Likelihood) 즉, 가능도를 의미한다.
- $P(B) = \int_{A} P(B|A)$   B에 대한 사전 확률로서 정규화 상수(Normalization Factor) 역할. 즉, 모든 A 조건에서의 B확률들을 합하여 추정


---
### 2. Maximum Likelihood Estimation
---
출처: [공돌이의 수학정리노트](https://angeloyeo.github.io/2021/02/08/GMM_and_EM.html)
<center>
<img src="https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-07-17-MLE/pic1.png" width =300 height=200>
</center>

- 우도 : 확률변수 $X$가 모수 $\theta$ 에 대한 확률분포 $P_{\theta}(X)$를 가지며, $X$가 특정한 값  $x$ 으로 표집되 었을 경우, $\theta$ 의 가능도 함수 $\mathcal L (\theta|x)$ 는 다음과 같이 정의 한다.

$$ \mathcal L (\theta|x) =  P(X=x|\theta)$$

- 만약에 $\theta$에 대해서 정규분포라고 가정할 수 있다면, $\theta = (\mu, \sigma)$  평균과 표준편차로 그 특성을 나타낼 수 있다. 이때 한 개의 데이터 $x_{n}$ 가 정규분포를 따를 확률은 해당 관찰 값을  해당 평균과 표준편차로 되어 있는 정규분포 식에 대입하는 것과 같다.

$$
	\begin{align}
		P(x_{n}|\theta) &= \frac{1}{\sqrt{2\pi\sigma}}exp \{-\frac{(x_{n}-\mu)^{2}}{2\sigma^{2}} \}
	\end{align}
$$
- 이후 모든 관찰 데이터를 독립이라고 가정하면 다음과 같이 우도(Likelihood: 가능도)를 계산할 수 있다.

$$
	\begin{align}
		\mathcal L (\theta) =  P(X|\theta) = \Pi_{n=1}^{N}p(x_{n}|\theta)
	\end{align}
$$

![[MaxLikehoood.png]]


 - MLE (Maximum Likelihood Estimation) 은 결국 이렇게 가능도를 계산하여 주어진 관찰 값을 통해서 가능도가 최대가 되는 분포를 찾는 것이 된다. (왜냐하면 가능도가 최대가 되는 정규분포가 해당 관찰 값들의 진짜 정규분포일 가능성이 높다는 뜻이기 때문이다.)
 - 딥러닝 시간의 그라디언트 디센팅을 생각해 보자 우리가 어떠한 최대값과 최소값이 되는 경우를 찾을 때 미분을 통해서 구하는 방법을 많이 사용함을 알고 있다.
- 이때 미분을 쉽게 구하기 위해서 log 형태로 우도식을 변형한다. 이 처럼 log 형태를 취하는 우도를 log likelihood  라고 한다. - 를 붙인 이유는 딥러닝과 같이 Convex 형태를 만들기 위해서다.

$$ E(\theta) = -ln\ \mathcal L(\theta) = - \sum_{n=0}^{N} ln\ p(x_{n}|\theta) $$
- 위 log likelihood 를 미분하여 최대값 - 를 붙여서 아래로 볼록하게 만들었으니 최소값이 된다)이 되는 $\theta$ 를 구하기 위하여 위의 정규분포 식을 미분 정리하면 다음과 같다. (미분 계산 과정은 생략)

$$ 
	\begin{align}
\frac{\partial}{\partial \theta} E(\theta) \longrightarrow \frac{\partial}{\partial \mu} E(\mu, \sigma) = \frac{1}{\sigma^{2}}(\sum_{n=1}^{N} x_{n} - N\mu)
	\end{align}
$$
- 이때 $\sigma$ (표준편차)는 분모에 있기 때문에 식을 0을 만들 수 없다. 따라서 $\hat\mu = \frac{1}{N}\sum_{n=1}^{N}x_{n}$의 식을 위에 대입 하여 구한다. $\hat\mu$를 구하면  분산도 계산 할  수 있다. 
- 즉, 데이터 샘플이 주어 지고, 이 데이터 샘플의 모수가 정규분포를 따른다고 전제한다면 최대 가능도를 추정하여 샘플이 가르키는 정규분포의 특성 $\mu$와 $\sigma$를 구할 있게 된다.

$$ 
	\begin{align}
 추정모수 =  argmax_{\theta}\ (\frac{\partial L (\theta|X)}{\partial \theta} )
	\end{align}
$$


---

---

https://scikit-learn.org/1.5/auto_examples/calibration/plot_calibration_curve.html#sphx-glr-auto-examples-calibration-plot-calibration-curve-py