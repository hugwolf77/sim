---
categories: 글쓰기
title: 2.3.4. 활성함수 - Activation Function
created: 2025-03-12
tags:
  - 교재
  - 수업
  - Activation_Function
---
---
#### *2.3.4 활성함수* - Activation Function
---
#### 활성함수(Activation Function) : 
- 신경망의 자극 또는 반도체의 비트 표현과 같은 작용을 하는 함수.
- 미분 가능한 형태의 출력을 가지는 함수여야만 한다.
- **역할**
	1) **뉴런 활성화 결정**: 활성화 함수는 뉴런의 활성화 여부를 결정.
	2) **비선형성 추가**: 활성화 함수는 신경망에 비선형성을 추가.
	3) **출력 범위 조절**: 활성화 함수는 출력값의 범위를 특정 범위로 변환하여 특정 작업에 적합한 출력을 생성.
---
#### 1) 시그모이드함수의 대표인 로지스틱 함수형태 
$$Sigmoid(\sigma) = \frac{1}{1+e^{-x}} = \frac{e^{x}}{e^{x}+1}$$
![[sigmoid_logistic.png]]
- 
	- 역함수는 logit 함수가 된다.
![[logit_function.png]]


####  2) 하이퍼 탄젠트 : Tanh

![[Pasted image 20240312070102.png]]

$$
\begin{align}
	& tanh(x) \quad = \quad \frac{sinh(x)}{cosh(x)} \quad = \quad \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
\end{align} 
$$
![[Pasted image 20240312070344.png]]

1) 모든 입력값에 대해 출력값이 실수값으로 정의
2) 값이 작아질수록 -1, 커질수록 1에 수렴
3) 입력갑싱 0에 가까울수록 미분이 크기 때문에 출력값이 빠르게 변함
4) vanishig gradient 문제 있음

---
#### **3) Gradient Vanishing**

- 기울기 소실(Gradient Vanishing)은 심층 신경망(Deep Neural Network)을 학습할 때 발생하는 문제 중 하나입니다. 이 문제는 역전파(Backpropagation) 과정에서 입력층에 가까운 층으로 갈수록 기울기(Gradient)가 점차적으로 작아져서 결국에는 거의 0에 가까워지는 현상을 의미.
- 원인 : 
		1) **활성화 함수(Activation Function)의 특성**
			시그모이드(Sigmoid)나 하이퍼볼릭 탄젠트(Hyperbolic Tangent)와 같은 활성화 함수는 입력값이 커지거나 작아질수록 기울기가 0에 가까워지는 특성
		2) 심층 신경망의 깊이
			신경망의 깊이가 깊어질수록 역전파 과정에서 기울기를 계산하는 과정에서 여러 번의 곱셈 연산이 발생. 기울기가 지수적으로 감소
- 영향 : 
		- **학습 속도 저하**
		- **학습 성능 저하**
		- **지역 최적점(Local Minima) 문제**
- 방안 : 
		- **ReLU(Rectified Linear Unit) 활성화 함수 사용**
		- **가중치 초기화(Weight Initialization) 기법 사용**
		- **배치 정규화(Batch Normalization) 사용**
		- **잔차 연결(Residual Connection) 사용**

---

### 4) ReLu

![[Pasted image 20240312070631.png]]

![[Pasted image 20240312070802.png]]

1) 가장 많이 사용되는 활성함수
2) vanishing gradient 문제 없음Y = X1+X2+X3+ .... + Xn + bias
3) 입력값이 음수일 경우 출력값과 미분값을 모두 0으로 강제, 한번 가중치가 0으로 가면 살릴수 없음
4) 연산 속도가 빠름

### 5) Leaky ReLu
![[Pasted image 20240312071214.png]]
![[Pasted image 20240312071149.png]]



#### 6) SoftMax

![[Pasted image 20240312071256.png]]
![[Pasted image 20240312071310.png]]


1) 출력값이 N개
2) 입력값을 각각 지수함수로 취하고 이를 정규화(=총합을 1로 만듦)
3) 정규화로 인해 각 출력값은 0~1을 가짐
4) 모든 출력값의 합은 반드시 1
5) N가지 중 한 가지에 속할 확률 표현 가능


>![Note] 자연상수의 지수형태를 사용하는 이유
> - 많은 데이터 분석이나 분석 모델링의 계산 과정에서 자연상수 $e$ 를 사용하거나 $log$ 형태를 사용하여 계산의 편의를 도모하는 경우기 있다. SoftMax 함수 역시 같은 이유에서 $e$의 지수형태를 사용한다.
> 	- **수치 안정성:** 소프트맥스 함수를 직접 계산할 때, 입력값 x가 매우 커지면 exp(x) 값이 급격히 증가하여 오버플로(overflow)가 발생할 수 있다. 반대로, x가 매우 작아지면 exp(x) 값이 0에 가까워져 언더플로(underflow)가 발생할 수 있다. 자연상수 지수 형태를 사용하면 이러한 문제를 완화할 수 있다.
> 	- **계산 효율성:** 지수 함수는 미분하기 쉽고, 경사 하강법과 같은 최적화 알고리즘에서 효율적으로 사용될 수 있다.
> - $log$ 형태를 취하는 이유도 비슷하다.
> 	- **수치 안정성:** 매우 작은 값이나 매우 큰 값을 다룰 때, 로그를 취하면 값의 범위를 줄여 수치적 안정성을 높일 수 있다. 특히, 확률과 같이 작은 값을 다룰 때 유용.
> 	- **곱셈을 덧셈으로 변환:** 로그의 성질에 따라 log(a * b) = log(a) + log(b)가 성립. 따라서 곱셈 연산을 덧셈 연산으로 변환하여 계산을 단순화하고 수치적 안정성을 높일 수 있다.
> 	- **경사 하강법과의 호환성:** 로그 함수는 미분하기 쉽고, 경사 하강법과 같은 최적화 알고리즘에서 효율적으로 사용될 수 있다. 특히, 교차 엔트로피 손실 함수와 같이 로그를 포함하는 손실 함수를 사용할 때 유용.


- pytorch 활성함수
https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions


