---
categories: 글쓰기
title: 2.3.2. 데이터의 표현 - Tensor
created: 2025-03-15
tags:
  - 교재
  - 수업
  - Tensor
---
---
#### *2.3.2. 데이터의 표현 - Tensor*
---

텐서(Tensor)는 다차원 배열(array) 또는 행렬(matrix)를 일반화한 개념으로, 머신러닝 및 딥러닝에서 데이터를 표현하는 방법으로 수학, 물리학, 컴퓨터 과학 등 다양한 분야에서 사용되며, 데이터의 구조와 관계를 나타내는 데 유용하다.

- **다차원 배열:** 텐서는 스칼라, 벡터, 행렬을 포함하는 다차원 배열.
    - 스칼라: 0차원 텐서 (단일 숫자)
    - 벡터: 1차원 텐서 (숫자들의 배열)
    - 행렬: 2차원 텐서 (숫자들의 2차원 배열)
    - 3차원 이상: 3차원 이상의 텐서는 데이터를 더 복잡한 형태로 표현. 예: 이미지는 3차원 텐서(높이, 너비, 색상 채널)로 표현.
- **랭크(Rank):** 텐서의 차원 수. 예: 벡터 => 랭크 1, 행렬 => 랭크 2.
- **모양(Shape):** 각 차원의 크기. 예: 3x4 행렬 => shape (3, 4).
- **데이터 타입(Data Type):** 텐서에 저장된 데이터의 종류. 예: 정수, 실수, 문자열 등


- 정량화한 데이터를 표현하는 방식

| 차원                | 이름         | 설명                                                                                             | 예시                                                           |
| ----------------- | ---------- | ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------ |
| 0 dim tensor      | scalar 스칼라 | 하나의 값을 가지는 데이터 타입. 단일한 값으로 방향의 개념이 없음.                                                         | $[2],[3]$                                                    |
| 1 dim tensor      | Vector 벡터  | 크기와 방향을 모두 가지는 데이터 타입. 숫자의 배열, 행 벡터(데이터 형태), 열 벡터 (수학적 연산). 계열(column), 케이스(row, sample, case) | $[6,7,8]$                                                    |
| 2 dim tensor      | Matrix 행렬  | 계열의 집합, 행 또는 샘플의 집합, 2차원 배열로 구성. DataFrame, Table, 행과 열로 구성, 각 행렬의 위치 인덱스로 표현가능                | $[[1,3,4],[5,2,9]]$                                          |
| 3 dim over tensor | Tensor 텐서  | 행렬의 중첩. 다차원의 배열로 구성된 데이터 (개념적으로는 하위 차원의 데이터 타입을 포함)                                            | $[[[1,2],[3,4]],[[5,6],[7,8]]]$ , 시계열, 동영상 $\cdots$ 고차원의 데이터 |

##### 노름 or 놈(norm) 
- 벡터의 크기를 부여하는 함수. (크기의 측정하는 기준)
- 선형대수학 - 벡터의 크기(magnitud) 또는 길이(length)를 측정하는 방법
	
	- 1-norm : L1-norm (Manhatten norm, Taxi norm):
		맨하탄 거리의 블럭의 바둑판 직각에서 택시가 움직이는 거리를 측정하는 방식. 절대값. 차이의 절대값
	$$||\mathcal{X}||_{1}\quad=\quad \sum_{i=1}^{n}\ |x_{i}|$$
	- 2-norm : L2-norm (Euclidean norm)
		일반적인 직선의 거리. 대각선의 길이. 다차원의 직통의 거리.
		$$||\mathcal{X}||_{2}\ =\ \sqrt{x^{H}x} =\ \sqrt{\sum_{i=1}^{n}\ |x_{i}|^{2}}$$
	- 기타 p-norm, $\infty -norm$ $\cdots$ 
	- (normed space) : 놈이 되기 위한 조건. 놈이 될 수 있는 장(field, vector space)

- **특정한 공간에서의 노름의 표현**

	$Lp$-space or $Lebegue$ Space $\Longrightarrow$ $Lp-norm \quad or \quad p-norm$ 

$$
\begin{align}
	&||X||_{p} = \Big( \sum_{i=1}^{n}|x_{i}|^{p} \Big)^{1/p}\ ,\quad 1\le p \le \infty \\ \\
	& p,\ n \in \mathbb{R},\quad p:norm's\ dim,\quad n: vertor's\ dim
\end{align}
$$

![[norm_vector_dim.png|700]]
2차원 벡터 공간에서 p값 변화에 따른 p-norm의 분포 형태 (출처 : https://ekamperi.github.io/machine%20learning/2019/10/19/norms-in-machine-learning.html)

- p=1 이 L1의 형태, p=2 가 L2의 형태이다.

![[3D_norm_dim.png|700]]
3차원 벡터 공간에서 p값 변화에 따른 p-norm의 분포 형태 (출처 : https://wikidocs.net/21467)


```python
import numpy as np

# 벡터 정의
vector = np.array([1, 2, 3, 4])

# L1 norm 계산 (맨해튼 거리)
l1_norm = np.linalg.norm(vector, ord=1)
print(f"L1 norm: {l1_norm}")

# L2 norm 계산 (유클리드 거리)
l2_norm = np.linalg.norm(vector)  # ord=2는 기본값이므로 생략 가능
print(f"L2 norm: {l2_norm}")

# 무한대 norm 계산 (최대 절대값)
inf_norm = np.linalg.norm(vector, ord=np.inf)
print(f"Infinite norm: {inf_norm}")

# 특정 p-norm 계산 (예: p=3)
p_norm = np.linalg.norm(vector, ord=3)
print(f"p-norm (p=3): {p_norm}")

# 행렬의 Frobenius norm 계산
matrix = np.array([[1, 2], [3, 4]])
frobenius_norm = np.linalg.norm(matrix, ord='fro')
print(f"Frobenius norm: {frobenius_norm}")
```

```python
import torch

vector = torch.tensor([5.0, 7.0])

L1_norm = torch.norm(vector,p=1)
L2_norm = torch.norm(vector,p=2)

print(f"L1 norm : {L1_norm}")
print(f"L2 norm : {L2_norm}")

```


- 일종의 거리 개념으로 접근하게 되면 차이의 측정하는 방법으로도 사용
- 또는 특정 함수식의 패널티의 크기를 결정하는 방법으로 사용 (Lasso, Ridge)

- 고차원의 데이터를 일반 함수식(미적분적 접근)으로 계산하는 것은 힘듬.
- 컴퓨터를 사용하여 연산하기 쉬운 형태 (단순화, 반복화)
- 따라서 머신러닝에서는 행열을 이용한 연산화를 많이 사용함.
- 따라서 선형대수(Linear Algebra)와 행렬(배열:array)의 데이터를 다루는 numpy가 중요함.

### 1. pytorch : Tensor

```python
import torch
import numpy as pd

# 파이썬 리스트의 중첩형태에서 바로 tensor 생성 가능 
data = [[1, 2], [3, 4]]
x_data = torch.tensor(data)
x_data

# numpy 배열에서 생성 가능. (역으로 torch tensor 에서 numpy array로 변환 가능)
np_array = np.array(data)
x_np = torch.from_numpy(np_array)
x_np

x_ones = torch.ones_like(x_data) # x_data의 속성을 유지합니다.
print(f"Ones Tensor: \n {x_ones} \n")

x_rand = torch.rand_like(x_data, dtype=torch.float) # x_data의 속성을 덮어씁니다.
print(f"Random Tensor: \n {x_rand} \n")
```

##### [입력(Inputs)] :  

- 학습하고자 하는 대상으로 부터 수집된 데이터로 대상의 정보를 가지고 있음.
- 대상 또는 모집단의 상태, 특성, 성질, 활동 등에 대해 수집된 샘플 (전체 모집단은 아님) 
- 그런 의미에서 feature라고도 함 (기능, 속성, 특징)
- feature는 추상적이고 함축적인 요인(factor) 또는 잠재요인(latant factor)으로 함축 시키기도 함.
- 하나의 feature는 하나의 차원(dimention)을 형성한다. 데이터 x가 d개의 feature를 가진다고 하면
$$
\begin{align}
	& x\quad \in \quad \mathbb R^{d} \\ \\
	&x\quad=\quad 
				\begin{bmatrix}
					x_{1} \\
					x_{2} \\
					x_{3} \\
					\vdots \\
					x_{d} \\
					 
				\end{bmatrix}
\end{align}
$$
 처럼 d 차원을 가진다.
- tensor
![[Pasted image 20240312161423.png]]
https://pytorch.org/docs/stable/tensors.html

- pytorch 나 tensorflow는  tensor를 노드로 가지는 그래프 기반 연산 구조를 가지고 있다.

>[!Warning] 생각해 볼 것! 
> [분류 문제와 데이터의 균형 문제: Classification]
> 1) 통계적인 관점에서 모집단 전체를 학습하는 것은 불가능 
> 2) 분포적 관점에서 접근하면, 일반적인 (General) 보편적인 상태를 학습하게됨
> 3) 확률적으로 발생하기 힘든 특이한 상황도 파악하기 위해선 그러한 case도 학습해야함 => 학습 샘플 데이터의 균형 필요 

>[!Warning] 이것도 생각해 볼 것!
> [이상치 감지 문제: Anomaly Detection]
> 1) 정상적인 (Normal) 한 상태를 학습
> 2) 정상상태를 벗어난 상태를 구분해야함.
> 3) 이진 분류로서 이상치 경우도 학습해야 하는 것이 아닌가? *어떻게 생각함?*

