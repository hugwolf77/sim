---
categories: 글쓰기
title: 2.3.3. 신경망 모델 학습 과정 - Nueral Net Learning
created: 2025-03-12
tags:
  - 교재
  - 수업
  - Neural_Network
---
---
#### *2.3.3. 신경망 모델 학습 과정 - Nueral Net Learning*
---

#### 2.3.3.1. 신경망 학습 과정

- 입력값(Input) : tensor로 이뤄진 현상이나 데이터의 표현으로 특정한 행과 열의 특성을 가지고 있음.
- 가중치(Weight) : 입력값에 대응하여 작용하는 계수로 입력 변수의 차원 만큼 존재한다.
- 출력값(Output) : 신경망을 통과한 추론의 결과로 학습하고자 하는 정답 또는 진리 값을 나타내는 데이터의 형태(갯수나 크기)를 가지고 있음.
- 가중합(Weighted Sum) : 입력값들에 각각의 가중치를 곱해준 후 모두 합치고 편차(Bias)를 더해준 값.
- 활성함수(Activation Function) : 출력 신호를 만들어 마치 신경망의 자극 또는 반도체의 비트 표현과 같은 작용을 하는 함수.

	- 활성함수의 대표적 예 : 시그모이드함수의 대표인 로지스틱 함수형태 
$$Sigmoid(\sigma) = \frac{1}{1+e^{-x}} = \frac{e^{x}}{e^{x}+1}$$
![[sigmoid_logistic.png|500]]


![[신경망 학습 과정.png#center|200]]

- 단층 신경망을 행렬식으로 표현하면 다음과 같다.

$$ 
\begin{align}
W = [ w_{1},w_{2},\dots,w_{n}] \\
X = \begin{bmatrix} x_{1}\\ x_{1}\\ \vdots x_{n}\\\end{bmatrix} \\
\\
y = \sigma (WX + b) = \sigma(\sum_{i=1}^{n}w_{i}x_{i} + b )
\end{align}
$$

- **손실함수(Loss Function)**
	- 예측한 값과 실제 값 사이의 차이를 측정하는 함수

	- MSE(Mean Squared Error) : 가장 많이 쓰이는 손실함수
$$L = \frac{1}{N}\sum_{i=1}^{N}(pedict_{i}-true_{i})^{2}$$

#### 2.3.3.2. 가중치의 업데이트 방법 - 학습

- 대표적 학습 기법 : 경사하강법 (Gradient Decent)

![[Gradient descent.png|500]]

#### 2.3.3.3. 학습률(Learning Rate: $\alpha$)

- 모델이 학습 과정에서 매개변수를 얼마나 업데이트(step-size)할지 결정하는 중요한 하이퍼파라미터

$$  \begin{align}
	&w = w -\alpha\frac{\partial L(w)}{\partial w}\\
	&b = b -\alpha\frac{\partial L(b)}{\partial\ b}
	\end{align}
$$
- 적정하지 못한 학습률이 미치는 영향
	- gradient exploding : 최적점을 지나쳐 발산하거나 불안정한 학습을 초래 상태.
	- gradient ocilliation : 손실 함수가 급격하게 변동하여 학습이 제대로 이루어지지 않는 상태.
	- 너무 작은 학습률은 최적점에 수렴하는 데 너무 오랜 시간이 걸림.
	- 지역 최적점(Local Minima)에 갇혀 더 나은 성능을 내지 못할 수 있음.

- 학습률 설정 방법
	- 학습률 감쇠(Learning Rate Decay): 특별한 스케줄에 의해 학습률을 조정. 학습률 계획법(learning rate scheduling)들을 적용한다.
	- 적응적 학습률(Adaptive Learning Rate): 모델의 학습하는 최적화 기법들을 적용(opimizer). 경사하강법도 일종의 최적화 기법이다.


---
-  example : 소프트맥스 함수를 이용한 분류 문제 (bias가 없음.)

![[example_slp.png|200]]
-
	- 순전파(Forward) :
		$$  \begin{align}
			score_{1}= w_{11}x_{1}+w_{12}x_{2}+w_{13}x_{3}+w_{14}x_{4}\\
			score_{2}= w_{21}x_{1}+w_{22}x_{2}+w_{23}x_{3}+w_{24}x_{4}\\
			score_{3}= w_{31}x_{1}+w_{32}x_{2}+w_{33}x_{3}+w_{34}x_{4}\\
			 \end{align}
		 $$
	- 활성함수(softmax) :
		$$  \begin{align}
			y_{1}=\frac{e^{score_{1}}}{e^{score_{1}}+e^{score_{2}}+e^{score_{3}}}\\
			y_{2}=\frac{e^{score_{2}}}{e^{score_{1}}+e^{score_{2}}+e^{score_{3}}}\\
			y_{3}=\frac{e^{score_{3}}}{e^{score_{1}}+e^{score_{2}}+e^{score_{3}}}\\
			 \end{align}
		 $$
	- 손실함수 계산
$$L(w)=\sum_{n=1}^{N}\sum_{k=1}^{3}L(\hat y_{k},y_{k})$$
	- 역전파(backward) : 가중치 업데이트 (학습)
	$$w_{ij}=w_{ij} - \alpha\frac{\partial L(w)}{\partial w_{ij}}$$
---
#### 2.3.3.4. 단층 신경망의 한계

1) **비선형 문제 해결의 어려움**
	- 단층 신경망은 선형 분리 가능한 문제만 해결할 수 있음. 데이터를 직선이나 평면으로 나눌 수 있는 경우에만 작동.
	- 현실 세계의 대부분의 문제는 비선형적. 단층으로는 복잡한 패턴을 학습하고 분류하는 것이 불가능.
	- 대표적인 예시로 XOR 문제(배타적 논리합). XOR 문제는 단층 신경망으로는 해결할 수 없으며, 다층 신경망이 필요.
![[Pasted image 20240304152443 1.png]]
![[Pasted image 20240304152428 1.png]]

2) **복잡한 패턴 학습의 한계**
	- 이미지 인식, 자연어 처리 등 복잡한 문제의 복잡한 패턴 특징을 학습할 수 있는 능력이 제한적이다.

- 이러한 문제를 해결하기 위해서 은닉층(Hidden Layer)를 추가하여 다층신경망을 만들 필요가 있었다. 그러나 역전파를 위한 깊은 층의 가중츠 경사를 계산의 어려움. 그리고 연산을 위한 컴퓨팅 자원의 급증 등으로 어려움이 있었다.

#### 2.3.3.5. 다층 신경망 (MLP : Multi Layer Perceptron)

![[Pasted image 20240304151218 1.png]]

https://ang-love-chang.tistory.com/26

- 다층으로 이뤄진 신경망 layer로 구성된다.
- 다변수 함수의 미분(**편미분**)과 합성합수의 미분(**연쇄법칙:Chain Rule**)을 통해서 다중 layer의 gradient를 계산할 수 있게 되었다.
- 이를 통해 단순한 한 신경망 layer의 업데이트에서 복잡한 다층의 layer 들에 속한 가중치들의 업데이트가 가능해짐. 이를 **역전파(Back-Propagation)** 이라고 한다.

#### 2.3.3.5. 다층신경망의 문제

1) **가중치 소실(Gradient Vanishing)** 이 강해지는 문제

![[gradient_Vanishing.png|700]]
- **원인**
	- **활성화 함수(Activation Function)의 특성**: 시그모이드(Sigmoid)나 하이퍼볼릭 탄젠트(Hyperbolic Tangent)와 같은 활성화 함수의 출력값의 형태는 입력값이 커지거나 작아질수록 기울기가 0에 가까워지는 특성이 있음. 
	- **심층 신경망의 깊이**: 이러한 활성화 함수를 여러번 거치면서 기울기가 계속해서 곱해지기 때문에, 지수적으로 감소하여 입력층에 가까운 층에서는 기울기가 거의 0에 수렴.		 
- **영향**
	- **학습 속도 저하**: 기울기가 너무 작아지면 가중치(Weight) 업데이트가 거의 이루어지지 않아 학습 속도가 매우 느려짐.
	- **학습 성능 저하**: 앞쪽 층은 입력 데이터의 특징을 추출하는 중요한 역할을 담당하는데, 기울기 소실로 인해 앞쪽 층의 가중치가 제대로 학습되지 않아 전체적으로 학습 성능이 저하됨.
	- **지역 최적점(Local Minima) 문제**: 기울기가 0에 가까워지면 학습이 지역 최적점에 갇혀 더 이상 진행되지 않을 가능성이 높아짐.

2) **과적합(overfitting)** 이 심해지는 문제
		: 학습 데이터에 지나치게 특화되어 실제 데이터에 대한 예측 성능이 떨어지는 현상
		- 일반화 성능 저하 원인
		- 새로운 데이터에 대한 예측 실패
		- 모델의 신뢰성 저하
![[overfitting.png]]
3) 연산량이 급격히 증가하는 문제


#### 2.3.3.6. 다층 신경망(Deep learning)의 발전 기법들
- 가중치 초기화 (weight initiation)
- 활성함수의 변경
- 여러가지 최적화기법 (optimizer)
- 정규화 Regularization
- 확률가중치 선택 Dropout
- 배치 정규화 batch normalization
- 레이어 정규화 layer normalization
- 평가 데이터(validation data)를 이용한 조기학습종료(Early Stopping)
- 잔차 연결(Residual Connection)

---

- 일반적인 머신러닝 기법에서의 추가적인 과적합(overfitting) 대응
1) **데이터 증강(Data Augmentation):** 학습 데이터를 늘려 모델이 다양한 데이터 패턴을 학습.
2) **교차 검증(Cross-Validation):** 데이터를 여러 개의 하위 집합으로 나누어 모델의 성능을 평가.

---
#### 2.3.3.7. 데이터의 분할과 학습

1. 데이터 분할:
	- 학습 데이터의 준비와 분할
		1) 학습 데이터 준비
		2) 학습 데이터 분할
			- Train DataSet
			- Validation DataSet
			- Test DataSet
2. Epoch
	- 전체 데이터에 대한 학습 회수
	- 의미
3. batch
	- 데이터의 소분할(mini-batch)
	- 학습과의 관계
4. step
	- 실제 학습하는 데이터의 단위

