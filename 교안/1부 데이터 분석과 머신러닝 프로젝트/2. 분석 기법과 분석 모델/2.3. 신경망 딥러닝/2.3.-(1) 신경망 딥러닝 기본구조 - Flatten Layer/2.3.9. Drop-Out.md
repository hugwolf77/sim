---
categories: 글쓰기
title: 2.3.9. Drop-Out
created: 2025-03-22
tags:
  - 교재
  - 수업
  - DropOut
  - DL
---
---
#### *2.3.9. Drop-Out*
---

![[DropOut.png]]

- 일종의 규제(reqularization) 또는 정규화의 일종
- 학습 과정에서 각 미니배치(Mini-batch)마다 무작위로 일부 노드를 비활성화(Dropout)하여 가중치가 학습되지 않도록 함.
- 정해진 확률(일반적으로 0.2~0.5)에 따라 노드를 **무작위 비활성화**.
- 각 가중치는 다른 가중치와 무작위로 영향 관계가 끊기고 독립적으로 학습하도록 강제 됨. 
- **예측(Inference) 과정**에서는 모든 가중치를 활성화하고, 학습 시 비활성화했던 비율만큼 출력값을 조정(비활성화된 노드의 비율 만큼 활성화로 남아 있는 노드의 출력값이 커지게 된다고 함. 따라서 그만큼 예측시에 같은 비율로 출력값을 줄여주어야 함).

#### 효과
- 각 미니배치마다 다른 조합의 노드 조합으로, 마치 여러 개의 모델을 앙상블(Ensemble)하는 효과가 있어 모델의 성능을 높인다고 함.
- 학습 데이터의 특정 패턴에 지나치게 적응하는 것을 방지하여 과적합을 방지한다고 알려짐.
- 특정 노드 연결에 과도하게 의존하는 것을 방지하여 모델의 강건성(Robustness)을 향상시키는 것으로 알려짐.

#### 단점
- 학습 시간을 증가 시키는 것으로 알려짐.
- Drop-Out Rate 는 일종의 Hyper-parametor로서 적정한 값을 찾는 것이 문제가 됨.


```python
import torch 
import torch.nn as nn 
import torch.optim as optim 

# 신경망 모델 정의 
class SimpleNN(nn.Module): 
	def __init__(self): 
		super(SimpleNN, self).__init__() 
		self.fc1 = nn.Linear(784, 256) 
		self.dropout = nn.Dropout(p=0.5) 
		self.fc2 = nn.Linear(256, 10) 
		
	def forward(self, x): 
		x = torch.relu(    self.fc1(x)) 
		x = self.dropout(x) 
		x = self.fc2(x) 
		return x 

# 모델 초기화 
model = SimpleNN() 
# 손실 함수와 최적화 알고리즘 설정 
criterion = nn.CrossEntropyLoss() 
optimizer = optim.SGD(model.parameters(), lr=0.01)
```

---
##### Reference
- Dropout: A Simple Way to Prevent Neural Networks from Overfitting (2014)