---
categories: 글쓰기
title: 2.3.7. 학습 최적화(learning optimization) 방법
created: 2025-03-22
tags:
  - 교재
  - 수업
---
---
#### *2.3.7. 학습 최적화(learning optimization) 방법*
---

#### *2.3.7.1. 학습경로 탐색의 어려움*

  - 손실함수(목적함수)가 항상 미분으로 쉽게 최저점 방향을 찾는 가능한 형태로 주어지는 것은 아님.
  - 목표로하는 전역최저점(Global Minimum point)을 찾는 과정은 어려움이 있다.
	  - 미분으로 경사(gradient)가 0 인 지역은 전역점만이 있는 것이 아니라 지역최저점(Local Minimum point)이 존재한다.
	  - 다차원의 공간 속에서 항상 볼록(comvex)한 한쪽 방향이 있는 것이 아니다. 오목(concave)하거나 완만하거나 복잡한 형태를 띠고 있어 항상 최적의 경로를 찾을 수 있는 것은 아니다.
	  - 대표적인 예로 Saddle Point (말안장점; 미분해서 0이 된다고 해서 무조건 최적점이 아닐 수 있다) 등이 있다.


- Gradient로 항상 최적 경로를 찾기 어려운 지형
![[convex_concave.png]]

- 복잡한 Gradient Feild
![[gradient_field.png]]

- 지역최소점 존재
![[local_minimum.png]]


#### *2.3.7.2. 학습 최적화 기법들*


#### *1. 경사하강법(Gradient descent method)* 

- 아래 볼록 함수(convex function)의 최솟값을 찾는 데 사용되는 반복적인 최적화 알고리즘 -> 함수의 기울기를 계산하여 그 반대 방향으로 이동하면 함수의 최솟값에 가까워지는 방법

$$
\begin{align}
	& w_{t+1} \quad = \quad w_{t} \quad - \quad \eta\quad \triangledown f(w_{i})
\end{align} 
$$

- **원리:**
    - 손실 함수의 기울기(gradient)를 계산하여 손실을 최소화하는 방향으로 모델의 가중치를 반복적으로 업데이트.
    - 기울기는 손실 함수가 가장 가파르게 증가하는 방향을 나타내므로, 음의 기울기 방향으로 이동하면 손실을 줄일 수 있다.
- **종류:**
    - **배치 경사하강법 (Batch Gradient Descent):**
        - 전체 학습 데이터셋을 사용하여 각 업데이트 단계에서 기울기를 계산.
        - 장점: 안정적인 수렴이 가능.
        - 단점: 데이터셋이 큰 경우 계산 비용이 매우 높다.

	- **확률적 경사하강법 (Stochastic Gradient Descent, SGD):**
        - 각 업데이트 단계에서 하나의 무작위로 선택된 데이터 포인트에 대해서만 기울기를 계산.
        - 장점: 계산 비용이 낮고, 큰 데이터셋에서도 빠르게 학습할 수 있다.
        - 단점: 불안정한 업데이트로 인해 수렴이 불안정할 수 있다.

#### 2. **미니배치 경사하강법 (Mini-batch Gradient Descent)**

   - 미니배치라고 불리는 작은 크기의 데이터 서브셋에 대해 각 업데이트 단계에서 기울기를 계산다.
   - 장점: 배치 경사하강법과 확률적 경사하강법의 장점을 결합하여 안정성과 효율성을 모두 갖는다.
   - 단점: 미니배치 크기를 적절하게 설정해야 한다.

>[!Note] torch - SGD optimize
>- pytorch의 SGD 는 실제로는 Mini-batch Gradient Descent를 의미한다.


#### 3. 모멘텀 (momentum 관성학습) 

- 경사하강법의 문제를 보안하기 위해서  하강 방향의 momentum(관성)을 사용하는 방법

$$
\begin{align}
	& a_{t+1} = \quad\beta a_{t} \quad+ \quad g_{t} \\ \\
	& w_{t+1} \quad = \quad w_{t} \quad - \quad \eta\quad a_{t+1} \\ \\
	&\beta = momentum \\
	&\eta = learning rate \\
	&a_{t} = accumulation \\
	&g_{t} = \triangledown f(w_{i})_{t}
\end{align} 
$$
- 지금까지 학습에 사용된 기울기를 학습에 반영함.
- 학습의 방향이 계산된 기울기(gradient)의 진동에 의한 급변(shooting)에 대하여 어느정도 안정된다.


#### 4. NAG (Nesterov Accelerated Gradient) 

-  momentum 과 유사하지만 현재 시점에서 계산된 한 스텝 앞의 기울기도 현재 학습에 반영하는 것

$$
\begin{align}
	& a_{t+1} = \quad\beta a_{t} \quad+ \quad\triangledown L(w_{t}-\eta\beta a_{t}) \\ \\
	& w_{t+1} \quad = \quad w_{t} \quad - \quad \eta\quad a_{t+1} \\ \\
	&\triangledown L(w_{t}-\eta\beta a_{t}) = Lookhead \ \ gradient
\end{align} 
$$

#### 5. Adagrad

$$
\begin{align}
	
	& w_{t+1} \quad = \quad w_{t} \quad - \quad \frac{\eta}{\sqrt{G_{t}}+\epsilon}\quad g_{t} \\ \\
	& G_{t} = Sum\ of\ gradient\ squares \\
	& \epsilon \ = for\ \ numerical\ stabillity\\
	& g_{t} = \triangledown f(w_{i})
\end{align} 
$$
- 지금까지 학습된 경사기울기를 정보를 학습률(learning rate)에 반영시키는 방법
- 많이 변화한 가중치는 적게 변화시키고 그렇지 않은 가중치는 상대적으로 많이 변화 시켜 주는 방법
- 그러나 학습이 길어지면 $G_{t}$가 지속적으로 커지면서  학습률이 0 에 가까워져 학습이 안되는 단점이 있다.

#### 6. Adadelta
$$
\begin{align}
	& G_{t} \quad= \quad \gamma G_{t-1} + (1-\gamma)g_{t}^{2} \\ \\	
	& w_{t+1} \quad = \quad w_{t} \quad - \quad \frac{\sqrt{H_{t-1}}+\epsilon}{\sqrt{G_{t}}+\epsilon}\quad g_{t} \\ \\
	& H_{t} \quad= \quad \gamma H_{t-1} + (1-\gamma)(\triangle w_{t})^{2} \\ \\
	& G_{t} = EMA\ of\ gradient\ squares \\
	& H_{t} \ = EMA\ of\ difference\ squares\\
	& g_{t} = \triangledown f(w_{i}) \\
	& EMA = exponetial\ \ moving \ \ average
\end{align} 
$$

- $G_{t}$ 가 계속 커지는 것을 막기위해  일정 크기 만큼의 기울기 반영 시키고자함.
- 그러나 일정 크기의 기울기만 반영하기 위한 저장 공간 관리가 비효율적이라  지수평균을 이용함
- learning rate 가 별도로 없음.

#### 4. RMSProp 

-  Adagrad 문제점 보완: Adagrad 알고리즘은 과거의 학습률 누적이 너무 많아 업데이트 속도가 너무 느려지는 문제가 있
- Adam 보다 메모리 요구사항이 적게 들어감.
$$
\begin{align}
	& G_{t} \quad= \quad \gamma G_{t-1} + (1-\gamma)g_{t}^{2} \\ \\	
	& w_{t+1} \quad = \quad w_{t} \quad - \quad \frac{\eta}{\sqrt{G_{t}}+\epsilon}\quad g_{t} \\ \\
	& G_{t} = Sum\ of\ gradient\ squares \\
	& \epsilon \ = for\ \ numerical\ stabillity\\
	& g_{t} = \triangledown f(w_{i})	
\end{align}
$$

#### 8. *Adam (Adaptive Moment Estimation)*

$$
\begin{align}

	& m_{t} \quad = \quad \beta_{1}m_{t-1}+(1-\beta_{1})g_{t} \\ \\
	& V_{t} \quad = \quad \beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2} \\ \\
	& w_{t+1} =\quad w{t}\quad -\quad \frac{\eta}{\sqrt{v_{t}}+\epsilon} \quad \frac{\sqrt{1-\beta_{2}^{t}}}{1-\beta_{1}^{t}}\quad m_{t} \\ \\
	&m_{t} = gradient \\
	& v_{t} = EMA\ \ of \ \ gradient\ \ squares \\
	& \epsilon =  for\ \ numerical\ stabillity\\

\end{align}
$$
- 가장 무난하게 많이 사용
- 과거의 기울기(gradient)와 과거의 squared 기울기(gradient)를 모두 이용함
- adaptive learning rate 방법과 momentum 계열 방법을 적당히 합친 방법



##### 최적화 기법들의 관계

![[Optimizer.png]]


https://pytorch.org/docs/stable/optim.html

---
pytorch AugoGrad
https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html


pytorch train
https://pytorch.org/tutorials/beginner/introyt/trainingyt.html


- pytorch 예제 : 3차식 모델을 딥러닝으로 바꾸는 과정
https://pytorch.org/tutorials/beginner/pytorch_with_examples.html


