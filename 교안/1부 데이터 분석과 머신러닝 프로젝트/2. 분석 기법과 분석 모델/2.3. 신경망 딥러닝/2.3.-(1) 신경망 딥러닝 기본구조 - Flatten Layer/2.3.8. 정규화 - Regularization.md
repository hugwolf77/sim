---
categories: 글쓰기
title: 2.3.8. 정규화 - Regularization
created: 2025-03-22
tags:
  - 교재
  - 수업
  - Regularization
  - ML
---
---
#### *2.3.8. 정규화 - **Regulization***
---

- Drop-Out의 '규제'라고 하면서 왜 '정규화'라고하는지 모르겠다.

- 모델의 과적합(Overfitting)을 방지하고 일반화 성능을 향상시키기 위하여 모델의 가중치가 작용하는 크기에 따라서 발생하는 오차에 대해 추가로 페널티를 부과하는 방법.

- **주요 정규화 기법:**

1) **L1 정규화 (L1 Regularization):**
    - 가중치의 절대값의 합을 손실 함수에 추가.
    - 일부 가중치를 0으로 만들어 모델을 희소하게 만듬.
    - 특성 선택(Feature Selection) 효과가 있어 중요한 특성만 남기고 불필요한 특성을 제거.
$$
\begin{align}
	Loss_{L1}(W)\ 
	&=\ Loss(W)\ +\ \lambda\ \cdot\ ||W|| 
	\\ \\
	&=\ \sum_{i=0}^{n}(y_{i}- \sum_{j=0}^{M}x_{ij}W_{j})^{2}\ +\ \lambda\ \cdot\ \sum_{j=0}^{M}|W_{j}| 
\end{align}
$$

2) **L2 정규화 (L2 Regularization):**
    - 가중치의 제곱의 합을 손실 함수에 추가.
    - 가중치의 크기를 줄여 모델의 복잡도를 낮춤.
    - 모든 가중치를 0에 가깝게 만들지만, 완전히 0으로 만들지는 않음.

$$
\begin{align}
	Loss_{L2}(W)\ &=\ Loss(W)\ +\ \lambda\ \cdot\ ||W||^{2}
	\\ \\
		&=\ \sum_{i=0}^{n}(y_{i}- \sum_{j=0}^{M}x_{ij}W_{j})^{2}\ +\ \lambda\ \cdot\ \sum_{j=0}^{M}W_{j}^{2} 
\end{align}
$$


- 정규화가 최적 가중치에 미치는 영향 
![[파라메터와_L1L2.png]]

- 선형모델과의 관계
![[선형모델과의관계.png]]


---

- torch는 기본적으로 L2 형태의 weight decay를 지원한다는 주장과 실제로 L2로 실행된다고 하는 의견이 있음

```
In SGD optimizer, L2 regularization can be obtained by `weight_decay`. But `weight_decay` and L2 regularization is different for Adam optimizer. More can be read here: [openreview.net/pdf?id=rk6qdGgCZ](https://openreview.net/pdf?id=rk6qdGgCZ) 

– [Ashish](https://stackoverflow.com/users/2892806/ashish "388 reputation")

@Ashish your comment is correct that `weight_decay` and L2 regularization is different but in the case of PyTorch's implementation of Adam, they actually implement L2 regularization instead of true weight decay. Note that the weight decay term is applied to the gradient before the optimizer step [here](https://github.com/pytorch/pytorch/blob/40d1f77384672337bd7e734e32cb5fad298959bd/torch/optim/_functional.py#L94) 

– [Eric Wiener](https://stackoverflow.com/users/6942666/eric-wiener "6,007 reputation")

 [CommentedJan 21, 2022 at 16:18](https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch#comment125171392_46597531)

```

$$
\begin{align}
	Loss_{L2}(W)\ &=\ Loss(W)\ +\ \frac{1}{2}\cdot \lambda\ \cdot\ ||W||^{2}
	\\ \\
		&=\ \sum_{i=0}^{n}(y_{i}- \sum_{j=0}^{M}x_{ij}W_{j})^{2}\ +\ \frac{1}{2}  \cdot \lambda\ \cdot\ \sum_{j=0}^{M}W_{j}^{2} 
\end{align}
$$

```python
import torch
optimizier = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.9)

```


- 하드 코딩 구현

```python
import torch  
from torch.utils.data import DataLoader, TensorDataset  
from sklearn.model_selection import train_test_split  
import numpy as np  
import torch.nn as nn  
import torch.optim as optim  
import matplotlib.pyplot as plt

# Set seed for reproducibility  
np.random.seed(0)  
torch.manual_seed(0)

# Create a synthetic dataset  
X = np.random.randn(1000, 10).astype(np.float32)  
y = (np.random.randn(1000) > 0).astype(np.float32)  
  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  
  
train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))  
test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))  
  
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)


# Define the neural network model  
class SimpleNN(nn.Module):  
    def __init__(self):  
        super(SimpleNN, self).__init__()  
        self.fc1 = nn.Linear(10, 50)  
        self.fc2 = nn.Linear(50, 1)  
  
    def forward(self, x):  
        x = torch.relu(self.fc1(x))  
        x = self.fc2(x)  
        return x

# Define the training function with regularization  
def train_model(model, criterion, optimizer, train_loader, regularization_type=None, lambda_reg=0.01, epochs=20):  
    epoch_losses = []  
      
    for epoch in range(epochs):  
        model.train()  
        running_loss = 0.0  
          
        for inputs, targets in train_loader:  
            optimizer.zero_grad()  
            outputs = model(inputs)  
            loss = criterion(outputs.squeeze(), targets)  
              
            # Apply L1 regularization  
            if regularization_type == 'L1':  
                l1_norm = sum(p.abs().sum() for p in model.parameters())  
                loss += lambda_reg * l1_norm  
                
            # l1loss = loss + 0.005 * sum([torch.abs(p).sum() for p in model.parameters()])


            # Apply L2 regularization  
            elif regularization_type == 'L2':  
                l2_norm = sum(p.pow(2).sum() for p in model.parameters())  
                loss += lambda_reg * l2_norm  

			# l2loss = loss + 0.005 * sum([torch.square(torch.abs(p)).sum() for p in model.parameters()])


              
            loss.backward()  
            optimizer.step()  
              
            running_loss += loss.item() * inputs.size(0)  
          
        epoch_loss = running_loss / len(train_loader.dataset)  
        epoch_losses.append(epoch_loss)  
        print(f"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}")  
      
    return epoch_losses

# Define the evaluation function  
def evaluate_model(model, test_loader):  
    model.eval()  
    correct = 0  
    total = 0  
      
    with torch.no_grad():  
        for inputs, targets in test_loader:  
            outputs = model(inputs)  
            predicted = (outputs.squeeze() > 0.5).float()  
            total += targets.size(0)  
            correct += (predicted == targets).sum().item()  
      
    accuracy = correct / total  
    print(f"Accuracy: {accuracy:.4f}")  
    return accuracy

# Plot loss over epochs  
def plot_training_loss(losses, title):  
    plt.figure(figsize=(10, 5))  
    plt.plot(range(1, len(losses) + 1), losses, marker='o')  
    plt.title(title)  
    plt.xlabel('Epochs')  
    plt.ylabel('Loss')  
    plt.grid(True)  
    plt.show()

  
# Training and evaluating with L1 regularization
print("Training with L1 Regularization:")
model = SimpleNN()
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
l1_losses = train_model(model, criterion, optimizer, train_loader, regularization_type='L1', lambda_reg=0.01)
evaluate_model(model, test_loader)
plot_training_loss(l1_losses, 'Training Loss with L1 Regularization')

# Reinitialize model, optimizer, and train with L2 regularization
print("\nTraining with L2 Regularization:")
model = SimpleNN()
optimizer = optim.SGD(model.parameters(), lr=0.01)
l2_losses = train_model(model, criterion, optimizer, train_loader, regularization_type='L2', lambda_reg=0.01)
evaluate_model(model, test_loader)
plot_training_loss(l2_losses, 'Training Loss with L2 Regularization')
```


```python
def train(model, loss_func, optimizer, x, y, epochs,
    c=1.0, penalty='l1', sparse_threshold=0.0005, use_gpu=False):

    # for training error compute
    y_true = y.numpy()

    # regularity
    c = torch.FloatTensor([c])
    p = 1 if penalty == 'l1' else 2

    # cuda
    use_cuda = use_gpu and torch.cuda.is_available()
    if use_cuda:
        print('CUDA is {}available'.format('' if use_cuda else 'not '))
        model = model.cuda()
        x = x.cuda()
        y = y.cuda()
        c = c.cuda()

    # Loop over all epochs
    for epoch in range(epochs):

        # clean gradient of previous epoch
        optimizer.zero_grad()

        # predict
        y_pred = model(x)

        # defining cost
        loss = loss_func(y_pred, y)
        regularity =  torch.norm(model.fc.weight, p=p)
        cost = loss + c * regularity

        # back-propagation
        cost.backward()
        optimizer.step()

        if epoch % 100 == 0 or epoch <= 10:
            # training error
            if use_cuda:
                y_pred = y_pred.cpu()
            y_pred = torch.argmax(y_pred, dim=1).numpy()            
            train_accuracy = accuracy(y_true, y_pred)

            # informations
            l1_norm = torch.norm(model.fc.weight, p=1)
            l2_norm = torch.norm(model.fc.weight, p=2)
            parameters = model.fc.weight.data.cpu().numpy().reshape(-1)
            t = abs(parameters).max() * sparse_threshold
            nz = np.where(abs(parameters) < t)[0].shape[0]

            print('epoch = {}, training accuracy = {:.3}, l1={:.5}, l2={:.3}, nz={}'.format(
                epoch, train_accuracy, l1_norm, l2_norm, nnz))

    if use_cuda:
        model = model.cpu()

    return model
```