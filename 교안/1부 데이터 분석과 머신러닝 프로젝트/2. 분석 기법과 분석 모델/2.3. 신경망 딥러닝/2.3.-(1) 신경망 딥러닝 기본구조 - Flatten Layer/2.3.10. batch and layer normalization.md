---
categories: 글쓰기
title: 2.3.10. batch and layer normalization
created: 2025-03-22
tags:
  - 교재
  - 수업
  - batch-layer-normalization
  - DL
  - ML
---
---
#### *2.3.10. batch and layer normalization*
---


---
#### *batch normalization*
---

![[DataScaling.png]]

- Data Normalization (정규화) 
	- 다른 이름으로 scaler 를 통하여 데이터 샘플에 의해 feature 들이 가진 scale(척도)의 특성과 분포의 특징을 일정하게 정규화 해주는 작업.  

- Decorrelated (비상관화)
	- 데이터의 특성들 간의 선형적인 상관관계를 제거하는 과정.
	- 즉, 데이터의 공분산 행렬을 대각 행렬로 만드는 것
	- 주성분 분석(PCA)과 같은 방법을 사용하여 데이터의 주성분을 찾고, 이 주성분들을 새로운 축으로 사용하여 데이터를 변환

- Whitened (백색화)
	- 'decorrelated' 시키는 것뿐만 아니라, 각 특성의 분산을 동일하게 만드는 과정.
	- 데이터의 공분산 행렬을 단위 행렬(대각 성분이 1이고 나머지 성분이 0인 행렬)로 만드는 것. 즉, 데이터의 특성들이 서로 독립적이고 동일한 분산을 가지도록 변환.
	- PCA를 수행한 후, 각 주성분의 분산을 1로 정규화하는 과정을 추가.

##### 1. `Internal covariate Shift 현상`

- Covariate Shift
![[CovariateShift.png|400]]
	- `Covariate Shift`는 공변량 변화라고 부르며 입력 데이터의 분포가 학습할 때와 테스트할 때 다르게 나타나는 현상.
![[InternalCovariateShift.png|500]]
	- `Internal Covariate Shift`는 Batch 단위로 MLP를 학습하게 되면 Layer마다의 입력의 공분산(즉, 입력 feature 마다의 데이터의 분포와 다른 feature와의 공분산 형태)데이터의 분포가 다르게 되고 이러한 현상은 뒷단에 위치한 Layer일 수록 변형이 누적되어 input data의 분포와 최종 출력이 상당히 많이 달라게 된다. 결국, 모델의 parameter들이 일관적인 학습을 하기 어려워 학습 성능이 떨어지는 문제가 발생한다. 이러한 현상은 결국 모델의 실제 예측시 데이터의 입력 변화에 따른 성능을 저하시킨다.
	- 즉, `Covariate Shift`가 뉴럴 네트워크 내부에서 일어나는 현상을 `Internal Covariate Shift`라고하며, 네트워크의 각 Layer나 Activation마다 출력값의 데이터 분포가 Layer마다 다르게 나타나는 현상을 말한다.

`배치 정규화(Batch Normalization)`는  이러한 문제를 감소히키고 학습 속도를 향상시키고 안정화하는 데 사용되는 기술. 각 레이어의 활성화 함수 입력값을 평균이 0, 분산이 1인 분포로 정규화하여 학습을 안정화하고 성능을 향상시킴.
![[batch_normalize_1.png|400]]
#### 2. **배치 정규화의 작동 방식**

1) **미니 배치 단위 정규화:**
    - 신경망의 각 레이어에서 활성화 함수에 입력되는 값을 미니 배치 단위로 평균과 분산을 계산하여 정규화.
    - 정규화된 값은 평균이 0, 분산이 1인 분포를 갖도록 변환.
2) **스케일 및 이동:**
	    - 정규화된 값에 학습 가능한 스케일(scale) 파라미터와 이동(shift) 파라미터를 적용하여 최종 출력값을 생성합니다.
	    - 이를 통해 정규화된 분포가 항상 평균 0, 분산 1을 갖도록 제한하지 않고, 신경망이 최적의 분포를 학습할 수 있도록 합니다.

![[batch normalize_2.png]]

3) 구체적 방법 내용
	- layer input distribution이 항상 fixed distribution(평균 0, 분산 1)을 따르도록 하는 것은 Normalization Layer에서 Gradient Descent의 의미가 없어 진다는 것을 뜻한다. 
![[processBN.png|700]]

- BN은 Activation Layer 이전에 위치시킨다. Normalization은 Activation Layer의 non-linearity를 감소시킬 수 있다. Gradient Decent와 Activation의 관계성을 단순히 고정된 선형성(마치 선형 활성함수의 비선형성 불기능)처럼 작용하게 한다.
- $\gamma$ (scale 파라메터), $\beta$ (shift 파라메터) 파라메터는 학습하는 가중치를 부과하여 이러한 문제를 입력되는 batch와 Layer에 따라 적응 시키는 방법을 사용한다.
![[BN_learnable_w.png]]
#### 3. **배치 정규화의 효과**
- **학습 속도 향상:**
    - 내부 공변량 변화(Internal Covariate Shift) 문제를 완화하여 학습률 파라메터의 수치를 높을 수 있다고 함.이를 통해 학습 속도를 크게 향상시킬 수 있다 주장.
- **학습 안정화:**
    - 가중치 초기화에 대한 민감도를 줄여 학습을 안정화.이를 통해 다양한 모델 구조에서 안정적인 학습을 가능.
- **과적합 방지:**
    - 일종의 규제 효과를 제공하여 과적합을 방지하고 모델의 일반화 성능을 향상.

 `Optimization landscape` 즉, solution space를 smoothing 하기 때문
 
#### 4. **배치 정규화의 단점**
- 미니 배치 크기에 민감
- 테스트 시 배치 통계 사용

#### 5. IMHO
- 파라메터 학습으로 정말 ICS가 제거되는 것인지 불불명하다. (성능이 좋아 지는 것은 사실이다.)
- 이후 다른 논문들에서도 비슷한 shifting 방식들을 사용하여 성능이 좋아지는 것은 확인 되었다.
- 신경망 사이의 데이터가 가진 노이즈를 제거하는 효과가 있다고는 보지만 그 과정에서 어떠한 정보와 상호작용이 사라지는 알지 못한다.

https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html

```python
# With Learnable Parameters
m = nn.BatchNorm2d(100)
# Without Learnable Parameters
m = nn.BatchNorm2d(100, affine=False)
input = torch.randn(20, 100, 35, 45)
output = m(input)

```

```python
import torch
import torch.nn as nn

# 간단한 MLP 모델 정의
model = nn.Sequential(
    nn.Linear(784, 128),
    nn.BatchNorm1d(128),  # 배치 정규화 적용
    nn.ReLU(),
    nn.Linear(128, 10),
    nn.Softmax(dim=1)
)
print(model)
```

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# 간단한 MLP 모델 정의
model = keras.Sequential([
    layers.Dense(128, input_shape=(784,)),
    layers.BatchNormalization(),  # 배치 정규화 적용
    layers.ReLU(),
    layers.Dense(10, activation='softmax')
])

print(model.summary())
```


##### - Reference 
- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167)

---
#### *레이어 정규화 layer normalization*
---

- 배치 정규화(Batch Normalization)와 유사하지만, Layer 단위로 이뤄진다는 것이 다르다.

- **레이어 단위 정규화**:
    - 신경망의 각 레이어에서 활성화 함수에 입력되는 값을 레이어 단위로 평균과 분산을 계산하여 정규화.
    - 즉, 각 데이터 샘플에 대해 레이어 내의 모든 뉴런의 활성화 값을 사용하여 정규화를 수행합니다.
    - 정규화된 값은 평균이 0, 분산이 1인 분포를 갖도록 변환됩니다.
- **스케일 및 이동**:
    - 정규화된 값에 학습 가능한 스케일(scale) 파라미터와 이동(shift) 파라미터를 적용하여 최종 출력값을 생성합니다.
    - 이를 통해 정규화된 분포가 항상 평균 0, 분산 1을 갖도록 제한하지 않고, 신경망이 최적의 분포를 학습할 수 있도록 합니다.

**레이어 정규화의 효과**
- **RNN(Recurrent Neural Network)에 효과적**:
    - 배치 정규화는 RNN과 같이 시퀀스 데이터를 처리하는 모델에서는 적용하기 힘듬.
    - 레이어 정규화는 레이어 단위로 정규화를 수행하므로 RNN에서도 사용할 수 있음.
- **작은 배치 크기에서도 안정적인 학습**:
    - 배치 정규화는 배치 크기에 민감하지만, 레이어 정규화는 배치 크기에 영향을 받지 않아 작은 배치 크기에서도 안정적인 학습이 가능하다고 함.
- **학습 안정화**:
    - 가중치 초기화에 대한 민감도를 줄여 학습을 안정화한다고 함.이를 통해 다양한 모델 구조에서 안정적인 학습을 가능하게 한다고 함.
- CNN(Convolutional Neural Network)에서는 배치 정규화보다 성능이 떨어질 수 있다고 알려져 있음.
- 추가적인 계산 비용 발생

# Layer Normalization

[Jimmy Lei Ba](https://arxiv.org/search/stat?searchtype=author&query=Ba,+J+L), [Jamie Ryan Kiros](https://arxiv.org/search/stat?searchtype=author&query=Kiros,+J+R), [Geoffrey E. Hinton](https://arxiv.org/search/stat?searchtype=author&query=Hinton,+G+E)

---
#### *Batch VS Layer Normalization*
---

![[CNN_BN_01.png]]
![[LayerNorm_01.png]]

![[batch_VS_Layer_BN.png]]

![[CNN_BN_02.png|700]]
![[CNN_BN_03.png|400]]
![[LayerNorm_03.png|400]]