---
categories: 글쓰기
title: 2.3.1. 신경망 모델의 발달
created: 2024-12-18
tags:
  - 교재
  - 수업
---
---
#### *2.3.1. 신경망 모델의 발달*
---

## 인공지능 _AI(Artificial Intelligence)_

- Artificial : made or produced by human beings rather than occurring naturally, especially as a copy of something natural.
- 인조의, 인공적인, 
- Intelligence : the ability to understand and learn well, and to form judgments and opinions based on reason 
- 지적 능력을 말한다. 지능은 심리학적으로 새로운 대상이나 상황에 부딪혀 그 의미를 이해하고 합리적인 적응 방법을 알아내는 지적 활동의 능력
## 신경망 (neural network)

- Norm: 벡터 또는 행렬의 크기를 나타내는 수치입니다. 벡터의 Norm은 벡터의 모든 원소의 제곱근의 합으로 계산됩니다. 행렬의 Norm은 행렬의 모든 원소의 제곱근의 합을 행렬의 크기로 나눈 값

- *퍼셉트론(perceptron)* : 로젠 블렛이 1957년에 고안한 알고리즘으로 신경망의 가장 기본적인 모델, 일종의 선형 이진 분류기 (Binary Classification) 모델.
- 
- 지도학습(Supervised Learning)을 통해서 2개의 선택지(Class)를 구분하는  기준선(Decision Boundary)를 학습하는 알고리즘. 

## 1) 인공지능 _AI(Artificial Intelligence)_

- 현대적인 의미의 인공지능의 개념 범위
 '튜링 테스트' : 튜링 테스트(Turing Test)는 영국의 수학자이자 컴퓨터 과학자인 **앨런 튜링(Alan Turing)** 이 1950년에 제안한 개념으로, 기계(컴퓨터)가 인간처럼 사고할 수 있는지를 판별하는 테스트이다. 튜링은 논문 "Computing Machinery and Intelligence"에서 "기계가 생각할 수 있는가?"라는 질문에 대해 "생각한다"는 개념이 모호하기 때문에, 그는 이를 검증할 수 있는 실험을 제안하였다.
 
![[튜링테스트.png]]

- 인공지능(AI) 연구의 초석을 마련한 개념.
- 기계 지능을 평가하는 최초의 시도.

## 2) 신경망 (neural network)


![[Pasted image 20240304151332 1.png]]
https://compmath.korea.ac.kr/deeplearning/Perceptron.html

### 단층 신경망 (SLP : Single Layer Perceptron)

- 단층 퍼셉트론의 *XOR 문제* : 1969년 마빈 민스키(Marvin Minsky)와  시모어(Seymore Papert)sms "Perceptrons: An Introduction to Computational Geometry"에서 XOR 과 같은 비선형 문제에 대한 퍼셉트론의 한계를 지적함.

- 1943년 워렌 캑컬럭과 월터 피츠의 논문 "A Logical calculus of Ideas Immanent in Nervous Activity, Bullentin of Mathematical Biophysics 1943" 에서 TUC 라는 회로구조 "인공신경망 모델" 제시

- 1949년 올딩 헵(Donald Olding Hebb) 신경망의 학습방법에 대한 "헵스법칙" 그의 저서에서 설명

- 1958년 프랭크 로젠블렛(Frank Rosenblatt)이 최초로 *퍼셉트론(perceptron)*이란 단순한 신경망 제시. 단일 뉴런으로 구성. 시그모이드 활성화 함수가 적용된 선형 회귀 모델 이진분류기.
- 1962년 로젠블렛은 "Principles of Neurodynamics"라는 책을 통해 단순 포셉트론을 정리 센서, 연계, 반응 유닛으로 구체화함.

### 다층 신경망 (MLP : Multi Layer Perceptron)

- 1969년 동적 최적화 문제에 대한 아서 브라이슨과 유치 호의 연구에서 "연쇄법칙"을 통해서 동적최적화가 역전메카니즘으로 가능하다는 것을 보였다.

- 1974년 폴 워보스(Paul Werbos)의 박사논문에서 이 문제를 해결하였으나, 시기적으로 인공지능의 겨울의 시기로 크게 관심 받지 못함. 

- 1985년 얀 르쿤(Yann LeCun)이 박사논문에 의해 재발견되었으나 아직 크게 알려지지는 않았다.

- 1986년 데이비드 러멜하트, 제프리 힌튼, 노날드 윌리엄스가 발표한 "Learning representations by back-propagating error"라는 논문이 발표되면서 다시 주목 받기 시작한다. 