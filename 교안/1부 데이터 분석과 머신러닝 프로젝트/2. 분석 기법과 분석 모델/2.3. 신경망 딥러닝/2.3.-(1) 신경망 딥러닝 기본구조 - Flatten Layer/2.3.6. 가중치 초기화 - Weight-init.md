---
categories: 글쓰기
title: 2.3.6. 가중치 초기화 - Weight-init
created: 2025-02-05
tags:
  - Neural_Network
  - Weight-init
  - 교재
  - 수업
---
---
### 2.3.6. 가중치 초기화 - Weight-init
---
### parameter (weight) 초기화의 중요성

- **그래디언트 소실/폭발 문제 완화:** 심층 신경망에서 역전파 과정 중 그래디언트가 점차 작아지거나 커져 학습이 어려워지는 현상을 완화.
- **빠른 수렴:** 학습 속도를 향상.
- **높은 성능:** 모델의 성능을 향상.
- **안정적인 학습:** 학습 과정을 안정화.


![[가중치초기화.png]]


### 균등 분포 초기화 (Uniform Distribution Initialization)
---
uniform ( 1/sqrt(in_ feaures) , 1/sqrt(in_ feaures)):

```
W ~ Uniform(-limit, limit)
```

- W: 가중치
- limit: 범위의 최댓값


### Xavier Initialization :Glorot 초기화
---
```
std = sqrt(2 / (fan_in + fan_out))
```

- std: 가중치 표준편차
- fan_in: 이전 레이어의 뉴런 수
- fan_out: 다음 레이어의 뉴런 수

이렇게 계산된 표준편차를 사용하여 정규 분포 또는 균등 분포에서 무작위 값을 생성하여 가중치를 초기화


Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the 13th international 1 conference on artificial intelligence and statistic 2 s (pp. 249-256).  
(http://revistademorfologiaurbana.org/index.php/rmu/article/view/140)

시그모이드(Sigmoid)나 하이퍼볼릭 탄젠트(Hyperbolic Tangent)와 같은 S자형 활성화 함수에 효과적


### Kaiming Initialization : He initialization
---
Kaiming 초기화는 2015년 He et al.에 의해 제안된 방법으로, 심층 신경망에서 ReLU 활성화 함수를 사용할 때 가중치를 초기화하는 효과적인 방법.


- **ReLU 활성화 함수와의 궁합:** ReLU 함수는 입력값이 음수일 때 기울기가 0이 되어 뉴런이 "죽는" 현상이 발생할 수 있는데, Kaiming 초기화는 이를 완화.


1. **이전 레이어의 뉴런 수:** 이전 레이어의 뉴런 수가 많을수록 가중치의 분산을 작게 설정.


```
W = np.random.normal(0, sqrt(2/n), size)
```

- W: 가중치
- n: 이전 레이어의 뉴런 수
- size: 가중치 텐서의 크기


He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IE 1 EE international conference on computer visio 2 n (pp. 1026-1034)


### LeCun Initialization
---
Yann LeCun에 의해 제안된 방법으로, Xavier 초기화와 유사하지만, fan-in만을 고려

std = sqrt(1 / fan_in)

시그모이드(Sigmoid)나 하이퍼볼릭 탄젠트(Hyperbolic Tangent)와 같은 S자형 활성화 함수에 사용


### 기타 초기화 방법
---

### 상수 초기화 (Constant Initialization)

- 상수 초기화는 모든 가중치를 특정 상수 값으로 초기화.
- ex: 모든 가중치를 0 또는 1로 초기화
- 단순, 모든 가중치가 동일한 값을 가지기 때문에 학습이 제대로 이루어지지 않을 수 있음.

### 임의 초기화 (Random Initialization)

- 작은 무작위 값으로 가중치를 초기화.
- 일반적으로 정규 분포 또는 균등 분포를 사용하여 무작위 값을 생성.
- 값이 너무 작으면 기울기 소실 문제가 발생, 너무 크면 기울기 폭발 문제가 발생.

### 희소 초기화 (Sparse Initialization)

- 희소 초기화는 대부분의 가중치를 0으로 설정하고 일부 가중치만 무작위 값으로 초기화하는 방법입니다.
- 특징 추출에 효과적이며, 과적합을 방지하는 데 도움을 줄 수 있습니다.

### 사전 훈련된 가중치 사용 (Using Pre-trained Weights)

- 대규모 데이터셋으로 미리 훈련된 모델의 가중치를 가져와 사용하는 방법.
- 전이 학습(Transfer Learning)의 한 방법, 작은 데이터셋으로도 좋은 성능을 얻을 수 있음.
