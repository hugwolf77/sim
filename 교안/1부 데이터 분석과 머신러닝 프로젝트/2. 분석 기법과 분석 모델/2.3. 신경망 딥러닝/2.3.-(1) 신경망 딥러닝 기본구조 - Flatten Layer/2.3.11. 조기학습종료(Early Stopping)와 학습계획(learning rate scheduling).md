---
categories: 글쓰기
title: 2.3.11. 조기학습종료(Early Stopping)와 학습계획(learning rate scheduling)
created: 2025-03-22
tags:
  - 교재
  - 수업
  - DL
---
---
#### 2.3.11. 조기학습종료(Early Stopping)와 학습계획(learning rate scheduling)
---

#### 1. *조기학습종료(Early Stopping)*
---

- 딥러닝 모델의 학습 과정에서 과적합(Overfitting)을 방지하기 위해 검증 데이터(validation dataset)를 사용하여 모델의 일반화 성능을 최대화하기 위한 기법.
- 학습에 사용하지 않은 데이터를 사용하여 성능을 평가하고 더이상 성능(Loss의 감소 등) 증가하지 않을 것으로 파악될대 학습 중지.

1) **조기 학습 종료의 작동 방식**
	1. **검증 데이터 사용**: 학습 데이터와 별도로 검증 데이터 세트를 준비.
	2. **검증 성능 모니터링**: 학습 과정에서 매 에폭(epoch)마다 검증 데이터 세트에 대한 모델의 성능(예: 정확도, 손실)을 측정.
	3. **성능 감소 감지**: 검증 데이터 세트에 대한 성능이 일정 기간 동안 개선되지 않거나 감소하기 시작하면 학습을 중단.
	4. **최적 모델 선택**: 학습이 중단된 시점 또는 그 이전 시점의 검증 데이터 세트에 대한 성능이 가장 좋은 모델을 최종 모델로 선택.
	5. **Patient 적용**: 참을 성을 적용하여 한번이 아닌 몇번의 성능 증가가 없을때 학습 종료. 

2) **조기 학습 종료의 효과**
	- **과적합 방지**: 학습 데이터에만 지나치게 맞춰지는 과적합 현상을 방지하여 모델의 일반화 성능을 높임. (IMHO:데이터가 부족한 경우 실무에서는 "마른걸레 짜기"가 성행)
	- **학습 시간 단축**: 불필요한 학습 반복을 줄여 학습 시간을 단축, 계산 자원을 절약.
	- **최적 모델 선택**: 검증 데이터 세트에 대한 성능을 기준으로 최적의 모델을 선택하여 모델의 성능을 최대화.

3) **조기 학습 종료 시 고려사항**
- **검증 데이터 세트**: 검증 데이터 세트는 학습 데이터 세트와 동일한 분포를 가져야 하며, 충분한 크기를 가져야 한다. (셔플 가능한 데이터라면 원데이터에서 충분히 shuffle된 데이터를 분할하여 사용.)
- **성능 측정 지표**: 문제의 특성에 맞는 적절한 성능 측정 지표(예: 정확도, 정밀도, 재현율, F1 점수, 손실)를 선택. (일반적으로 loss)
- **중단 조건**: 성능 감소를 감지하는 중단 조건(예: 에폭 수, 성능 개선 폭)을 적절하게 설정.
- **모델 저장**: 학습 중 검증 데이터 세트에 대한 성능이 가장 좋은 모델을 항상 저장.


- train dataset 에서 지속적으로 학습을 진행하여 over-fitting 하도록 하지 않고, validation dataset 에 의한 평가에서 더 이상 Loss가 줄어 들지 않아 학습이 의미가 없어 질 경우 학습을 중지하는 것.

- 보통 patient 라는 hyper parametor 를 사용하여 validation 과정에서 몇번이나 Loss가 감소하지 않고 진행하는지 확인하여 조기 학습 종료를 판단함. 

- 학습률 조정과 함께 고려해야 할 점이 많음. 아주 작은 학습률에서  patient를 얼마나 가져 갈 것인지 등을 결정해야 함. ("마른 걸레 짜기"라는 쥐어 짜는 학습에서 머리 아픈 문제임. validation에서 얼마나 더 학습이 반영될지 직접적인 관계를 반영하기 어럽기 때문) 

- [데이터] [kaggle 핫도그 이미지](https://www.kaggle.com/datasets/thedatasith/hotdog-nothotdog)
- [pytorch ResNet](https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/resnet.py#L124)

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision import transforms, datasets
import matplotlib
import matplotlib.pyplot as plt
import time
import argparse
from tqdm import tqdm

matplotlib.style.use('ggplot')
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

```python
train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomVerticalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])
val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

train_path = ""
test_path = ""

 
train_dataset = datasets.ImageFolder(
    root=train_path,
    transform=train_transform
)
train_dataloader = torch.utils.data.DataLoader(
    train_dataset, batch_size=16, shuffle=True,
)
val_dataset = datasets.ImageFolder(
    root=test_path,
    transform=val_transform
)
val_dataloader = torch.utils.data.DataLoader(
    val_dataset, batch_size=16, shuffle=False,
)

```

```python
class LRScheduler():

    def __init__(self, optimizer, patience=5, min_lr=1e-6, factor=0.5):
        self.optimizer = optimizer
        self.patience = patience
        self.min_lr = min_lr
        self.factor = factor # LR을 factor배로 감소시킴
        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            patience=self.patience,
            factor=self.factor,
            min_lr=self.min_lr,
            verbose=True
        )
    def __call__(self, val_loss):
        self.lr_scheduler.step(val_loss)
```

```python
class EarlyStopping():
    def __init__(self, patience=5, verbose=False, delta=0, path=path):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False # 조기 종료를 의미하며 초기값은 False로 설정
        self.delta = delta # 오차가 개선되고 있다고 판단하기 위한 최소 변화량
        self.path = path
        self.val_loss_min = np.Inf
 
    def __call__(self, val_loss, model):
        # 에포크 만큼 한습이 반복되면서 best_loss가 갱신되고, bset_loss에 진전이 없으면 조기종료 후 모델을 저장
        score = -val_loss
        
        if best_score is None:
            self.bset_score = score
            self.save_checkpoint(val_loss, model)
 
        elif score < self.best_score + self.delta:
            self.counter += 1
            print(f'Early Stopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
 
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
            self.counter = 0
    
    def save_checkpoint(self, val_loss, model):
        if self.verbos:
            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}. Saving model ...')
        torch.save(model.state_dict(), self.path)
        self.val_loss_min = val_loss
```


```python
parser = argparse.ArgumentParser()
parser.add_argument('--lr-scheduler', dest='lr_scheduler', action='store_true')
parser.add_argument('--early-stopping', dest='early_stopping', action='store_true')
parser.add_argument("-f", "--fff", help="a dummy argument to fool ipython", default="1") #주피터 노트북에서 실행할때 필요합니다
args = vars(parser.parse_args())


model = models.resnet50(pretrained=True).to(device)

lr = 0.001
epochs = 100
optimizer = optim.Adam(model.parameters(), lr=lr)
criterion = nn.CrossEntropyLoss()


# 모델 학습 함수
def training(model, train_dataloader, train_dataset, optimizer, criterion):
    print('Training')
    model.train()
    train_running_loss = 0.0
    train_running_correct = 0
    counter = 0
    total = 0
    prog_bar = tqdm(enumerate(train_dataloader), total=int(len(train_dataset)/train_dataloader.batch_size))
    for i, data in prog_bar:
        counter += 1
        data, target = data[0].to(device), data[1].to(device)
        total += target.size(0)
        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, target)
        train_running_loss += loss.item()
        _, preds = torch.max(outputs.data, 1)
        train_running_correct += (preds == target).sum().item()
        loss.backward()
        optimizer.step()
        
    train_loss = train_running_loss / counter
    train_accuracy = 100. * train_running_correct / total
    return train_loss, train_accuracy
    
# 콜백을 적용할 검증 함수
def validate(model, test_dataloader, val_dataset, criterion):
    print('Validating')
    model.eval()
    val_running_loss = 0.0
    val_running_correct = 0
    counter = 0
    total = 0
    prog_bar = tqdm(enumerate(test_dataloader), total=int(len(val_dataset)/test_dataloader.batch_size))
    with torch.no_grad():
        for i, data in prog_bar:
            counter += 1
            data, target = data[0].to(device), data[1].to(device)
            total += target.size(0)
            outputs = model(data)
            loss = criterion(outputs, target)
            
            val_running_loss += loss.item()
            _, preds = torch.max(outputs.data, 1)
            val_running_correct += (preds == target).sum().item()
        
        val_loss = val_running_loss / counter
        val_accuracy = 100. * val_running_correct / total
        return val_loss, val_accuracy


```

```python

# 모델 학습
train_loss, train_accuracy = [], []
val_loss, val_accuracy = [], []
start = time.time()
for epoch in range(epochs):
    print(f"Epoch {epoch+1} of {epochs}")
    train_epoch_loss, train_epoch_accuracy = training(
        model, train_dataloader, train_dataset, optimizer, criterion
    )
    val_epoch_loss, val_epoch_accuracy = validate(
        model, val_dataloader, val_dataset, criterion
    )
    train_loss.append(train_epoch_loss)
    train_accuracy.append(train_epoch_accuracy)
    val_loss.append(val_epoch_loss)
    val_accuracy.append(val_epoch_accuracy)
    
    if args['lr_scheduler']:
        lr_scheduler(val_epoch_loss)
    if args['early_stopping']:
        early_stopping(val_epoch_loss, model)
        if early_stopping.early_stop:
            break
    print(f"Train Loss: {train_epoch_loss:.4f}, Train Acc: {train_epoch_accuracy:.2f}")
    print(f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_accuracy:.2f}')
end = time.time()
print(f"Training time: {(end-start)/60:.3f} minutes")


```

```python
class EarlyStopping:
    def __init__(self,model, patience=3, delta=0.0, mode='min', verbose=True):
        """
        patience (int): loss or score가 개선된 후 기다리는 기간. default: 3
        delta  (float): 개선시 인정되는 최소 변화 수치. default: 0.0
        mode     (str): 개선시 최소/최대값 기준 선정('min' or 'max'). default: 'min'.
        verbose (bool): 메시지 출력. default: True
        """
        self.early_stop = False
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        
        self.best_score = np.Inf if mode == 'min' else 0
        self.mode = mode
        self.delta = delta
        self.model = model

    def __call__(self, score):

        if self.best_score is None:
            self.best_score = score
            self.counter = 0
        elif self.mode == 'min':
            if score < (self.best_score - self.delta):
                self.counter = 0
                self.best_score = score
                if self.verbose:
                    # 모델 저장
                    torch.save(self.model.state_dict(), f'best_model.pth')
                    print(f'[EarlyStopping] (Update) Best Score: {self.best_score:.5f} & Model saved')
            else:
                self.counter += 1
                if self.verbose:
                    print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience}, ' \
                          f'Best: {self.best_score:.5f}' \
                          f', Current: {score:.5f}, Delta: {np.abs(self.best_score - score):.5f}')
                
        elif self.mode == 'max':
            if score > (self.best_score + self.delta):
                self.counter = 0
                self.best_score = score
                if self.verbose:
                    # 모델 저장
                    torch.save(model.state_dict(), f'best_model.pth')
                    print(f'[EarlyStopping] (Update) Best Score: {self.best_score:.5f} & Model saved')
            else:
                self.counter += 1
                if self.verbose:
                    print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience}, ' \
                          f'Best: {self.best_score:.5f}' \
                          f', Current: {score:.5f}, Delta: {np.abs(self.best_score - score):.5f}')
                
            
        if self.counter >= self.patience:
            if self.verbose:
                print(f'[EarlyStop Triggered] Best Score: {self.best_score:.5f}')
            # Early Stop
            self.early_stop = True
        else:
            # Continue
            self.early_stop = False
```

---
## 학습률 조정 스케줄링 (Learning rate schedules)
---

: Training dynamics
- 딥러닝 모델 학습 과정에서 학습률(Learning Rate)을 동적으로 조절하는 기법 
- 학습률은 모델의 가중치를 업데이트하는 속도를 결정하는 중요한 하이퍼파라미터이며, 적절한 학습률을 설정하는 것은 모델의 성능과 학습 효율성에 큰 영향.

**1) 학습 단계별 학습률 조정 필요성**
- **학습 초기 단계**: 학습 초기 단계에서는 큰 학습률을 사용하여 모델이 빠르게 최적점에 접근.
- **학습 후반 단계**: 학습 후반 단계에서는 작은 학습률을 사용하여 모델이 최적점에 정밀하게 수렴하도록 해야함.
- **지역 최적점 탈출**: 적절한 학습률 변화는 모델이 지역 최적점(Local Minima)에 갇히는 것을 방지.

**2) 주요 학습률 스케줄링 기법**
- **시간 기반 감쇠(Time-based Decay)**:
    - 학습이 진행됨에 따라 학습률을 점차적으로 감소시키는 방법.
    - 일반적으로 에폭(epoch) 수나 학습 반복 횟수에 따라 학습률을 감소.
    - 감쇄 eporch 시점에 대한 또 다른 하이퍼 파라메터 설정이 필요해짐.
- **단계별 감쇠(Step Decay)**:
    - 특정 에폭 또는 반복 횟수에 도달할 때마다 학습률을 일정한 비율로 감소시키는 방법.
    - 예를 들어, 10 에폭마다 학습률을 0.5배로 줄일 수 있다.
    - 감쇄 eporch 시점에 대한 또 다른 하이퍼 파라메터 설정이 필요해짐.
- **선형 감쇠(Linear decay)**: 
	- 일정 선형식 비율로 감쇄 		$$\alpha_{t} = \alpha_{0}(1-t/T))$$
- **지수 감쇠(Exponential Decay)**:
    - 학습률을 지수 함수적으로 감소시키는 방법.
    - 학습 초기에는 빠르게 감소하고, 후반에는 느리게 감소.
- **코사인 감쇠(Cosine Annealing)**:
    - 코사인 함수를 사용하여 학습률을 주기적으로 변화시키는 방법.
    - 학습률이 주기적으로 증가하고 감소하여 모델이 다양한 최적점 탐색.
    - 하프 코사인 학습률 스케줄링. 시작 학습률과 전체 eporch 수만 있으면 됨. $$\alpha_{t} = \frac{1}{2}\alpha_{0}(1+cos(t\pi/T))$$
- **적응적 학습률(Adaptive Learning Rate)**:
    - Adam, RMSprop, AdaGrad 등과 같이 학습 과정에서 학습률을 자동으로 조절하는 최적화 알고리즘을 사용하는 방법.
    - 각 매개변수에 대해 개별적인 학습률을 적용하여 효율적인 학습 가능.
- **기타**
	- Inverse sqrt: 역제곱근 학습률 $\alpha_{t} = \alpha_{0}/\sqrt{t}$ 
	- constant : 일정 상수 

**3) 장점**
	- **학습 속도 향상**: 적절한 학습률 변화는 모델이 최적점에 빠르게 수렴.
	- **모델 성능 향상**: 모델이 더 나은 최적점을 찾도록 도와 일반화 성능을 향상.
	- **학습 안정성 향상**: 학습 과정에서 발생할 수 있는 불안정성을 완화.

#### pytorch 학습 스케줄링

```python
import torch 
import torch.nn as nn 
import torch.optim as optim 
from data import AudioDataset, AudioDataLoader 
from matplotlib import pyplot as plt 

class Model(nn.Module): 
	def __init__(self): 
		super(Model, self).__init__() 
		self.linear = nn.Linear(10, 10) 
		self.activation = nn.ReLU() 
		
	def forward(self, x): 
		return self.activation(self.linear1(x)) 
		
# data 
tr_dataset = AudioDatset('tr') 
data_loader = AudioDataLoader(tr_dataset, batch_size=3, shuffle=1) 
# model model = Model() 
# loss 
loss = nn.MSELoss() 
# optimizer 
optimizer = optim.Adam(model.parameters(), lr=1e-3) 
#scheduler 
scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,
										lr_lambda=lambda epoch: 0.95 ** epoch,
										 last_epoch=-1, verbose=False) 

epochs=100 
for epoch in range(epochs): 
	for i, (data) in enumerate(data_loader): 
		x_data, y_data = data 
		optimizer.zero_grad() 
		estimated_y = model(x_data) 
		loss = loss(y_data, estimated_y) 
		loss.backward() 
		optimizer.step()
		print("lr: ", optimizer.param_groups[0]['lr']) 
	scheduler.step() # you can set it like this!
```

- 함수식에서 나온 값을 초기 learning rate 에 곱해줌 적용
```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.001) 
scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)
```
- 함수식에서 나온 값을 누적 곱하에 초기 learning rate 에 곱해서 반영
```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.001) 
scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)
```
- step 크기 마다 일정 비율로 lr 감소
```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.001) 
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)
```
- 특정 eporch  마다 일정 비율로 lr 감소
```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.001) scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.5)
```
기타 등등 많음.
- 성능 향샹이 없을 때 lr 감소
```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9) 
scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.00005, step_size_up=5, max_lr=0.0001, gamma=0.5, mode='exp_range')
```

