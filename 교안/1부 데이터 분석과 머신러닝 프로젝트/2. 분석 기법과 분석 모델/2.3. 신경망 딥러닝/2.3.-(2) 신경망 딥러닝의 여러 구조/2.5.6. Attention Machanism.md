---
categories: 
title: 2.5.6. Attention Machanism
created: 2025-03-22
tags:
---
---
#### *2.5.6. Attention Machanism*
---
### Attention Machanism

#### seq2seq의 문제

-  seq2seq 모델은 Encoder 에서 입력 sequence를 context vector라는 하나의 고정된 크기의 vector로 정보를 압축하고 Decoder는 이를 받아  반영하여 출력 sequence를 만들어 낸다
- 이때 고정된 문맥 정보(context)로 압출할 때 정보 손실이 발생하며,
- sequence 가 많이 길어 질 때, 가중치 소실 (vanishing gradient)가 여전히 존재한다.
- 이는 결국 입력 sequence의 길이가 길어지면 출력 sequence의 정확도가 떨어지는 문제를 발생 시킨다.

![[attention&seq2seq 1.png]]

#### Attention 기본 아이디어

- Decoder에서 출력을 만드는 각 시점마다 인코더의 전체 sequence에 정보를 전부 다 동일한 비율로 반영하는 것이 아니라
- 해당 출력 sequence 단계와 관련성이 높은 입력 sequence 단계에 집중(Attention)을 높여서 출력 결과의 정확도를 높이고자 함.

#### Attention Function

- 집중(Attention)을 반영시킬 관계 비율을 계산하는 함수
- 이를 통해서 Attention Value를 계산

- Q = Query : Decoder의 t 시점의 hidden state  
- K = Keys : Encoder의 모든 시점들의 hidden state 들 (다른 경우 별도의 step의 key 값들 임)
- V = Values : Encoder의 모든 시점들의 hidden state 들

$$Attention(Q,K,V)=Attention \ \ Value $$
- 주어진 Q에 대한 모든 K에 대한 상관도(유사도) 계산
- 계산된 상관도를 V에 반영
- 반영된 V를 모두 합하여 Attention Value 계산

https://wikidocs.net/22893

1) Attention Scroe 계산 : decoder 현재 state와 Encoder의 각 seqence 모두의 dot-product 계산
$$Attention\ Score:\ socre(\vec s_{t},\vec h_{i}) =  \vec s_{t}^{T}h_{i}$$

2) 각 Encode 의 Sequence 의 hidden과 계산한 묶음 
$$e^{t}=[\vec s_{t}^{T}h_{1},\dots,\vec s_{t}^{T}h_{N}]$$
3) Attention Dstribution 계산 : SoftMax를 사용하여

$$ SoftMax(\vec x) = \frac{e^{x_{i}}}{\sum_{k=1}^{K}e^{x_{k}}} \quad for \ i = 1,\dots,K$$

$$\alpha^{t}=SoftMax(e^{t})$$
5) Attention Value (context vector)
$$\alpha^{t} = \sum_{i=1}^{N}\alpha_{i}^{t}h_{i}$$
6) 이렇게 구해진 Attention Value 를 decoder 출력 vector에 concatenate
7) 보통 concatenate 된 vector 에 가중치를 와 활성함수를 사용하여 출력을 만듬

 - dot-product (Luong  방식) 
 - concat (Bahdanau  방식) : t-1 시점 Query 시점 사용
$$
\begin{align}
	Attention\ Score:\ socre(\vec s_{t-1},\vec h_{i}) &=  W_{a}^{T}tanh(W_{b}\vec s_{t}^{T}+W_{c}h_{i}) \\
	&=W_{a}^{T}tanh(W_{b}\vec s_{t}^{T}+W_{c}H)
\end{align}
$$

---

- [pytorch Seq2Seq with Attention](https://tutorials.pytorch.kr/intermediate/seq2seq_translation_tutorial.html)


---
### *2. self Attention Machanism*
---
- Attention을 자기 자신에게 수행

- Q = Querys : 모든 시점의 Decoder hidden state  
- K = Keys : Encoder의 모든 시점들의 hidden state 들 
- V = Values : Encoder의 모든 시점들의 hidden state 들

- 위에 Q,K,V 가 모두 같은 vector를 사용.
- 이때, 전체 feature 차원  $d_{model}$ 을 사용하면 행렬 계산이 너무 커서 특정 크기 가중치를 이용하여 낮은 차원으로 감소 $d_{model} \times ( d_{model} / num\ heads )$  시켜서 Q,K,V 를 만듬.

- Scaled dot-product Attention : $Score(q,k)=q\cdot k \diagup \sqrt{n}$   

$$
Attention(Q,K,V) = SoftMax(\frac{QK^{T}}{\sqrt{d_{k}}}V)
$$



---


https://wikidocs.net/31379

<img src="https://wikidocs.net/images/page/31379/transformer_attention_overview.PNG">
