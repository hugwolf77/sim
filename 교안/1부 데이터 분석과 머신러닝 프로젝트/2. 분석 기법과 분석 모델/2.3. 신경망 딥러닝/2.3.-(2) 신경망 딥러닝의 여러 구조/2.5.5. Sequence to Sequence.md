---
categories: 
title: 2.5.5. Sequence to Sequence
created: 2025-03-22
tags:
---
---
#### *2.5.5. Sequence to Sequence*
---

#### Sequence to Sequence
- 초기 자연어처리(NLP:Netural Language Processing) 분야에 기계 번역 연구에 많이 사용되었던 t신경망 모델 구조이다.

- 중요성 : `하나의 시퀀스(sequence)를 다른 시퀀스로 변환하는 태스크`

- **인코더 (Encoder):**
    - 입력 sequence에 대해 주로  RNN계열 특히 LSTM을 사용하는 Encoder를 통해서 **고정 길이의 벡터(문맥: context)** 로 변환 단계이다. 입력 sequence 에 대한 일종의 함축적 정보화이기 때문에 **Encoder** 라고 한다.
    - **역할:** 입력 시퀀스(X1​,X2​,...,Xm​)를 받아서 처리하고, 해당 시퀀스의 모든 정보를 압축하여 고정된 크기의 **컨텍스트 벡터(Context Vector)** 또는 **히든 상태(Hidden State)** 로 변환합니다. 이 벡터는 입력 시퀀스의 "의미"나 "요약"을 담고 있다고 볼 수 있다.
    - **구현:** 주로 순환 신경망(RNN, Recurrent Neural Network)의 일종인 LSTM(Long Short-Term Memory)이나 GRU(Gated Recurrent Unit) 같은 모델이 사용. 이들은 시퀀스 데이터를 처리하고 장기 의존성(long-term dependencies)을 학습하는 데 효과적.
    - **작동 방식:** 인코더는 입력 시퀀스의 각 요소를 순차적으로 처리하면서 내부 상태를 업데이트하고, 마지막 단계의 히든 상태를 컨텍스트 벡터로 디코더에게 전달.

- **디코더 (Decoder):**
    - **Decoder** 는 인코더로 부터 전달 받은 **컨텍스트 벡터(Context Vector)** 또는 **히든 상태(Hidden State)** 를 가지고 새로운 시퀀스 정보를 받아 그에 상응 하는 tensor 정보로 출력해 낸다. 기계어 번역으로 생각해 보면 일반적으로 SoftMax 함수를 사용하여 확률분포로 변환하여, 가장 확률이 높은 단어를 선택하고 순차적인 출력을 만들어 낸다.
    - **역할:** 인코더로부터 전달받은 컨텍스트 벡터(또는 인코더의 최종 히든 상태)를 사용하여 출력 시퀀스(Y1​,Y2​,...,Yn​)를 생성합니다.
    - **구현:** 디코더 또한 일반적으로 RNN (LSTM, GRU) 기반으로 구현됩니다.
    - **작동 방식:** 디코더는 컨텍스트 벡터를 초기 상태로 받아들이고, 첫 번째 출력 요소를 생성합니다. 그리고 이 생성된 출력 요소를 다음 단계의 입력으로 사용하여 다음 요소를 예측하는 방식으로, **자기회귀(auto-regressive)** 적으로 시퀀스를 한 번에 한 요소씩 생성합니다. 기계어 번역의 예로 생각해 보면, 이 과정은 특별한 종료 토큰(end-of-sequence token, `<EOS>`)이 생성될 때까지 반복됩니다.


![[seq2seq.png]]

 - Reference
	"Sequence to Sequence Learning with Neural Networks"
	[Ilya Sutskever](https://arxiv.org/search/cs?searchtype=author&query=Sutskever,+I), [Oriol Vinyals](https://arxiv.org/search/cs?searchtype=author&query=Vinyals,+O), [Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&query=Le,+Q+V)


![[seq2seq_torch.png]]


###### `Teacher forcing` :

1) 개념 :
	- 훈련 시 디코더가 이전 시점의 예측값 대신 실제 정답(ground truth)을 입력으로 받는 기법.
	- 학습 안정성과 학습 속도를 높이기 위해 사용.
2) 방법 :
	- **디코더의 다음 입력으로 모델 자신이 예측한 토큰 대신, 실제 정답 시퀀스(ground truth)의 토큰을 제공하는 기법**
	- 언어모델로 예를 들면 모델이 다음 단어를 예측할 때, 모델이 이전에 잘못 예측했더라도 실제 정답 단어를 알려주어 올바른 방향으로 학습하도록 '강제(force)'하는 것
	- **일반적인 디코딩 (Teacher Forcing 미사용):**
		- 디코더는 이전 스텝에서 자신이 예측한 yt−1′​을 다음 스텝의 입력으로 사용.
		- yt′​=Decoder(context,yt−1′​)
	- **Teacher Forcing 사용:**
		- 디코더는 이전 스텝에서 실제 정답 시퀀스의 yt−1​을 다음 스텝의 입력으로 사용.
		- yt′​=Decoder(context,yt−1​)
3) 장점 :
	- **학습 안정성 향상:** 훈련 초기에 모델의 예측이 불안정할 때, 잘못된 예측이 누적되는 것을 방지하여 모델이 보다 안정적으로 수렴하도록 도움.
	- **학습 속도 향상:** 모델이 항상 올바른 이전 토큰을 입력받기 때문에, 각 스텝에서 올바른 방향으로 더 빠르게 학습할 수 있음.
	- 간단히 구현가능.

4) 단점 :
	- **훈련-추론 불일치 (Train-Test Discrepancy):** Teacher Forcing은 훈련 시에는 모델에게 항상 올바른 입력(정답 토큰)을 제공하지만, 실제 추론(inference) 시에는 모델 자신이 예측한 토큰을 입력으로 사용해야만 함.

	- 이러한 문제를 노출 편향(exposure bias) 문제라 함.
		- Seq2Seq 모델은 일반적으로 인코더(encoder)와 디코더(decoder)로 구성.
		- 일반적인 디코딩 과정에서는 디코더가 이전에 예측한 토큰을 다음 입력으로 사용.
		- 훈련 초기 단계에서는 디코더의 예측이 매우 부정확할 수 있음.
		- 부정확한 예측이 누적되면 오류가 점점 커져 모델이 제대로 학습하지 못하게 되는 문제가 발생할 수 있음.

> 노출 편향(exposure bias): 
> 	- 훈련 환경과 추론 환경 사이에 불일치(discrepancy)가 발생.
> 	- 훈련 시에는 '쉬운' 환경에서 학습하지만, 실제 추론 시에는 '어려운' 환경에 놓이게 되는 것.
> 	- 실제 배포 환경에서 성능 저하를 일으킬 수 있는 원인.

###### `scheduled sampling` :

1) 개념 :
	- 훈련 단계에서 각 시간 스텝마다 디코더의 다음 입력으로 무엇을 사용할지 확률적으로 결정.

2) 방법 :
	- 일정 확률(p)로 실제 정답(ground truth)을 사용(Teacher Forcing).
	- 나머지 확률(1−p)로 모델이 이전 시점에서 예측한 결과(model’s own prediction)를 사용.
	- 훈련 초기에는 p 값을  높게 사용
	- 훈련 중 점점 p 값을 낮춤
	- 훈련 후기에는 p 값이 거의 사라지거나 없도록 만듬

3) 장점 :
	- **노출 편향 완화:** 훈련 환경과 추론 환경 간의 간극을 줄여 모델이 실제 사용 시 발생할 수 있는 오류 누적 상황에 더 잘 대처하도록 돕는다.
	- **강건성(Robustness) 향상:** 모델이 자신의 오류로부터 회복하는 능력을 학습하게 하여, 더 강건한 시퀀스 생성 능력을 갖게 한다.
	- **성능 향상:** 장기적으로 모델의 실제 추론 성능을 향상시키는 데 기여.

###### `professor forcing` :

1) 개념 :
	- 훈련할 때의 동작 방식(Teacher Forcing 모드)과 실제로 시퀀스를 생성할 때의 동작 방식(자유 실행 모드, free-running mode)을 가능한 한 유사하게 만드는 것.

2) 방법 :
	- GAN(Generative Adversarial Networks)의 아이디어를 차용.
	- RNN 구조를 생성자(Generator)로 사용한다. 따라서 GAN의 원리에 따라서 
		1) Teacher Forcing 모드를 사용하여 정답값으로 생성한 시퀀스
		2) 자유실행 모드로 이전 예측값을 입력으로 받아 정답값을 생선한 시퀀스
		두가지 결과가 만들어짐.
	- 판별자(Discriminator, D): 시퀀스 생성 모델의 '동작 시퀀스(behavior sequence)'를 입력으로 받아서, 이 동작 시퀀스가 Teacher Forcing 모드에서 생성된 것인지, 아니면 자유 실행 모드에서 생성된 것인지를 구별하는 역할을 주어줌.
	- GAN의 원리대로 RNN 손실함수 외에 생성자 역할의 손실과 판별자 역할에 대한 손실을 줄이는 역할을 학습함.
3) 장점 :
	- 노출 편향 완화, 강건성 향상, 정규화(Regularization)(과적화 방지)효과
4) 단점 :
	- 훈련 복잡성 증가, 수렴의 어려움 (GAN의 약점과 똑같이 수렴성이 약해지는 무제가 있음.)