---
categories: 
title: 2.5.5. Sequence to Sequence
created: 2025-03-22
tags:
---
---
#### *2.5.5. Sequence to Sequence*
---

#### Sequence to Sequence
- 초기 자연어처리(NLP:Netural Language Processing) 분야에 기계 번역 연구에 많이 사용되었던 t신경망 모델 구조이다.

- 중요성 : `하나의 시퀀스(sequence)를 다른 시퀀스로 변환하는 태스크`

- **인코더 (Encoder):**
    - 입력 sequence에 대해 주로  RNN계열 특히 LSTM을 사용하는 Encoder를 통해서 **고정 길이의 벡터(문맥: context)** 로 변환 단계이다. 입력 sequence 에 대한 일종의 함축적 정보화이기 때문에 **Encoder** 라고 한다.
    - **역할:** 입력 시퀀스(X1​,X2​,...,Xm​)를 받아서 처리하고, 해당 시퀀스의 모든 정보를 압축하여 고정된 크기의 **컨텍스트 벡터(Context Vector)** 또는 **히든 상태(Hidden State)** 로 변환합니다. 이 벡터는 입력 시퀀스의 "의미"나 "요약"을 담고 있다고 볼 수 있다.
    - **구현:** 주로 순환 신경망(RNN, Recurrent Neural Network)의 일종인 LSTM(Long Short-Term Memory)이나 GRU(Gated Recurrent Unit) 같은 모델이 사용. 이들은 시퀀스 데이터를 처리하고 장기 의존성(long-term dependencies)을 학습하는 데 효과적.
    - **작동 방식:** 인코더는 입력 시퀀스의 각 요소를 순차적으로 처리하면서 내부 상태를 업데이트하고, 마지막 단계의 히든 상태를 컨텍스트 벡터로 디코더에게 전달.

- **디코더 (Decoder):**
    - **Decoder** 는 인코더로 부터 전달 받은 **컨텍스트 벡터(Context Vector)** 또는 **히든 상태(Hidden State)** 를 가지고 새로운 시퀀스 정보를 받아 그에 상응 하는 tensor 정보로 출력해 낸다. 기계어 번역으로 생각해 보면 일반적으로 SoftMax 함수를 사용하여 확률분포로 변환하여, 가장 확률이 높은 단어를 선택하고 순차적인 출력을 만들어 낸다.
    - **역할:** 인코더로부터 전달받은 컨텍스트 벡터(또는 인코더의 최종 히든 상태)를 사용하여 출력 시퀀스(Y1​,Y2​,...,Yn​)를 생성합니다.
    - **구현:** 디코더 또한 일반적으로 RNN (LSTM, GRU) 기반으로 구현됩니다.
    - **작동 방식:** 디코더는 컨텍스트 벡터를 초기 상태로 받아들이고, 첫 번째 출력 요소를 생성합니다. 그리고 이 생성된 출력 요소를 다음 단계의 입력으로 사용하여 다음 요소를 예측하는 방식으로, **자기회귀(auto-regressive)** 적으로 시퀀스를 한 번에 한 요소씩 생성합니다. 기계어 번역의 예로 생각해 보면, 이 과정은 특별한 종료 토큰(end-of-sequence token, `<EOS>`)이 생성될 때까지 반복됩니다.


![[seq2seq.png]]

 - Reference
	"Sequence to Sequence Learning with Neural Networks"
	[Ilya Sutskever](https://arxiv.org/search/cs?searchtype=author&query=Sutskever,+I), [Oriol Vinyals](https://arxiv.org/search/cs?searchtype=author&query=Vinyals,+O), [Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&query=Le,+Q+V)

