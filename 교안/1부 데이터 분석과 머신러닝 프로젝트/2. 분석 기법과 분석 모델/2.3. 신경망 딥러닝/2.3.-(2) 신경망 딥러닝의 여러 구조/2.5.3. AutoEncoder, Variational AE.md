---
categories: 
title: 2.5.3. AutoEncoder, Variational AE
created: 2025-05-01
tags:
---
---
#### 2.5.3. AutoEncoder, Variational AE
---
##### AutoEncoder
- 비지도 학습 방식의 인공 신경망 모델
- 주어진 입력 데이터를 압축하여 잠재 공간(latent space)에 표현한 후, 이 잠재 표현으로부터 원래 입력을 최대한 복원하는 것을 목표로 하는 구조. 
- `mainfold learning`

> ![Note] AutoEncoder vs PCA
> - PCA는 데이터의 분산(variance)을 최대한 보존하는 새로운 직교 기저(principal components)를 찾아, 고차원 데이터를 저차원 공간으로 선형적으로 투영하는 방식. 
> - 데이터의 공분산 행렬이나 데이터 행렬의 특이값 분해(SVD)를 통해 주성분을 계산.
> - 데이터의 차원을 줄이면서 정보 손실을 최소화하고, 데이터의 주요 패턴을 파악하는 데 중점
> - 데이터 시각화, 전처리 단계에서의 노이즈 제거, 특징 추출 등에 활용.
> - 대표적인 통계적 차원감소 기법.
> 
    - **선형 변환:** PCA는 입력 데이터에 선형적인 변환.
    - **직교성:** 찾아낸 주성성분들은 서로 직교. 이는 각 주성분이 서로 독립적인 정보를 담고 있다는 의미. (정사영을 통해 최대의 분산을 같는 주성분을 찾는다.)
    - **분산 기반:** 데이터의 분산을 가장 잘 설명하는 순서대로 주성분 찾음. 첫 번째 주성분이 가장 큰 분산을, 두 번째 주성분이 그 다음으로 큰 분산을 설명.
    - **해석 용이성:** 주성분들이 원래 변수들의 선형 결합으로 표현되기 때문에 어느 정도의 해석이 가능.
    - **계산 효율성:** 비교적 계산 비용이 저렴.
    
| 특징         | PCA (주성분 분석)         | 오토인코더 (AutoEncoder)         |
| ---------- | -------------------- | --------------------------- |
| **변환 방식**  | 선형 (Linear)          | 비선형 (Non-linear) 가능         |
| **모델 구조**  | 고정된 수학적 알고리즘         | 인코더-디코더 구조의 신경망 (유연한 설계)    |
| **학습 방식**  | 분석적 해법 (고유값 분해, SVD) | 경사 하강법 기반의 학습               |
| **직교성**    | 주성분 간 직교성 보장         | 잠재 표현이 직교적일 필요 없음           |
| **분산/오차**  | 분산 최대 보존             | 재구성 오차 최소화                  |
| **해석 용이성** | 비교적 용이               | 어려울 수 있음                    |
| **계산 비용**  | 비교적 저렴               | 학습 시 비용이 많이 들 수 있음          |
| **주요 장점**  | 간단하고 빠르며 해석 용이       | 복잡한 비선형 관계 학습 가능, 다양한 응용 가능 |
| **주요 단점**  | 선형적인 관계만 모델링 가능      | 학습 어려움, 잠재 표현 해석 어려움        |

##### 구조
- **인코더 (Encoder):** 
	- 입력 데이터를 받아 더 낮은 차원의 잠재 표현으로 압축하는 역할
	- 일련의 신경망 레이어를 통과하면서 입력 데이터의 중요한 특징을 추출하고 불필요한 정보를 제거
	- 인코더의 출력은 잠재 벡터(latent vector) 또는 코드(code)라고 함.
    
- **디코더 (Decoder):** 
	- 인코더를 통해 얻어진 잠재 벡터를 입력으로 받아 원래의 입력 데이터와 최대한 유사하게 복원하는 역할.
	- 디코더 역시 일련의 신경망 레이어로 구성되어 있으며, 인코더의 역과정을 수행.

![[AE_intro.png]]

> 활성함수가 없으며 히든 레이어가 한개인 경우를 Linear AutoEncoder라고 함.

- pytorch AutoEncoder
	- https://dacon.io/codeshare/4551
```python

import torch 
import torchvision 
import torch.nn.functional as F 
from torch import nn, optim 
from torchvision 
import transforms, datasets 
import matplotlib.pyplot as plt 
from mpl_toolkits.mplot3d  # 3차원 플롯을 그리는 용도
import Axes3D  
from matplotlib import cm # 데이터포인트 색상
import numpy as np

```

```python
# 하이퍼파라미터 준비 
EPOCH = 10 
BATCH_SIZE = 64 
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu") print("Using Device:", DEVICE)
```

```python
# Fashion MNIST 데이터셋 불러오기 (학습데이터만 사용) 
trainset = datasets.FashionMNIST( 
								root = './.data/', 
								train = True, 
								download = True, 
								transform = transforms.ToTensor()
							)
train_loader = torch.utils.data.DataLoader( 
								dataset = trainset, 
								batch_size = BATCH_SIZE, 
								shuffle = True, 
								num_workers = 2 
							)
```

```python
# 오토인코더 모듈 정의 
class Autoencoder(nn.Module): 
	def __init__(self): 
	super(Autoencoder, self).__init__() 
		self.encoder = nn.Sequential( 
									nn.Linear(28*28, 128), 
									nn.ReLU(), 
									nn.Linear(128, 64), 
									nn.ReLU(), 
									nn.Linear(64, 12), 
									nn.ReLU(), 
									nn.Linear(12, 3), 
									# 입력의 특징을 3차원으로 압축
								) 
		self.decoder = nn.Sequential( 
									nn.Linear(3, 12), 
									nn.ReLU(), 
									nn.Linear(12, 64), 
									nn.ReLU(), 
									nn.Linear(64, 128), 
									nn.ReLU(), 
									nn.Linear(128, 28*28), 
									nn.Sigmoid(), 
								) 
	def forward(self, x): 
		encoded = self.encoder(x) 
		decoded = self.decoder(encoded) 
		return encoded, decoded
```

```python
autoencoder = Autoencoder().to(DEVICE) 
optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.005)
criterion = nn.MSELoss()
```

```python
view_data = trainset.data[:5].view(-1, 28*28) 
view_data = view_data.type(torch.FloatTensor)/255. 
#픽셀의 색상값이 0~255이므로 모델이 인식하는 0부터 1사이의 값으로 만들기 위해 255로 나눔.
```

```python
# 학습하기 위한 함수 
def train(autoencoder, train_loader): 
	autoencoder.train() 
	for step, (x, label) in enumerate(train_loader): 
		optimizer.zero_grad() 
		x = x.view(-1, 28*28).to(DEVICE) 
		y = x.view(-1, 28*28).to(DEVICE) 
		label = label.to(DEVICE) 
		encoded, decoded = autoencoder(x) 
		loss = criterion(decoded, y) 
		loss.backward() 
		optimizer.step() 
```

```python
for epoch in range(1, EPOCH+1): 
	train(autoencoder, train_loader) 
	test_x = view_data.to(DEVICE) _, 
	decoded_data = autoencoder(test_x) 
	f, a = plt.subplots(2, 5, figsize=(5, 2)) 
	print("[Epoch {}]".format(epoch)) 
	for i in range(5): 
		img = np.reshape(view_data.data.numpy()[i],(28, 28)) 
		#파이토치 텐서를 넘파이로 변환합니다. 
		a[0][i].imshow(img, cmap='gray') 
		a[0][i].set_xticks(()) 
		a[0][i].set_yticks(()) 
	
	for i in range(5): 
		img = np.reshape(decoded_data.to("cpu").data.numpy()[i], (28, 28)) 
		a[1][i].imshow(img, cmap='gray') 
		a[1][i].set_xticks(())
		a[1][i].set_yticks(()) 
	plt.show()
```

```python
# 잠재변수를 3D 플롯으로 시각화 
view_data = trainset.data[:200].view(-1, 28*28) 
#원본이미지 200개를 준비 
view_data = view_data.type(torch.FloatTensor)/255. 
test_x = view_data.to(DEVICE) 
encoded_data, _ = autoencoder(test_x) 
encoded_data = encoded_data.to("cpu")
```

```python
CLASSES = {
    0: 'T-shirt/top',
    1: 'Trouser',
    2: 'Pullover',
    3: 'Dress',
    4: 'Coat',
    5: 'Sandal',
    6: 'Shirt',
    7: 'Sneaker',
    8: 'Bag',
    9: 'Ankle boot'
}

fig = plt.figure(figsize=(10,8))
ax = Axes3D(fig)

#잠재변수의 각 차원을 numpy행렬로 변환.
X = encoded_data.data[:, 0].numpy()
Y = encoded_data.data[:, 1].numpy()
Z = encoded_data.data[:, 2].numpy() 

#레이블도 넘파이행렬로 변환.
labels = trainset.targets[:200].numpy() 

for x, y, z, s in zip(X, Y, Z, labels): 
    name = CLASSES[s]
    color = cm.rainbow(int(255*s/9))
    ax.text(x, y, z, name, backgroundcolor=color)

ax.set_xlim(X.min(), X.max())
ax.set_ylim(Y.min(), Y.max())
ax.set_zlim(Z.min(), Z.max())
plt.show()
```

- tensorflow
	https://www.tensorflow.org/tutorials/generative/autoencoder?hl=ko


##### Stacking AutoEncoder for pre-train

- 가중치 초기화를 위한 각 레이어 사전 학습
![[AE_stacking_pre-train.png]]
- 초기화 학습된 가중치를 가지고 학습 진행하고 역전파
![[AE_stacking_pre-train_bkprop.png|500]]

- Batch-Norm, Xavier Initialization 을 사용하면서 지금은 이러한 pre-train을 사용하지 않고 있음.

##### Denoising AutoEncoder

![[AE_Denoising.png|550]]

---
#### Variational AE

- `Generative Model`
- **Generative** models capture the joint probability p(X, Y), or just p(X) if there are no labels.
- Training Data의 distribution을 근사.

- 데이터 x가 숨겨진 잠재 변수 z로부터 생성되었다고 가정.
- 실제 데이터의 분포 p(x)를 근사하는 생성 모델을 만드는 것.
- 입력 instance를 재구축하는 것이 아니라 instance 가 가진 latant factor라는 함축적 의미의 차원과 그 각 factor의 분포를 학습하는 것.

![[VAE.png]]

##### **VAE의 생성 과정 (Generative Process)** $\Rightarrow$ Decoder

1. 잠재 공간의 사전 분포 pθ​(z)로부터 잠재 변수 z를 샘플링. 
	- 일반적으로 표준 정규 분포 N(0,I)를 사용. (θ는 생성 모델의 파라미터)
2. 샘플링된 잠재 변수 z를 조건부 분포 $p_{θ}​(x∣z)$를 통해 관측 데이터 x로 디코딩. 
3. 조건부 분포는 디코더 네트워크에 의해 모델링.

##### 추론 과정 (Inference)
- 관측 데이터 x가 주어졌을 때, 어떤 잠재 변수(latant factor) z로부터 생성되었는지 추론이 필요.
- 사후 분포 $p(z∣x)$를 구하는 문제이나 일반적으로 계산이 어려움.

###### 근사 사후 분포 $q_{\phi}​(z∣x)$
 - 인식 네트워크(recognition network) 또는 추론 네트워크(inference network)
	 - $q_{\phi}​(z∣x)$를 도입하여 실제 사후 분포 $p(z∣x)$를 근사. 
	 - $q_{\phi}​(z∣x)$는 일반적으로 평균 $\mu_{\phi}​(x)$와 공분산 $\sigma_{\phi}​(x)$를 출력하는 신경망으로 모델
	 - 잠재 변수(latant factor) z는 가우시안 분포 $N(\mu_{\phi}​(x)$,$\sigma_{\phi}​(x))$를 따른다고 가정. ($\phi$ 는 추론 모델의 파라미터)

### VAE ELBO

![[VAE_ELBO_2.png]]


-  특정 latant factor로 표현되어지는 데이터 x의 로그 우도 최적화하는 파라메터를 찾는 과정
- latant factor가 나타내는 의미의 분포를 만들내는 파라메터
- 결국 MLE 처럼 x에 대한 로그 우도를 최대화 하는 파라메터를 찾는 문제가 된다.
$$
\begin{align}
	&log\ \ likelihood\quad of\quad VAE:\quad log\ p_{\theta}(X) \\ \\
	&=\int\,p_{\theta}(Z|X)log\,p_{\theta}(X)\,dz \\ \\
\end{align}
$$
- 그러나 일반적으로 위의 적분은 구하기 힘들다. 따라서 $q_{\phi}$를 사용하고 
- 옌센 부등식(Jensen's inequality)을 이용하여 하한을 유도
$$
\begin{align}
&log_{p_{\theta}}(X) \\ \\
&=\int\,q_{\Phi}(Z|X)log\,p_{\theta}(X)\,dz \\
&=\int\,q_{\Phi}(Z|X)log\, \frac{p_{\theta}(X|Z)p(Z)}{p_{\theta}(Z|X)} \,dz \\
&=\int\,q_{\Phi}(Z|X)log\, \frac{p_{\theta}(X|Z)p(Z)}{p_{\theta}(Z|X)}\times \frac{q_{\theta}(Z|X)}{q_{\theta}(Z|X)} \,dz \\
&=\int\,q_{\Phi}(Z|X)\,log\,p_{\theta}(X|Z)\, dz - KL(q_{\Phi}(Z|X)||p(Z))+KL(q_{\Phi}(Z|X)||p_{\theta}(Z|X))\\
\\
& \geq \int\,q_{\Phi}(Z|X)\,log\,p_{\theta}(X|Z)\, dz - KL(q_{\Phi}(Z|X)||p(Z))\;\longleftarrow\; Evidence\; lower\; bound(ELBO)\\
\end{align}
$$

- Loss Reconstruction : 부분은 encoder로 z 를 생성하고 decoder의 출력을 사용하여 구할 수 있다.
- Loss Regularization : 부분은 생성하는 q의 분포와 p의 분포가 정규분포라 KL-divergence 로 계싼이 가능하다.
- 그러나 $p(z|x)$가 처음에 q를 대신 사용한 이유처럼 알 수 없기 때문에
- KL-divergence 가 항상 양수여야 한다는 것을 근거로 부등호 식으로 바꿔 최소한의 경계를 찾는 것으로 표현한다.
- 그래서 이를  Evidence lower bound (ELBO)라 부른다.
- ELBO를 maximize함으로써 likelihood 또한 maximze할 수 있는 것
$$
	Reconstruction\ error \ :\ \int\,q_{\Phi}(Z|X)\,log\,p_{\theta}(X|Z)\, dz
$$
$$
	Reqularization\ error \ :\ KL(q_{\Phi}(Z|X)||p(Z))
$$
![[VAE_ELBO.png]]


- pytorch example

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# 1) 하이퍼파라미터 설정
batch_size = 128
learning_rate = 1e-3
num_epochs = 10
z_dim = 20  # 잠재변수 차원

# 2) MNIST 데이터셋 로드 & 전처리
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 평균, 표준편차
])

train_dataset = datasets.MNIST(
    root='data', train=True, download=True, transform=transform
)
test_dataset = datasets.MNIST(
    root='data', train=False, download=True, transform=transform
)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")


```


```python
class Encoder(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=400, z_dim=20):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, z_dim)
        self.fc_logvar = nn.Linear(hidden_dim, z_dim)

    def forward(self, x):
        """
        x: (batch_size, 784) 형태 (MNIST 이미지를 28x28 -> 784로 flatten)
        return: (mu, log_var)
        """
        h = torch.relu(self.fc1(x))
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

class Decoder(nn.Module):
    def __init__(self, z_dim=20, hidden_dim=400, output_dim=784):
        super().__init__()
        self.fc1 = nn.Linear(z_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, z):
        """
        z: (batch_size, z_dim) 형태
        return: (batch_size, 784)
        """
        h = torch.relu(self.fc1(z))
        x_recon = torch.sigmoid(self.fc2(h))  # 픽셀값 0~1 범위로
        return x_recon
```

```python
class VAE(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=400, z_dim=20):
        super().__init__()
        self.encoder = Encoder(input_dim, hidden_dim, z_dim)
        self.decoder = Decoder(z_dim, hidden_dim, input_dim)

    def reparameterize(self, mu, logvar):
        """
        Reparameterization Trick:
        z = mu + sigma * eps,
        where eps ~ N(0, I)
        """
        std = torch.exp(0.5 * logvar)   # logvar = log(sigma^2)
        eps = torch.randn_like(std)     # std와 같은 shape의 표준정규 샘플
        return mu + eps * std

    def forward(self, x):
        """
        x: (batch_size, 784)
        return: x_recon, mu, logvar
        """
        mu, logvar = self.encoder(x)
        z = self.reparameterize(mu, logvar)
        x_recon = self.decoder(z)
        return x_recon, mu, logvar
```


```python
def vae_loss_function(x_recon, x, mu, logvar):
    """
    x_recon: Decoder가 출력한 복원 이미지 (batch_size, 784)
    x: 실제 입력 이미지 (batch_size, 784)
    mu, logvar: Encoder에서 나온 z 분포 파라미터
    """
    # 1) Reconstruction Loss
    #   - x_recon은 0~1 범위의 확률처럼 해석 가능
    #   - x는 (0~1 범위로 정규화된) 실제 픽셀값
    recon_loss = nn.functional.binary_cross_entropy(
        x_recon, x, reduction='sum'
    )

    # 2) KL Divergence: D_KL(q(z|x) || p(z))
    #    (정규분포 p(z)=N(0,I)로 가정)
    #    = 0.5 * sum(logvar.exp() + mu^2 - 1 - logvar)
    kl_div = 0.5 * torch.sum(logvar.exp() + mu.pow(2) - 1. - logvar)

    # 최종 loss = 재구성 오차 + KL
    return recon_loss + kl_div

```

```python
model = VAE(input_dim=784, hidden_dim=400, z_dim=z_dim).to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

model.train()  # 학습 모드
for epoch in range(num_epochs):
    train_loss = 0.0

    for batch_idx, (x_batch, _) in enumerate(train_loader):
        # 1) 데이터 준비
        x_batch = x_batch.view(-1, 784).to(device)  # (batch_size, 784)

        # 2) 순전파 (Forward)
        x_recon, mu, logvar = model(x_batch)

        # 3) 손실 계산 (ELBO의 음수 방향)
        loss = vae_loss_function(x_recon, x_batch, mu, logvar)

        # 4) 역전파 및 최적화
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

    # 배치 평균 또는 전체 샘플 수로 나눈 값으로 스케일 조정
    avg_loss = train_loss / len(train_loader.dataset)
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")
```

```python
# 간단한 테스트 or 샘플링 예시
model.eval()
with torch.no_grad():
    # 무작위 z 샘플링 -> 이미지 생성
    z_random = torch.randn((16, z_dim)).to(device)  # 16개 샘플
    generated_imgs = model.decoder(z_random).cpu()  # (16, 784)

    # MNIST형태로 reshape
    generated_imgs = generated_imgs.view(-1, 1, 28, 28)
    # 이제 matplotlib 등을 써서 시각화 가능

    # 복원 예시(첫 번째 배치)
    example_data, _ = next(iter(test_loader))
    example_data = example_data.view(-1, 784).to(device)

    x_recon, mu, logvar = model(example_data)
    x_recon = x_recon.view(-1, 1, 28, 28).cpu()
    # 이 역시 plot하여 원본 vs 복원본을 비교 가능


```

- [tensorflow VAE](https://www.tensorflow.org/tutorials/generative/cvae?hl=ko&_gl=1*z11o4a*_up*MQ..*_ga*NDA4MTg3MDguMTc0NjEwMTA2Ng..*_ga_W0YLR4190T*MTc0NjEwMTA2Ni4xLjAuMTc0NjEwMTA2Ni4wLjAuMA..)