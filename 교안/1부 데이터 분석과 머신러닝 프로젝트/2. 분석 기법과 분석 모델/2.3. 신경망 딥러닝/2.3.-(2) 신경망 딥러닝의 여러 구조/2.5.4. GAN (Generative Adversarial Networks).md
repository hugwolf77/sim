---
categories: 
title: 2.5.4. GAN (Generative Adversarial Networks)
created: 2025-05-12
tags:
---
---
#### *2.5.4. GAN* (Generative Adversarial Networks)
---
##### Generative Adversarial Networks (GANs, 생성적 적대 신경망)
- 2014년 Ian Goodfellow와 그의 동료들에 의해 소개
- 서로 경쟁하는 두 개의 신경망.
	- **생성자 (Generator, G):** 무작위 노이즈를 입력으로 받아 최대한 실제와 같은 데이터 샘플을 생성하여 판별자를 속이는 것이 목표. (비유: 위조지폐범이 진짜 같은 돈을 만들려고 노력.)
	- **판별자 (Discriminator, D):** 훈련 데이터셋의 실제 데이터와 생성자가 만든 가짜 데이터를 모두 입력 받아, 진짜와 가짜를 구별. (비유 :경찰이 위조지폐를 식별)

![[Generative_Adversarial_Networks.png]]

$$
min_{G}​\ max_{D}\ \ ​V(D,G)=E_{x∼p_{data​}(x)}​[logD(x + E_{z∼p_{z}(z)}[log⁡(1−D(G(z))Ez∼pz​(z)​[log(1−D(G(z))]]
$$
- $p_{data(x)}$: 실제 데이터의 분포
- x∼pdata(x)x∼pdata​(x): 실제 데이터 분포에서 샘플링된 데이터 xx
- $p_{z}(z)$: 잠재 변수 z의 사전 분포    
- $z∼p_{z}(z)$: 잠재 변수 분포에서 샘플링된 노이즈 벡터 z
- $G(z)$: 생성자가 노이즈 z로부터 생성한 가짜 데이터
- $D(x)$: 판별자가 실제 데이터 xx가 진짜일 확률을 출력하는 함수
- $D(G(z))$: 판별자가 생성된 데이터 G(z)가 진짜일 확률을 출력하는 함수
- $E$: 기댓값(Expectation)

```python
class Discriminator(nn.Module): 
	def __init__(self): 
		super(Discriminator, self).__init__() 
		self.linear1 = nn.Linear(img_size, hidden_size2) 
		self.linear2 = nn.Linear(hidden_size2, hidden_size1) 
		self.linear3 = nn.Linear(hidden_size1, 1) 
		self.leaky_relu = nn.LeakyReLU(0.2) 
		self.sigmoid = nn.Sigmoid() 
	
	def forward(self, x): 
		x = self.leaky_relu(self.linear1(x)) 
		x = self.leaky_relu(self.linear2(x)) 
		x = self.linear3(x) 
		x = self.sigmoid(x) 
		return x
```

```python
class Generator(nn.Module): 
	def __init__(self): 
		super(Generator, self).__init__() 
		self.linear1 = nn.Linear(noise_size, hidden_size1) 
		self.linear2 = nn.Linear(hidden_size1, hidden_size2) 
		self.linear3 = nn.Linear(hidden_size2, img_size) 
		self.relu = nn.ReLU() self.tanh = nn.Tanh() 

	def forward(self, x): 
		x = self.relu(self.linear1(x)) 
		x = self.relu(self.linear2(x)) 
		x = self.linear3(x) 
		x = self.tanh(x) 
		return x
```

```python
criterion = nn.BCELoss() 
d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate) 
g_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate)
```

```python
for epoch in range(num_epoch): 
	for i, (images, label) in enumerate(data_loader): 
		real_label = torch.full((batch_size, 1), 1, dtype=torch.float32).to(device) 
		fake_label = torch.full((batch_size, 1), 0, dtype=torch.float32).to(device) 
		
		real_images = images.reshape(batch_size, -1).to(device)

		# Initialize 
		grad g_optimizer.zero_grad() 
		d_optimizer.zero_grad() 
		
		# fake image를 generator와 noize vector 'z' 를 통해 만들어주기 
		z = torch.randn(batch_size, noise_size).to(device) 
		
		fake_images = generator(z) 
		# loss function에 fake image와 real label을 넘겨주기 
		# 만약 generator가 discriminator를 속이면, g_loss가 줄어든다. 
		g_loss = criterion(discriminator(fake_images), real_label) 
		# backpropagation를 통해 generator 학습 
		g_loss.backward() 
		g_optimizer.step()
		
#-----------------------------------------------------------#
		d_optimizer.zero_grad() 
		g_optimizer.zero_grad() 
		# generator와 noise vector 'z'로 fake image 생성 
		z = torch.randn(batch_size, noise_size).to(device) 
		fake_images = generator(z) 
		# fake image와 fake label, real image와 real label을 넘겨 loss 계산 
		fake_loss = criterion(discriminator(fake_images), fake_label) 
		real_loss = criterion(discriminator(real_images), real_label) 
		d_loss = (fake_loss + real_loss) / 2 
		# backpropagation을 통해 discriminator 학습 
		# 이 부분에서는 generator는 학습시키지 않음 
		d_loss.backward() 
		d_optimizer.step()


```