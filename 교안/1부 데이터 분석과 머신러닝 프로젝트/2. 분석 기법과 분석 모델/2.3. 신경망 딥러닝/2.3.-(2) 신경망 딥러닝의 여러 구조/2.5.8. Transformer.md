---
categories: 
title: 2.5.8. Transformer
created: 2025-06-09
tags:
---
---
#### *2.5.8. Transformer*
---


- 3 종류의 어텐션 메카니즘
- 엠베딩, 포지셔닝, 마스킨
- 모델 구조
- BERT, GPT

- 예제

##### Seq2Seq 구조 복습 

- 의미 : 
	- 계량적 (컴퓨팅이 가능한) 표현으로 변환하고, 이를 사용하여 순차(sequence)가 있는 데이터를 다른 종류의 순차가 있는 표현으로 변환하여 재해석하거나 추론의 정보로 사용할 수 있게 하는 것.
	
- 초기 구현과 문제 :
	- 초기에는 순환 신경망(Recurrent Neural Network, RNN)을 사용하여 구현.
	- 기울기 소실(Vanishing Gradient)과 기울기 폭발(Exploding Gradient) 문제.
	- 장기 의존성(Long-Term Dependency) 문제로 아주 긴 장기 순차구조의 함축 추상화 문제.

- 대안책으로 등장한 Attention Mechanism :
	- 입력 시퀀스의 중요 부분에 ‘집중’하여 필요한 정보를 선택적으로 강조하는 방법.
	- Attention 메커니즘의 등장으로 RNN 기반 Seq2Seq 모델은 크게 개선.

##### Transformer의 등장

- Attention 메커니즘만을 사용하여 Seq2Seq 문제를 해결하는 새로운 방식을 제시.
 [# Attention Is All You Need](https://arxiv.org/abs/1706.03762)

- 구조

![[Transformer_structure.png]]



---

#### Input Embedding :
- 언어 도는 순서 데이터 등 정성적 데이터 성격의 데이터를 계량화.
- 트랜스퍼머에서도 중요한 작업이다.

#### Positional Encoding
- 어텐션 메카니즘은 순차적 정보나 문맥적 정보가 없기 때문에 추가적으로 위치에 따른 정보를 추가로 입력 vector에 반영해주어야 한다.

---
### *2. self Attention Machanism 과 Multi-Head*
---


![[Scaled_dot_product_AT.png]]

### (1) Self Attention

- Attention을 자기 자신에게 수행

- Q = Querys : 모든 시점의 hidden state 들  
- K = Keys : Encoder의 모든 시점들의 hidden state 들 
- V = Values : Encoder의 모든 시점들의 hidden state 들

- 위에 Q,K,V 가 모두 같은 vector를 사용.
- 이때, 전체 feature 차원  $d_{model}$ 을 사용하면 행렬 계산이 너무 커서 특정 크기 가중치를 이용하여 낮은 차원으로 감소 $d_{model} \times ( d_{model} / num\ heads )$  시켜서 Q,K,V 를 만듬.

1) 입력 벡터(여기서는 Word Embedding)를 활용하여, 3개(Query, Key, Value)의 백터를 생성.

2) **Query와 Key 백터를 내적 하여** 계산 $\Rightarrow$ Attention Score

3) Key 백터 크기의 제곱근 만큼 Scaling (단위 변경)
	- 임베딩으로 생성된 key의 vector 크기가 만약 64 라면 $d_{k} = 64\ \rightarrow \frac{1}{\sqrt{d_{k}}} = \frac{1}{8}$
	- $d_{k}$의 중가하면 쿼리(Q)와 키(K)의 내적(QKT)은 dk​가 커질수록 그 값이 매우 커지는 경향이 있기 때문이다.
	- 내적의 값이 커지면 SoftMax 함수를 통과할때 입력갑에 대한 변화율의 기울기가 0에 가까워지는 문제가 있음. `(softmax 함수는 입력 값이 매우 커지거나 작아지면, 그 기울기가 0에 가까워지는 포화(saturation) 현상이 발생 -> 기울기 소실 원인)`

4)  Softmax 계산을 통해, 전체 Score를 정규화

5) Value 백터에 Softmax값을 곱함

6) Value 백터를 가중치 합 계산


- Scaled dot-product Attention : $Score(q,k)=q\cdot k \diagup \sqrt{n}$   

$$
Attention(Q,K,V) = SoftMax(\frac{QK^{T}}{\sqrt{d_{k}}}V)
$$

### (2) Multi Head Attention

- 단일 Attention 이 아니라 여러개 (h개)의 병열 Attention 구조를 통하여 Q, K, V 를 서로 다른 선형 관점에서 Attention 계산 접근.
- Attention을 병렬 수행하여,  dv−dimensional Output 값을 생성.
- 단일 Atteintion 은 Averaging 효과로 다양한 주의 관점 탐지가 억제됨.
- ex) 8개의 Scaled Dot-Product Attention 사용한다면.
	- $d_{v} \times\ Head\ = 64 \times 8 = 512$ 로 싱글 attention 과 총계산 비용은 유사해짐. 

	$$
	\begin{align}
		MultiHead( Q, K, V) = Concat(head_{1},\dots,head_{h})W^{o} \\ \\
		where\ head_{i} = Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})
	\end{align}
	$$

![[Multi-Head.png|700x267]]


### (3) 어텐션 블록층을 지원하는 서브구조 층 

- Position-wise Feed-Forward Neural Network
	$FFNN(x) = MAX(0, xW_{1} + b_{1})W_{2} + b_{2}$

![[residual-connect-at.png|255x254]]

x :  앞의 멀티 헤드 어텐션 연산 결과로 $(sequence\ length, d_{model})$ 모양
$W_{1}$ : $(d_{model}, d_{ff})$, $W_{2}$ : $(d_{ff}, d_{model})$
$d_{ff}$ : 일정한 크기의 선형 신경망층
	- 각 파라메터는 인코더 같은 층에서 다른 문장, 다른 단어들마다 같은 것이 적용됨.
	- 그러나 인코더 다른 층에과는 다른 값을 가짐.

![[residual-connect-lynorm.png]]

- **잔차 연결 (Residual Connection)** 

$$H(x) = x + Multi-Head\ Attention(x)$$

- **계층 정규화 (Layer Normalization)**

$$ 
\begin{align}
LN = LayerNorm(x+Sublayer(x)) \\ \\
\hat x_{i,k} = \frac{x_{i,k}-\mu_{i}}{\sqrt{\sigma_{i}^{2}+\epsilon}} \\ \\
In_{i} = \gamma\hat{x}_{i} + \beta = LayerNorm(x_{i})


\end{align}
$$

https://wikidocs.net/31379

<img src="https://wikidocs.net/images/page/31379/transformer_attention_overview.PNG">


---
### 3. Transformer 내부의 3 종류의 Attention
---

### (1) 인코더 셀프 어텐션 (Encoder Self-Attention)

- **위치:** 트랜스포머 인코더 스택의 각 인코더 레이어 내부에 위치.
- **목적:** 입력 시퀀스 내의 모든 토큰(단어)들이 서로 간의 관계를 파악하고, 각 토큰이 시퀀스 내의 다른 토큰들에 얼마나 "주의를 기울여야" 하는지를 학습.

### (2) 마스킹된 디코더 셀프 어텐션 (Masked Decoder Self-Attention)

- **위치:** 트랜스포머 디코더 스택의 각 디코더 레이어 내부에 위치.
- **목적:** 디코더가 현재 예측하려는 출력 토큰에 대해, 이미 생성된 **이전 시점의 출력 토큰들**만 참고하여 어텐션을 수행. 이는 순차적인 정보 생성과 노출을 모방하기 위함.
- 기본적인 셀프 어텐션 메커니즘은 인코더와 동일하지만, 마스킹(Masking)만 다름.
    - Query 토큰이 Key 토큰들과 어텐션 가중치를 계산할 때, 아직 생성되지 않은 (즉, 미래 시점의) 토큰들에 해당하는 Key 값에는 매우 작은 음수 값을 적용하여 소프트맥스 출력값이 0이 되도록 만들어 미래 정보가 없는 상태를 만들기 위함.
### (3). 인코더-디코더 어텐션 (Encoder-Decoder Attention) / 크로스 어텐션 (Cross-Attention)

- **위치:** 트랜스포머 디코더 스택의 각 디코더 레이어의 두 번째 서브 레이어에 위치(마스킹된 디코더 셀프 어텐션 다음).
- **목적:** 디코더가 현재 출력하려는 토큰을 생성하기 위해, 인코더로부터 전달받은 전체 입력 시퀀스의 정보를 어텐션 정보로 바꾸기 위함. 즉, 인코더의 출력과 디코더의 현재 상태를 연결하는 다리 역할.
- **동작 방식:**
    - 디코더의 마스킹된 셀프 어텐션 레이어의 출력을 Query (Q)로 사용.
    - 인코더의 최종 출력(인코더가 입력 시퀀스 전체를 압축한 정보)이 Key (K)와 Value (V)로 사용.
