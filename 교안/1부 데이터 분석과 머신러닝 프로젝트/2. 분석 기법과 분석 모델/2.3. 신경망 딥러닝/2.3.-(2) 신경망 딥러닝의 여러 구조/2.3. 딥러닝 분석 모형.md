---
categories: 글쓰기
title: 3.3. 딥러닝 분석 모형
created: 2024-10-29
tags:
  - 수업
  - 교재
---
---
#### *3.3. 딥러닝 분석 모형*
---

### 순환신경망 (Recurrent Neural Network)

- 순서가 있는 **연속형 데이터(Sequential Data)** 의 패턴과 순서에 따른 변화의 영향관계를 학습하기 위해서 만든 신경망 모델
- 대표적인 연속형 데이터의 종류는 시계열, 언어, 계절 기후 데이터, 센서 데이터, 소리나 음악 데이터 등의 데이터 이다.
![[RNN_01.png]]

- 말 그대로 순환 신경망이다. 입력층에서는 sequence 에 따라 데이터가 입력되고 
- 히든층은 반복해서 입력되어 지는 데이터와 함께, 이전 스텝의 출력을 다시 입력 받는다.
-  다시 다음 sequence 의 입력과 히든층의 출력은 함께 반복해서 입력된다.
- 이를 입력되는 창 (**Window**: 입력되는 Sequence data 의 길이) 만큼 진행한다.
- 이때 2개의 출력이 있음에 유의 (이를 통해 위로 Stack이 가능)

- 이를 통해 입력되는 window 길이의 sequence data의 순차적인 변화 또는 패턴을 학습하게 된다.
- t 시점의 RNN 계층의 출력 텍터 생성
$$h_{t}\ =\  tanh(h_{t-1}W_{h} + x_{t}W_{x}+\ b) \ $$
![[RNN_02.png]]

- RNN 사용 형태
![[RNN_03.png]]

- 모델이 간단하고 길이와 상관없이 어떠한 sequence 데이터도 처리 가능하다. 비-선형적인 순차 데이터 분석에 강함.
- 시계열 데이터 분석 시 기존 시계열 통계분석에서 가지는 데이터의 정상성 문제 등에 강함.
- 순차입력으로 인핸 행렬 연산의 장점인 병렬분해 연산의 설계가 힘들어 GPU 의 이점을 살리기 힘듬
 - tanh 가 가진  gradient vanishing 과 exploding gradient 문제가 발생한다. 특히, sequence가 길어질 수록 신경망의 구조적 특성 상 모든 sequence 에 대한 정보를 계속 반영해야 하기 때문에 입력 window 의 길이가 길어질 수록 이런 문제가 심각해 진다.

### LSTM (Long Shot Term Memory)

![[LSTM_01.png]]

- Gate

1) **Cell State** : 핵심 정보 전달체로 모든 state를 지나오면 정보를 전달한다. 3개의 Gate를 통해  조절되어지는 정보를 다음 sequence 에 따른 state로 전달한다. 
2) Forget Gate : sigmoid 활성함수로 통과되는 출력을 통해서 과거 state 의 정보를 얼마나 Cell에 반영할 지 $C_{t-1}$ 에 product 한다. 
$$f_{t}\ =\ \sigma \ (W_{f}\ \cdot [h_{t-1},\ x_{t}]\ +\ b_{f}) $$
3) Input Gate : 이번 state sequence 에 입력된 정보에 대해서 어떻게(ex: 음, 양) 반영할지 C gate 에서 결정하고 얼마나 반영할지 결정하는 i gate로 학습해서 모두 product 해준다. 
$$
\begin{align}
	&i_{t}\ =\ \sigma \ (W_{i}\ \cdot [h_{t-1},\ x_{t}]\ +\ b_{i}) \\
	&\tilde C_{t}\ =\ tanh \ (W_{C}\ \cdot [h_{t-1},\ x_{t}]\ +\ b_{C}) 
\end{align}
$$
4) Update : 결정된 이번 sequence 의 정보를 Cell 에 더해서 반영해 준다.
5) Output Gate : 현재 state 의 출력을 결정하기 위해서 다시 현재 sequence 정보와 과거 state 정보를 입력으로 sigmoid를 통과해서 결정된 정보에 지금까지 계산된  cell정보를 product 한 후 이번 state 의 출력으로 내보낸다. 

- 입력되는 각 squence 를 통해 출력되는 정보를 **hidden state**라고 한다.

### pytorch LSTM
https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html

##### GRU 에 대해서 조사해 보세요.
- **hidden state 의 출력이 하나**라는 특징을 가지고 있다.

#### Sequence to Sequence
- 초기 자연어처리(NLP:Netural Language Processing) 분야에 기계 번역 연구에 많이 사용되었던 t신경망 모델 구조이다.
- 입력 sequence에 대해 주로  RNN계열 특히 LSTM을 사용하는 Encoder를 통해서 **고정 길이의 벡터(문맥: context)** 로 변환 단계이다. 입력 sequence 에 대한 일종의 함축적 정보화이기 때문에 **Encoder** 라고 한다.
- **Decoder** 는 인코더로 부터 전달 받은 Context 정보 또는 hidden state 정보를 가지고 일반적으로 SoftMax 함수를 사용하여 확률분포로 변환, 가장 확률이 높은 단어를 선택하고 순차적인 출력을 만들어 낸다.

![[seq2seq.png]]


### Attention Machanism


#### seq2seq의 문제

-  seq2seq 모델은 Encoder 에서 입력 sequence를 context vector라는 하나의 고정된 크기의 vector로 정보를 압축하고 Decoder는 이를 받아  반영하여 출력 sequence를 만들어 낸다
- 이때 고정된 문맥 정보(context)로 압출할 때 정보 손실이 발생하며,
- sequence 가 많이 길어 질 때, 가중치 소실 (vanishing gradient)가 여전히 존재한다.
- 이는 결국 입력 sequence의 길이가 길어지면 출력 sequence의 정확도가 떨어지는 문제를 발생 시킨다.

![[attention&seq2seq 1.png]]

#### Attention 기본 아이디어

- Decoder에서 출력을 만드는 각 시점마다 인코더의 전체 sequence에 정보를 전부 다 동일한 비율로 반영하는 것이 아니라
- 해당 출력 sequence 단계와 관련성이 높은 입력 sequence 단계에 집중(Attention)을 높여서 출력 결과의 정확도를 높이고자 함.

#### Attention Function

- 집중(Attention)을 반영시킬 관계 비율을 계산하는 함수
- 이를 통해서 Attention Value를 계산

- Q = Query : Decoder의 t 시점의 hidden state  
- K = Keys : Encoder의 모든 시점들의 hidden state 들 (다른 경우 별로의 step의 key 값들 임)
- V = Values : Encoder의 모든 시점들의 hidden state 들

$$Attention(Q,K,V)=Attention \ \ Value $$
- 주어진 Q에 대한 모든 K에 대한 상관도(유사도) 계산
- 계산된 상관도를 V에 반영
- 반영된 V를 모두 합하여 Attention Value 계산

https://wikidocs.net/22893

1) Attention Scroe 계산 : decoder 현재 state와 Encoder의 각 seqence 모두의 dot-product 계산
$$Attention\ Score:\ socre(\vec s_{t},\vec h_{i}) =  \vec s_{t}^{T}h_{i}$$

2) 각 Encode 의 Sequence 의 hidden과 계산한 묶음 
$$e^{t}=[\vec s_{t}^{T}h_{1},\dots,\vec s_{t}^{T}h_{N}]$$
4) Attention Dstribution 계산 : SoftMax를 사용하여

$$ SoftMax(\vec x) = \frac{e^{x_{i}}}{\sum_{k=1}^{K}e^{x_{k}}} \quad for \ i = 1,\dots,K$$

$$\alpha^{t}=SoftMax(e^{t})$$
5) Attention Value (context vector)
$$\alpha^{t} = \sum_{i=1}^{N}\alpha_{i}^{t}h_{i}$$
6) 이렇게 구해진 Attention Value 를 decoder 출력 vector에 concatenate
7) 보통 concatenate 된 vector 에 가중치를 와 활성함수를 사용하여 출력을 만듬

 - dot-product (Luong  방식) 
 - concat (Bahdanau  방식) : t-1 시점 Query 시점 사용
$$
\begin{align}
	Attention\ Score:\ socre(\vec s_{t-1},\vec h_{i}) &=  W_{a}^{T}tanh(W_{b}\vec s_{t}^{T}+W_{c}h_{i}) \\
	&=W_{a}^{T}tanh(W_{b}\vec s_{t}^{T}+W_{c}H)
\end{align}
$$

---
### *2. self Attention Machanism*
---
- Attention을 자기 자신에게 수행

- Q = Querys : 모든 시점의 Decoder hidden state  
- K = Keys : Encoder의 모든 시점들의 hidden state 들 
- V = Values : Encoder의 모든 시점들의 hidden state 들

- 위에 Q,K,V 가 모두 같은 vector를 사용.
- 이때, 전체 feature 차원  $d_{model}$ 을 사용하면 행렬 계산이 너무 커서 특정 크기 가중치를 이용하여 낮은 차원으로 감소 $d_{model} \times ( d_{model} / num\ heads )$  시켜서 Q,K,V 를 만듬.

- Scaled dot-product Attention : $Score(q,k)=q\cdot k \diagup \sqrt{n}$   

$$
Attention(Q,K,V) = SoftMax(\frac{QK^{T}}{\sqrt{d_{k}}}V)
$$

