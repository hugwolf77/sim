---
categories: 
title: 2.5.7. Embedding
created: 2025-05-20
tags:
---
---
#### *2.5.5. Embedding*
---

#### 1. 의미
- 자연어 처리(NLP)를 포함한 다양한 머신러닝 분야에서 데이터의 특징을 저차원의 연속적인 벡터 공간으로 변환하는 기법을 총칭.

#### 2. 필요성
- 컴퓨터 또는 분석모델은 정량 데이터를 분석하지만 정성 데이터 즉, 텍스트나 범주형 데이터를 직접적으로 이해하지 못함. 
	- 예: "사과", "바나나", "오렌지" 같은 단어들은 단순히 다른 문자열일 뿐 그 정성적인 차이를 알지 못함.
- 일반적 방식(원-핫 인코딩, dummy 변수 등)은 각 단어를 고유한 벡터 차원으로 표현하여 매우 희소(sparse)하고 고차원적인 벡터를 생성하여 분석하지만 다음과 같은 문제 있음.
	
	1) **차원의 저주(Curse of Dimensionality):** 벡터의 차원이 너무 커져서 계산 비용이 많이 들고, 데이터가 희소해져 패턴 학습이 어려워짐.
	2) **의미적 유사성 부족:** "사과"와 "바나나"는 과일이라는 점에서 유사하지만, 원-핫 인코딩에서는 이들 벡터 간의 거리가 모두 동일하여 의미적 관계를 표현할 수 없음.
#### 3. 임베딩의 주요 개념
- **저차원:** 고차원의 희소 벡터를 훨씬 작은 차원의 벡터로 변환.
- **연속적:** 각 특징이 이산적인 값이 아닌 연속적인 실수 값으로 표현.
- **의미적 유사성 반영:** 임베딩 공간에서 의미적으로 유사한 단어들은 서로 가까운 위치에 배치되도록 학습. 
	- 예: "사과"와 "바나나"의 임베딩 벡터는 "자동차"의 임베딩 벡터보다 서로 더 가깝게 위치. 이는 벡터 공간에서의 거리(유클리드 거리, 코사인 유사도 등)를 통해 유사도를 측정할 수 있게 해야함.
- **관계성 표현:** 경우에 따라 정보 간의 복잡한 관계(예: "왕" - "남자" + "여자" ≈ "여왕")를 벡터 연산 또는 표현식을 통해 표현될 수 있어야 함.

#### 4. 임베딩의 종류

1) 워드 임베딩 (Word Embedding): 가장 흔히 사용되는 임베딩 유형으로, 단어를 벡터로 변환.
	- **Word2Vec (Skip-gram, CBOW):** 주변 단어를 예측하거나(Skip-gram), 주변 단어를 통해 중심 단어를 예측(CBOW)하는 방식으로 단어의 의미를 학습.
	- **GloVe (Global Vectors for Word Representation):** 단어의 동시 발생 행렬(co-occurrence matrix)을 기반으로 통계적인 정보를 활용하여 단어 벡터를 생성.
	- **FastText:** 단어를 서브워드(subword) 단위(character n-gram)로 분해하여 임베딩 학습. OOV(Out-Of-Vocabulary) 문제에 강하고 형태학적 특성을 반영.

2) 문장/문단 임베딩 (Sentence/Paragraph Embedding): 단어뿐만 아니라 문장이나 문서 전체의 의미를 하나의 벡터로 표현.
	- **Doc2Vec:** Word2Vec의 확장으로, 단어 벡터와 함께 문서 벡터도 학습.
	- **Universal Sentence Encoder (USE), Sentence-BERT (SBERT):** 문장 간의 유사도를 효율적으로 계산할 수 있는 문장 임베딩을 생성.

3) 문맥 기반 임베딩 (Contextualized Embedding): 단어의 의미가 주변 문맥에 따라 달라지는 것을 반영하여, 동일한 단어라도 문맥에 따라 다른 임베딩 벡터를 생성.
	- **ELMo (Embeddings from Language Models):** 양방향 LSTM을 사용하여 단어의 문맥에 따른 의미를 표현.
	- **BERT (Bidirectional Encoder Representations from Transformers):** 트랜스포머 아키텍처를 기반으로 양방향 문맥 정보를 학습.
	- **GPT (Generative Pre-trained Transformer):** 트랜스포머 디코더를 기반으로 다음 단어를 예측하는 방식으로 문맥을 학습.

4) 범주형 변수 임베딩 (Categorical Feature Embedding): NLP 외에도 추천 시스템, 표 형식 데이터(tabular data) 등에서 범주형 변수(예: 사용자 ID, 상품 ID, 도시 이름)를 벡터로 변환하는 데 사용. 
	- 이산적인 범주 정보를 연속적인 벡터 공간으로 매핑하여 신경망 모델의 입력으로 사용하기 용이하게 하기 위함.

#### 5. 학습 방법

1) **사전 학습 (Pre-training):** 대규모 코퍼스(corpus)에서 독립적으로 임베딩을 학습하고, 이를 다른 다운스트림 태스크(downstream task)에 전이 학습(transfer learning)으로 활용. (예: Word2Vec, GloVe, BERT)
2) **태스크-특정 학습 (Task-specific Learning):** 특정 모델(예: 신경망)의 입력 계층으로 임베딩 레이어를 포함시켜, 해당 태스크의 손실 함수를 최소화하는 과정에서 임베딩 벡터도 함께 학습.

---
### Positional Encoding

#### 1. 의미
- 시퀀스 데이터(예: 문장)에서 각 요소(단어)의 순서 또는 위치 정보를 모델에게 제공.
#### 2. 필요성
- RNN(Recurrent Neural Network)이나 LSTM(Long Short-Term Memory) 같은 순환 모델은 본질적으로 시퀀스를 순차적으로 처리하므로 단어의 순서 정보를 자연스럽게 학습 하지만 트랜스포머와 같은 어텐션 기반 모델은 시퀀스 내의 모든 단어를 병렬적으로 처리.
- 이는 계산 효율성을 높이지만, 모델 자체는 "고양이 개 쫓았다"와 "개 고양이 쫓았다"라는 문장의 단어 구성이 같다고 인식하여 의미 차이를 알 수 없음.
- 모델이 단어의 순서를 이해하고 문맥을 정확히 파악하도록 명시적으로 위치 정보를 주입.
#### 3. 주요 개념
- **순서(Order) 정보:** 단어가 어디에 있는가에 대한 정보.
- **추가적인 벡터:** 일반적으로 단어 임베딩 벡터와 동일한 차원을 가지는 벡터 형태.
- **덧셈:** 포지셔널 인코딩 벡터는 해당 위치의 단어 임베딩 벡터에 **더해져서** 입력. (때로는 Concatenate 하기도 하지만, Transformer 논문에서는 덧셈 방식을 사용.) 모델은 단어의 의미와 위치 정보를 동시에 고려하여 표현을 학습.
- **학습 가능 또는 고정:**
        - **고정된(Fixed) 포지셔널 인코딩:** 트랜스포머 원 논문에서 제안된 방식으로, 사인(sine) 및 코사인(cosine) 함수를 사용하여 각 위치마다 고유한 패턴의 벡터를 생성. 학습할 파라미터가 없으며, 다양한 길이의 시퀀스에도 유연하게 적용.
        - **학습 가능한(Learned) 포지셔널 인코딩:** 각 위치마다 별도의 임베딩 벡터를 정의하고, 모델 학습 과정에서 함께 학습시키는 방식. BERT와 같은 모델에서 사용.
#### 4. 예시:
- 문장의 첫 번째 단어의 임베딩 벡터에 [0.0, 0.01, 0.02, ..., 0.05]와 같은 positional encoding 벡터가 더해져서 모델의 입력.

#### Embedding과 관계 

1. **목적의 분리:**
    - **임베딩:** 단어의 의미(콘텐츠)를 벡터로 표현.
    - **포지셔널 인코딩:** 단어의 위치(순서)를 벡터로 표현.
2. **결합 방식:**
    - 두 벡터는 일반적으로 더해서 (element-wise addition) 하나의 통합된 입력 표현. `입력_임베딩 = 단어_임베딩 + 포지셔널_인코딩`
3. **상호 보완적 역할:**
 - 두 정보가 결합하여 문장 내에서 **'어떤 단어가', '어느 위치에'** 있는지를 종합적으로 표현.
 - 모델이 복잡한 언어 패턴을 학습하게 함.

---
#### pytorch Example

##### [Embedding](https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html)

```python
nn.Embedding(num_embeddings=3, embedding_dim=5)
```
- num_embeddings : 인코딩할 대상의 모든 category 가지수.
- embedding_dim : Output Tensor 의 Dimension 지정.

```python
import torch 
import torch.nn as nn

input_tensor = torch.tensor(1)
# tensor(1)

embed = nn.Embedding(3, 5)
print(embed.weight)
# tensor([[-1.2975, -0.4792, -0.1931, -0.4409,  0.5042],
#         [ 0.2923, -3.0725,  0.4268,  1.5539, -0.3350],
#         [-0.5267, -0.6775, -0.2996,  0.4482,  0.3179]], requires_grad=True)

output_tensor = embed(input_tensor)
# tensor([ 0.2923, -3.0725,  0.4268,  1.5539, -0.3350],
#       grad_fn=<EmbeddingBackward0>)
```


[pytorch transformer tutorial](https://tutorials.pytorch.kr/beginner/transformer_tutorial.html)

```python
class PositionalEncoding(nn.Module):

    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x: Tensor) -> Tensor:
        """
        Arguments:
            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``
        """
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)
```