---
categories: 
title: 2.5.2. Recurrent Neural Network
created: 2025-03-22
tags:
---
---
#### *2.5.2. RNN*
---

### 순환신경망 (Recurrent Neural Network)

- 순서가 있는 **연속형 데이터(Sequential Data)** 의 패턴과 순서에 따른 변화의 영향관계를 학습하기 위해서 만든 신경망 모델
- 대표적인 연속형 데이터의 종류는 시계열, 언어, 계절 기후 데이터, 센서 데이터, 소리나 음악 데이터 등의 데이터 이다.
![[RNN_01.png|700x267]]

- 말 그대로 순환 신경망이다. 입력층에서는 sequence 에 따라 데이터가 입력되고 
- 히든층은 반복해서 입력되어 지는 데이터와 함께, 이전 스텝의 출력을 다시 입력 받는다.
-  다시 다음 sequence 의 입력과 히든층의 출력은 함께 반복해서 입력된다.
- 이를 입력되는 창 (**Window**: 입력되는 Sequence data 의 길이) 만큼 진행한다.
- 이때 2개의 출력이 있음에 유의 (이를 통해 위로 Stack이 가능)

- 이를 통해 입력되는 window 길이의 sequence data의 순차적인 변화 또는 패턴을 학습하게 된다.
- t 시점의 RNN 계층의 출력 텍터 생성
$$h_{t}\ =\  tanh(h_{t-1}W_{h} + x_{t}W_{x}+\ b) \ $$
![[RNN_02.png]]

- RNN 사용 형태
![[RNN_03.png]]

- 모델이 간단하고 길이와 상관없이 어떠한 sequence 데이터도 처리 가능하다. 비-선형적인 순차 데이터 분석에 강함.
- 시계열 데이터 분석 시 기존 시계열 통계분석에서 가지는 데이터의 정상성 문제 등에 강함.
- 순차입력으로 인핸 행렬 연산의 장점인 병렬분해 연산의 설계가 힘들어 GPU 의 이점을 살리기 힘듬
 - tanh 가 가진  gradient vanishing 과 exploding gradient 문제가 발생한다. 특히, sequence가 길어질 수록 신경망의 구조적 특성 상 모든 sequence 에 대한 정보를 계속 반영해야 하기 때문에 입력 window 의 길이가 길어질 수록 이런 문제가 심각해 진다.

### LSTM (Long Shot Term Memory)

![[LSTM_01.png]]

- Gate

1) **Cell State** : 핵심 정보 전달체로 모든 state를 지나오면 정보를 전달한다. 3개의 Gate를 통해  조절되어지는 정보를 다음 sequence 에 따른 state로 전달한다. 
2) Forget Gate : sigmoid 활성함수로 통과되는 출력을 통해서 과거 state 의 정보를 얼마나 Cell에 반영할 지 $C_{t-1}$ 에 product 한다. 
$$f_{t}\ =\ \sigma \ (W_{f}\ \cdot [h_{t-1},\ x_{t}]\ +\ b_{f}) $$
3) Input Gate : 이번 state sequence 에 입력된 정보에 대해서 어떻게(ex: 음, 양) 반영할지 C gate 에서 결정하고 얼마나 반영할지 결정하는 i gate로 학습해서 모두 product 해준다. 
$$
\begin{align}
	&i_{t}\ =\ \sigma \ (W_{i}\ \cdot [h_{t-1},\ x_{t}]\ +\ b_{i}) \\
	&\tilde C_{t}\ =\ tanh \ (W_{C}\ \cdot [h_{t-1},\ x_{t}]\ +\ b_{C}) 
\end{align}
$$
4) Update : 결정된 이번 sequence 의 정보를 Cell 에 더해서 반영해 준다.
5) Output Gate : 현재 state 의 출력을 결정하기 위해서 다시 현재 sequence 정보와 과거 state 정보를 입력으로 sigmoid를 통과해서 결정된 정보에 지금까지 계산된  cell정보를 product 한 후 이번 state 의 출력으로 내보낸다. 

- 입력되는 각 squence 를 통해 출력되는 정보를 **hidden state**라고 한다.

### pytorch LSTM
https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html

### pytorch GRU 
https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html
- **hidden state 의 출력이 하나**라는 특징을 가지고 있다.


### Example
---
#### RNN
- [예제 Reference](https://tutorials.pytorch.kr/intermediate/char_rnn_classification_tutorial.html)

```python
from __future__ import unicode_literals, print_function, division
from io import open
import glob
import os

def findFiles(path): return glob.glob(path)

print(findFiles('data/names/*.txt'))

import unicodedata
import string

all_letters = string.ascii_letters + " .,;'"
n_letters = len(all_letters)

# 유니코드 문자열을 ASCII로 변환, https://stackoverflow.com/a/518232/2809427
def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn'
        and c in all_letters
    )

print(unicodeToAscii('Ślusàrski'))

# 각 언어의 이름 목록인 category_lines 사전 생성
category_lines = {}
all_categories = []

# 파일을 읽고 줄 단위로 분리
def readLines(filename):
    lines = open(filename, encoding='utf-8').read().strip().split('\n')
    return [unicodeToAscii(line) for line in lines]

for filename in findFiles('data/names/*.txt'):
    category = os.path.splitext(os.path.basename(filename))[0]
    all_categories.append(category)
    lines = readLines(filename)
    category_lines[category] = lines

n_categories = len(all_categories)
```


#### LSTM
- [예제 Reference](https://tutorials.pytorch.kr/beginner/nlp/sequence_models_tutorial.html)

```python
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

# 1. 데이터 생성
def create_sequence_data(sequence_length, num_sequences, period=50):
    """
    사인(sine) 파동 시계열 데이터를 생성합니다.
    """
    data = []
    for _ in range(num_sequences):
        start_point = np.random.rand() * period # 시작점을 무작위로 설정하여 다양한 시퀀스 생성
        sequence = np.sin(np.linspace(start_point, start_point + sequence_length, sequence_length) * 2 * np.pi / period)
        data.append(sequence)
    return np.array(data)

# 시퀀스 길이 (과거 몇 개의 데이터를 볼 것인가)
sequence_length = 20
# 전체 시퀀스 수 (데이터셋 크기)
num_sequences = 1000
# 예측할 다음 스텝 수
prediction_length = 1

# 데이터 생성
raw_data = create_sequence_data(sequence_length + prediction_length, num_sequences)

# 학습 데이터와 테스트 데이터 분리
train_size = int(0.8 * num_sequences)
train_data = raw_data[:train_size]
test_data = raw_data[train_size:]

# 입력 (X)과 출력 (Y) 분리
# X는 과거 sequence_length 만큼의 데이터
# Y는 그 다음 prediction_length 만큼의 데이터 (여기서는 1 스텝 다음 값)
def prepare_data_for_lstm(data, sequence_length, prediction_length):
    X = []
    Y = []
    for seq in data:
        X.append(seq[:sequence_length])
        Y.append(seq[sequence_length:sequence_length + prediction_length])
    return np.array(X), np.array(Y)

X_train_np, Y_train_np = prepare_data_for_lstm(train_data, sequence_length, prediction_length)
X_test_np, Y_test_np = prepare_data_for_lstm(test_data, sequence_length, prediction_length)

# NumPy 배열을 PyTorch Tensor로 변환
# LSTM 입력 형태: (batch_size, sequence_length, input_size)
# 여기서는 각 시점의 입력이 1개의 특징을 가지므로 input_size=1
X_train = torch.FloatTensor(X_train_np).unsqueeze(-1)
Y_train = torch.FloatTensor(Y_train_np).unsqueeze(-1)
X_test = torch.FloatTensor(X_test_np).unsqueeze(-1)
Y_test = torch.FloatTensor(Y_test_np).unsqueeze(-1)

print(f"X_train shape: {X_train.shape}") # 예: torch.Size([800, 20, 1])
print(f"Y_train shape: {Y_train.shape}") # 예: torch.Size([800, 1, 1])

# 2. LSTM 모델 정의
class LSTMRegressor(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMRegressor, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # batch_first=True는 입력 텐서의 첫 번째 차원이 배치 사이즈임을 의미합니다.
        # 즉, (batch_size, sequence_length, input_size) 형태의 입력을 받습니다.
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        
        # LSTM의 최종 출력은 hidden_size 크기이므로, 이를 output_size로 매핑하는 Linear 계층
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # LSTM의 초기 hidden state와 cell state를 0으로 초기화
        # h0, c0의 형태: (num_layers, batch_size, hidden_size)
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # LSTM 순전파
        # output: 모든 시점의 출력 (batch_size, sequence_length, hidden_size)
        # (hn, cn): 최종 시점의 hidden state와 cell state
        lstm_out, (hn, cn) = self.lstm(x, (h0, c0))
        
        # 마지막 시점의 hidden state를 사용하여 최종 예측
        # lstm_out[:, -1, :]는 마지막 시점의 모든 hidden_size 특징을 가져옵니다.
        out = self.fc(lstm_out[:, -1, :])
        return out

# 3. 모델 인스턴스화, 손실 함수, 옵티마이저 설정
input_size = 1 # 각 시점의 특징 개수 (여기서는 사인 값 1개)
hidden_size = 50 # LSTM 은닉 상태의 크기
num_layers = 2 # LSTM 계층의 수
output_size = 1 # 예측할 출력의 크기 (다음 스텝의 값 1개)

model = LSTMRegressor(input_size, hidden_size, num_layers, output_size)
criterion = nn.MSELoss() # 평균 제곱 오차 손실
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# GPU 사용 설정 (가능하다면)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
X_train, Y_train = X_train.to(device), Y_train.to(device)
X_test, Y_test = X_test.to(device), Y_test.to(device)

# 4. 모델 학습
num_epochs = 200

for epoch in range(num_epochs):
    model.train() # 모델을 훈련 모드로 설정
    
    # 순전파
    outputs = model(X_train)
    loss = criterion(outputs, Y_train)
    
    # 역전파 및 최적화
    optimizer.zero_grad() # 이전 기울기 초기화
    loss.backward()       # 역전파
    optimizer.step()      # 가중치 업데이트
    
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# 5. 모델 평가 및 예측
model.eval() # 모델을 평가 모드로 설정 (dropout, batchnorm 등이 비활성화됨)
with torch.no_grad(): # 기울기 계산 비활성화
    test_predictions = model(X_test).cpu().numpy() # 예측 후 CPU로 이동하여 NumPy 변환

# 시각화 (첫 번째 테스트 시퀀스에 대한 예측)
plt.figure(figsize=(12, 6))

# 원본 테스트 시퀀스 (입력 + 정답)
# NumPy 배열로 다시 변환하고 차원 축소
original_test_sequence = X_test_np[0].flatten()
true_next_value = Y_test_np[0].flatten()

# 예측값
predicted_next_value = test_predictions[0].flatten()

# 전체 시퀀스 플롯
plt.plot(np.arange(sequence_length), original_test_sequence, 'b-', label='Input Sequence')
plt.plot(np.arange(sequence_length, sequence_length + prediction_length), true_next_value, 'g-', label='True Next Value')
plt.plot(np.arange(sequence_length, sequence_length + prediction_length), predicted_next_value, 'r--', label='Predicted Next Value')

plt.title('Time Series Prediction using LSTM')
plt.xlabel('Time Step')
plt.ylabel('Value')
plt.legend()
plt.grid(True)
plt.show()

# 여러 테스트 시퀀스에 대한 예측 및 실제 값 비교 (선택 사항)
num_samples_to_plot = 5
plt.figure(figsize=(15, 10))
for i in range(num_samples_to_plot):
    plt.subplot(num_samples_to_plot, 1, i + 1)
    
    original_seq = X_test_np[i].flatten()
    true_next = Y_test_np[i].flatten()
    predicted_next = test_predictions[i].flatten()

    plt.plot(np.arange(sequence_length), original_seq, 'b-')
    plt.plot(np.arange(sequence_length, sequence_length + prediction_length), true_next, 'g-', label='True')
    plt.plot(np.arange(sequence_length, sequence_length + prediction_length), predicted_next, 'r--', label='Predicted')
    
    if i == 0:
        plt.legend()
    plt.title(f'Test Sequence {i+1}')
    plt.grid(True)
plt.tight_layout()
plt.show()

```

