---
categories: 
title: 1.3.3.1.4. L1, L2 Regularizers
created: 2025-04-02
tags:
---
---
#### *1.3.3.1.4. L1, L2 Regularizers*
---

- pytorch Weight decay
```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=l2_lambda)
```

- L1, L2 
```python
import torch
import torch.nn as nn

# 모델 정의
model = nn.Linear(10, 1)

# 손실 함수 및 옵티마이저 정의
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# L1 및 L2 정규화 강도 설정
l1_lambda = 0.01
l2_lambda = 0.01

# 훈련 루프
for epoch in range(100):
    # 순전파 및 손실 계산
    outputs = model(inputs)
    loss = criterion(outputs, targets)

    # L1 정규화 항 추가
    l1_reg = torch.tensor(0., requires_grad=True)
    for param in model.parameters():
        l1_reg = l1_reg + torch.norm(param, 1)
    loss = loss + l1_lambda * l1_reg

    # L2 정규화 항 추가
    l2_reg = torch.tensor(0., requires_grad=True)
    for param in model.parameters():
        l2_reg = l2_reg + torch.norm(param, 2)
    loss = loss + l2_lambda * l2_reg

    # 역전파 및 가중치 업데이트
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

- Elastics Net
```python
import torch
import torch.nn as nn

# 모델 정의
model = nn.Linear(10, 1)

# 손실 함수 및 옵티마이저 정의
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# L1 및 L2 정규화 강도 설정
l1_lambda = 0.01
l2_lambda = 0.01

# 훈련 루프
for epoch in range(100):
    # 순전파 및 손실 계산
    outputs = model(inputs)
    loss = criterion(outputs, targets)

    # Elastic Net 정규화 항 추가
    l1_reg = torch.tensor(0., requires_grad=True)
    l2_reg = torch.tensor(0., requires_grad=True)
    for param in model.parameters():
        l1_reg = l1_reg + torch.norm(param, 1)
        l2_reg = l2_reg + torch.norm(param, 2)
    loss = loss + l1_lambda * l1_reg + l2_lambda * l2_reg

    # 역전파 및 가중치 업데이트
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```



---

- torch는 기본적으로 L2 형태의 weight decay를 지원한다는 주장과 실제로 L2로 실행된다고 하는 의견이 있음

```
In SGD optimizer, L2 regularization can be obtained by `weight_decay`. But `weight_decay` and L2 regularization is different for Adam optimizer. More can be read here: [openreview.net/pdf?id=rk6qdGgCZ](https://openreview.net/pdf?id=rk6qdGgCZ) 

– [Ashish](https://stackoverflow.com/users/2892806/ashish "388 reputation")

@Ashish your comment is correct that `weight_decay` and L2 regularization is different but in the case of PyTorch's implementation of Adam, they actually implement L2 regularization instead of true weight decay. Note that the weight decay term is applied to the gradient before the optimizer step [here](https://github.com/pytorch/pytorch/blob/40d1f77384672337bd7e734e32cb5fad298959bd/torch/optim/_functional.py#L94) 

– [Eric Wiener](https://stackoverflow.com/users/6942666/eric-wiener "6,007 reputation")

 [CommentedJan 21, 2022 at 16:18](https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch#comment125171392_46597531)

```

$$
\begin{align}
	Loss_{L2}(W)\ &=\ Loss(W)\ +\ \frac{1}{2}\cdot \lambda\ \cdot\ ||W||^{2}
	\\ \\
		&=\ \sum_{i=0}^{n}(y_{i}- \sum_{j=0}^{M}x_{ij}W_{j})^{2}\ +\ \frac{1}{2}  \cdot \lambda\ \cdot\ \sum_{j=0}^{M}W_{j}^{2} 
\end{align}
$$

```python
import torch
optimizier = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.9)

```


- 하드 코딩 구현

```python
import torch  
from torch.utils.data import DataLoader, TensorDataset  
from sklearn.model_selection import train_test_split  
import numpy as np  
import torch.nn as nn  
import torch.optim as optim  
import matplotlib.pyplot as plt

# Set seed for reproducibility  
np.random.seed(0)  
torch.manual_seed(0)

# Create a synthetic dataset  
X = np.random.randn(1000, 10).astype(np.float32)  
y = (np.random.randn(1000) > 0).astype(np.float32)  
  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  
  
train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))  
test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))  
  
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)


# Define the neural network model  
class SimpleNN(nn.Module):  
    def __init__(self):  
        super(SimpleNN, self).__init__()  
        self.fc1 = nn.Linear(10, 50)  
        self.fc2 = nn.Linear(50, 1)  
  
    def forward(self, x):  
        x = torch.relu(self.fc1(x))  
        x = self.fc2(x)  
        return x

# Define the training function with regularization  
def train_model(model, criterion, optimizer, train_loader, regularization_type=None, lambda_reg=0.01, epochs=20):  
    epoch_losses = []  
      
    for epoch in range(epochs):  
        model.train()  
        running_loss = 0.0  
          
        for inputs, targets in train_loader:  
            optimizer.zero_grad()  
            outputs = model(inputs)  
            loss = criterion(outputs.squeeze(), targets)  
              
            # Apply L1 regularization  
            if regularization_type == 'L1':  
                l1_norm = sum(p.abs().sum() for p in model.parameters())  
                loss += lambda_reg * l1_norm  
                
            # l1loss = loss + 0.005 * sum([torch.abs(p).sum() for p in model.parameters()])


            # Apply L2 regularization  
            elif regularization_type == 'L2':  
                l2_norm = sum(p.pow(2).sum() for p in model.parameters())  
                loss += lambda_reg * l2_norm  

			# l2loss = loss + 0.005 * sum([torch.square(torch.abs(p)).sum() for p in model.parameters()])


              
            loss.backward()  
            optimizer.step()  
              
            running_loss += loss.item() * inputs.size(0)  
          
        epoch_loss = running_loss / len(train_loader.dataset)  
        epoch_losses.append(epoch_loss)  
        print(f"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}")  
      
    return epoch_losses

# Define the evaluation function  
def evaluate_model(model, test_loader):  
    model.eval()  
    correct = 0  
    total = 0  
      
    with torch.no_grad():  
        for inputs, targets in test_loader:  
            outputs = model(inputs)  
            predicted = (outputs.squeeze() > 0.5).float()  
            total += targets.size(0)  
            correct += (predicted == targets).sum().item()  
      
    accuracy = correct / total  
    print(f"Accuracy: {accuracy:.4f}")  
    return accuracy

# Plot loss over epochs  
def plot_training_loss(losses, title):  
    plt.figure(figsize=(10, 5))  
    plt.plot(range(1, len(losses) + 1), losses, marker='o')  
    plt.title(title)  
    plt.xlabel('Epochs')  
    plt.ylabel('Loss')  
    plt.grid(True)  
    plt.show()

  
# Training and evaluating with L1 regularization
print("Training with L1 Regularization:")
model = SimpleNN()
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
l1_losses = train_model(model, criterion, optimizer, train_loader, regularization_type='L1', lambda_reg=0.01)
evaluate_model(model, test_loader)
plot_training_loss(l1_losses, 'Training Loss with L1 Regularization')

# Reinitialize model, optimizer, and train with L2 regularization
print("\nTraining with L2 Regularization:")
model = SimpleNN()
optimizer = optim.SGD(model.parameters(), lr=0.01)
l2_losses = train_model(model, criterion, optimizer, train_loader, regularization_type='L2', lambda_reg=0.01)
evaluate_model(model, test_loader)
plot_training_loss(l2_losses, 'Training Loss with L2 Regularization')
```


```python
def train(model, loss_func, optimizer, x, y, epochs,
    c=1.0, penalty='l1', sparse_threshold=0.0005, use_gpu=False):

    # for training error compute
    y_true = y.numpy()

    # regularity
    c = torch.FloatTensor([c])
    p = 1 if penalty == 'l1' else 2

    # cuda
    use_cuda = use_gpu and torch.cuda.is_available()
    if use_cuda:
        print('CUDA is {}available'.format('' if use_cuda else 'not '))
        model = model.cuda()
        x = x.cuda()
        y = y.cuda()
        c = c.cuda()

    # Loop over all epochs
    for epoch in range(epochs):

        # clean gradient of previous epoch
        optimizer.zero_grad()

        # predict
        y_pred = model(x)

        # defining cost
        loss = loss_func(y_pred, y)
        regularity =  torch.norm(model.fc.weight, p=p)
        cost = loss + c * regularity

        # back-propagation
        cost.backward()
        optimizer.step()

        if epoch % 100 == 0 or epoch <= 10:
            # training error
            if use_cuda:
                y_pred = y_pred.cpu()
            y_pred = torch.argmax(y_pred, dim=1).numpy()            
            train_accuracy = accuracy(y_true, y_pred)

            # informations
            l1_norm = torch.norm(model.fc.weight, p=1)
            l2_norm = torch.norm(model.fc.weight, p=2)
            parameters = model.fc.weight.data.cpu().numpy().reshape(-1)
            t = abs(parameters).max() * sparse_threshold
            nz = np.where(abs(parameters) < t)[0].shape[0]

            print('epoch = {}, training accuracy = {:.3}, l1={:.5}, l2={:.3}, nz={}'.format(
                epoch, train_accuracy, l1_norm, l2_norm, nnz))

    if use_cuda:
        model = model.cpu()

    return model
```

