---
categories: 
title: 1.3.5.1 Tensor
created: 2025-03-10
tags:
---
---
#### *1.3.5.1 Tensor*
---

텐서(Tensor)는 다차원 배열(array) 또는 행렬(matrix)를 일반화한 개념으로, 머신러닝 및 딥러닝에서 데이터를 표현하는 방법으로 수학, 물리학, 컴퓨터 과학 등 다양한 분야에서 사용되며, 데이터의 구조와 관계를 나타내는 데 유용하다.

- **다차원 배열:** 텐서는 스칼라, 벡터, 행렬을 포함하는 다차원 배열.
    - 스칼라: 0차원 텐서 (단일 숫자)
    - 벡터: 1차원 텐서 (숫자들의 배열)
    - 행렬: 2차원 텐서 (숫자들의 2차원 배열)
    - 3차원 이상: 3차원 이상의 텐서는 데이터를 더 복잡한 형태로 표현. 예: 이미지는 3차원 텐서(높이, 너비, 색상 채널)로 표현.
- **랭크(Rank):** 텐서의 차원 수. 예: 벡터 => 랭크 1, 행렬 => 랭크 2.
- **모양(Shape):** 각 차원의 크기. 예: 3x4 행렬 => shape (3, 4).
- **데이터 타입(Data Type):** 텐서에 저장된 데이터의 종류. 예: 정수, 실수, 문자열 등

- tensor norm 계산


```python
import torch

vector = torch.tensor([5.0, 7.0])

L1_norm = torch.norm(vector,p=1)
L2_norm = torch.norm(vector,p=2)

print(f"L1 norm : {L1_norm}")
print(f"L2 norm : {L2_norm}")

``````

### (1) pytorch : Tensor

```python
import torch
import numpy as pd

# 파이썬 리스트의 중첩형태에서 바로 tensor 생성 가능 
data = [[1, 2], [3, 4]]
x_data = torch.tensor(data)
x_data

# numpy 배열에서 생성 가능. (역으로 torch tensor 에서 numpy array로 변환 가능)
np_array = np.array(data)
x_np = torch.from_numpy(np_array)
x_np

x_ones = torch.ones_like(x_data) # x_data의 속성을 유지합니다.
print(f"Ones Tensor: \n {x_ones} \n")

x_rand = torch.rand_like(x_data, dtype=torch.float) # x_data의 속성을 덮어씁니다.
print(f"Random Tensor: \n {x_rand} \n")

shape = (2, 3,)
rand_tensor = torch.rand(shape)
ones_tensor = torch.ones(shape)
zeros_tensor = torch.zeros(shape)

print(f"Random Tensor: \n {rand_tensor} \n")
print(f"Ones Tensor: \n {ones_tensor} \n")
print(f"Zeros Tensor: \n {zeros_tensor}")

```

```python
# Tensor's Attribute
tensor = torch.rand(3, 4)

print(f"Shape of tensor: {tensor.shape}")
print(f"Datatype of tensor: {tensor.dtype}")
print(f"Device tensor is stored on: {tensor.device}")
```

### (2) pytorch : Tensor Operation

1) 속성의 디바이스 변경
```python
# GPU가 존재하면 텐서를 이동합니다
if torch.cuda.is_available():
  tensor = tensor.to('cuda') # 연산에 대한 대상 메모리 위치를 옴김.
  print(f"Device tensor is stored on: {tensor.device}")
```

2) Tensor의 인덱싱과 슬라이싱 : numpy 형식으로 접근 
```python
tensor = torch.ones(4, 4)
print(tensor)
tensor[:,1] = 0
print(tensor)
```
```output
```


3) 텐서 합치기
```python
t1 = torch.cat([tensor, tensor, tensor], dim=1)
print(t1)
```

4) 텐서 곱하기 (element-wise product vs matrix multiplication)
```python

# element-wise product

# 요소별 곱(element-wise product)을 계산합니다
print(f"tensor.mul(tensor) \n {tensor.mul(tensor)} \n")
# 다른 문법:
print(f"tensor * tensor \n {tensor * tensor}")


# matrix multiplication
print(f"tensor.matmul(tensor.T) \n {tensor.matmul(tensor.T)} \n")
# 다른 문법:
print(f"tensor @ tensor.T \n {tensor @ tensor.T}")
```

5) in-place (바꿔치기 연산)
```python
# 대상 tensor 자체를 연산해서 바꿈
print(tensor, "\n")
tensor.add_(5)
print(tensor)
```

>[!warning] torch의 경고
> " 바꿔치기 연산은 메모리를 일부 절약하지만, 기록(history)이 즉시 삭제되어 도함수(derivative) 계산에 문제가 발생할 수 있습니다. 따라서, 사용을 권장하지 않습니다."
> 자동으로 계산되는 gradient 계산을 통해 backpropagation 추적에 문제가 생길 수 있다.

- 의도적으로 gradient 계산을는 멈추는 방법 
	(예: validation, test, predict 수행 시.)
- `toch.no_grad()`
```python 
import torch

x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)

with torch.no_grad():
    z = x + y  # 연산 기록에 영향을 주지 않음

print(z)
print(z.requires_grad)  # False
```

- `tensor.detach()`
- 텐서의 일부만 연산 기록에서 분리해야 하는 경우: `detach()` 메서드를 사용하면 특정 텐서만 연산 기록에서 분리.
- 텐서를 NumPy 배열로 변환해야 하는 경우: `detach()` 메서드를 사용하여 텐서를 연산 기록에서 분리한 후 NumPy 배열로 변환
```python
import torch

x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)

x_detached = x.detach()
z = x_detached + y  # 연산 기록에 영향을 주지 않음

print(z)
print(z.requires_grad)  # False
```

- 연산 기록이 비활성화된 텐서에 대한 기울기는 계산할 수 없어, 역전파(backpropagation)를 수행할 수 없을 수 있다.

- gradient를 계산하도록 설정하기.
- `enable_grad()`
```python
import torch

x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)

with torch.no_grad():
    z = x + y  # 연산 기록에 영향을 주지 않음
    with torch.enable_grad():
        w = z * 2  # 이 부분은 gradient 계산 활성화

print(w.requires_grad)  # True
```

- `torch.set_grad_enabled(True)`
```python
import torch

x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)

with torch.no_grad():
    z = x + y  # 연산 기록에 영향을 주지 않음
    with torch.set_grad_enabled(True):
        w = z * 2  # 이 부분은 gradient 계산 활성화

print(w.requires_grad)  # True
```

- `requires_grad=True`
```python
import torch

x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)

z = x + y  # gradient 계산 활성화

print(z.requires_grad)  # True
```


### (3) pytorch : NumPy 변환(Bridge)

```python
t = torch.ones(5)
print(f"t: {t}")
n = t.numpy()
print(f"n: {n}")

t.add_(1)
print(f"t: {t}")
print(f"n: {n}")

n = np.ones(5)
t = torch.from_numpy(n)

np.add(n, 1, out=n)
print(f"t: {t}")
print(f"n: {n}")

```

