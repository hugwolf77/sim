---
layout: single
title: 4주차 데이터 수집과 관리
categories: 수업
tags:
  - 수업
  - Machine_learning
  - Data_analysis
toc: "true"
toc_sticky: "true"
toc_label: 목차
author_profile: "false"
nav: '"docs"'
search: "true"
use_math: "true"
created: "{{date}} {{time}}"
---
---
## 휴강 및 보강 일정 조율

![[휴강 및 보강일.png]]

1) python 모듈
- seaborn
- Plotly
- PyGWalker

3) 상용 서비스 Visual Tool
- FineReport
- Chartblocks 
- Google Data Studio
- Tableau
- Microsoft Power BI

4) JavaScripts
- D3



---
# *1. 데이터의 수집 관리*
---

- 연습 문제를 통해서 지난 시간에 
		DataWarehouse 및 Data Lake=> Data flow (PipeLine) 구성

### 데이터 파이프라인이란?  : 

- App 수준의 input dataset 세팅과 입력 흐름을 정의하는 것이 아니라 system 차원의 학습 데이터 데이터 베이스 관리 시스템 수준을 의미함.

다양한 데이터 소스에서 원시 데이터를 수집한 다음 분석을 위해 Data Lake 또는 Data WareHouse와 같은 데이터 저장소로 이전하는 방법.  
데이터는 저장소로 이동 전 데이터 처리 과정을 거쳐 저장을 하여 탐색형 데이터 분석, 데이터 시각화, 머신 러닝 작업에 활용 할 수 있도록 하는 것.

- 사용서비스, Cloude 서비스 등을 조사해보세요.
	ex) 
	[Google Dataflow](https://cloud.google.com/dataflow?hl=ko)
	[Tensorflow TFX](https://www.tensorflow.org/tfx?hl=ko)
	
	[DeltaTroch](https://delta.io/blog/deltatorch-intro/)
	

### 데이터 파이프라인 유형
**일괄 처리 (batch Processing)**  
batch는 미리 설정된 시간에 일괄적으로 로드하여, 대용량 데이터를 처리 할 수 있도록하는 일괄 처리 방식. 장점으로는 **안정성이 높음**.
**스트리밍 데이터(streaming data)**  
일괄 처리와 달리, 데이터를 지속적으로 업데이트 할 때 활용됨. 예로 POS(point of Sale) 시스템은 제품의 실시간 재고 여부가 필요하기 때문에 실시간 데이터가 필요함.  장점으로는 **지연시간 짧음.**

### 데이터 분석 파이프라인 5단계
**1. CAPTURE (데이터 수집)**
- 시스템 log, 다른 DW, 다른 Data Lake, DB, sensor, App 
- 수집할 수 있는 연결을 가지고 있는 것.
**2. PROCESS (데이터 처리)**
**3. STORE (저장)**
**4. Analyze (분석)**
**5. USE (데이터 사용 및 시각화)**

### 사용되는 도구 :
**1) 분산 처리 프레임워크**
- Hadoop, Spark
**2) 데이터 레이크**
- S3, HDFS
- DB
	 : 과거  -  NoSQL (MongDB, Cassandra) 와 RDMBS (Postgrass, Oracle, Sql-server)
	 : 최근  -  정보의 사용 목적과 분석 목적에 따라 다양한 DB 가 활성화되고 있음.
		   1) Graph DB - 관계 정보, 네트워크 정보를 그래프 정보로 저장 (Neo4j)
		   2) TimeSeries DB - 시계열 데이터에 특화되어 빠른 입력과 시계열 저장이 특징 (Influx DB)
		   3) Vector DB - 다양한 속성이나 특성을 나타내는 다차원 벡터로 데이터를 저장, ML 이 중요해지면서 주목받고 있음. (Pinecone, Chroma)
	 
- filestream
**3) Work flow 관리** : ETL 과 같은 작업의 흐름을 관리하는 툴들
- Airflow, Oozie, Dagster, Argo


---
### log 수집  - 시스템 모니터링

- [Splunk](https://www.splunk.com/en_us/download.html?utm_campaign=google_apac_kor_en_search_brand&utm_source=google&utm_medium=cpc&utm_content=free_trials_downloads&utm_term=splunk&_bk=splunk&_bt=690424212238&_bm=e&_bn=g&_bg=159169282696&device=c&gad_source=1&gclid=CjwKCAjwnv-vBhBdEiwABCYQA29NnNgaONQbSLM1ZGjRu2WDP5Fy7uzjcHzvtGuCsk_-_tEwfXCGMhoCj6YQAvD_BwE)
- [XpoLog](https://www.xpolog.com/)
- [GA4](https://developers.google.com/analytics/devguides/collection/ga4?hl=ko)
- [Graylog](https://www.graylog.org/)
- [Fluentd](https://www.fluentd.org/)
- [Flume](https://flume.apache.org/)
- [Kiwi Syslog ® Server](https://www.solarwinds.com/free-tools/kiwi-free-syslog-server?CMP=ORG-BLG-DNS-X_WW_X_NP_X_X_EN_X_X-KSSF-20191116_TOP7BestFREELog_X_X_VidNo_X-X)
- [PRTG Network Monitor](https://www.paessler.com/log-monitoring)
-  [엘라스틱(Elastic)](https://www.elastic.co/products)
	- 'LAMP' : 리눅스, 아파치 HTTP 웹 서버, 마이SQL 데이터베이스 및 PHP(또는 펄(Perl)이나 파이썬(Python))로 구성된 웹 스택
	- 'ELK' :  색 기능용 [엘라스틱서치(Elasticsearch)](https://www.elastic.co/products/elasticsearch), 데이터 수집용 [로그스태시(Logstash)](https://www.elastic.co/products/logstash), 데이터 시각화용 [키바나(Kibana)](https://www.elastic.co/products/kibana)로 작성한 로그 분석 스택

-  [로그신(Logsene)](https://sematext.com/logsene/)
	- ELK를 서비스로 제공
	- [로그에이전트(Logagent)](https://sematext.com/logagent) 다양한 소스의 로그를 가져와 세마텍스의 클라우드 또는 엘라스틱서치 인스턴스로 전달하는 오픈 소스 프로젝트

- [로그자이오(Logz.io)](https://logz.io/)
	- '라이브 테일(live tail)'(콘솔에서 실시간으로 로그를 보는 기능), 아마존(Amazon) S3 오브젝트 스토리지에 자동 보관하는 기능과 함께 ELK 서비스를 제공

- [큐박스(Qbox)](https://qbox.io/)
	- AWS, IBM 클라우드, 랙스페이스(Rackspace) 등 다양한 클라우드 인프라에 대한 ELK 스택 관련 모든 호스팅 에디션을 제공

 - [수모 로직(Sumo Logic)](https://www.sumologic.com/)
	 - 머신 러닝과 예측 분석을 사용해 변칙 데이터와 비정상 데이터를 발견해 잠재적인 장애 요소를 찾을 수 있도록 돕는 클라우드 로그 분석 서비스

- [페이퍼트레일(Papertrail)](https://papertrailapp.com/)
	- 수집된 로그의 라이브 뷰를 보거나 편리한 검색 기능 및 로그 히스토리 내의 컨텍스트 링크 등 다른 경쟁 업체들의 것과 비슷한 많은 기능을 보유

- [로글리(Loggly)](https://www.loggly.com/product/)
	- 서비스로 적용된 광범위한 범주에서 로그를 수집하는 클라우드 서비스

- [인사이트옵스(InsightOps)](https://www.rapid7.com/products/insightops/)
	- 래피드7(Rapid7)이 제공하는 클라우드 호스팅 분석, 가시성과 자동화 제품군의 일부

- [큐박스(Qbox)](https://qbox.io/)
	- AWS, IBM 클라우드, 랙스페이스(Rackspace) 등 다양한 클라우드 인프라에 대한 ELK 스택 관련 모든 호스팅 에디션을 제공


----
## 공부용 데이터 조사 소개
# SMD (Server Machine Dataset)

# CICIDS2018 (CSE-CIC-IDS2018 on AWS)
https://www.unb.ca/cic/datasets/ids-2018.html

[AWS CLI 설치 ](https://docs.aws.amazon.com/ko_kr/cli/latest/userguide/getting-started-install.html)

```cmd
 aws s3 sync --no-sign-request --region <your-region> "s3://cse-cic-ids2018/" "dest-dir"
```

```cmd
aws s3 sync --no-sign-request --region ap-northeast-2 "s3://cse-cic-ids2018/" "D:\2024\영남이공대\수업준비\dataset"
```

# IDS 2018 Intrusion CSVs (CSE-CIC-IDS2018)
https://www.kaggle.com/datasets/solarmainframe/ids-intrusion-csv

# Kitsune Network Attack Dataset
https://www.kaggle.com/datasets/ymirsky/network-attack-dataset-kitsune

---



# *2. ODBC -  SQLite3, SQLAlchemy*
---
- 가장 기본적인  데이터의 store 구조인 RDBMS (여전히 system의 DB 구조의 중심)

- *SQLite3*
	 a. *SQLite*는 별도의 서버 프로세스가 필요 없고 SQL 언어의 비표준 변형을 사용하여 데이터베이스에 액세스할 수 있는 경량 디스크 기반 데이터베이스를 제공하는 C 라이브러리.
	 c. SQLite를 사용하여 응용 프로그램을 프로토타입 한 다음 PostgreSQL 이나 Oracle과 같은 더 큰 데이터베이스로 코드를 이식할 수 있음.
	 c. SQLite3 는 SQLite 데이터베이스용 python 기본 인터페이스 라이브러리 모듈

- *일반 RDBMS 인터페이스*

```python
# sqlserver(MSSQL)
!pip install oyodbc # 또는 pymssql
import pyodbc # import pymssql

SERVER = '<server-address>' 
DATABASE = '<database-name>' 
USERNAME = '<username>' 
PASSWORD = '<password>'

connectionString = f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={SERVER};DATABASE={DATABASE};UID={USERNAME};PWD={PASSWORD}'

conn = pyodbc.connect(connectionString)

SQL_QUERY = """ 
	SELECT 
		TOP 5 c.CustomerID, 
		c.CompanyName, 
		COUNT(soh.SalesOrderID) AS OrderCount 
	FROM 
		SalesLT.Customer AS c LEFT OUTER JOIN SalesLT.SalesOrderHeader AS soh 
	ON c.CustomerID = soh.CustomerID 
	GROUP BY c.CustomerID, c.CompanyName 
	ORDER BY OrderCount DESC; 
"""
cursor = conn.cursor() 

cursor.execute(SQL_QUERY)

conn.commit()

cursor.close() 
conn.close()

# Oracle DB
!pip install oracledb   # oracledb 라이브러리 설치하기
import oracledb  

```python
conn = oracledb.connect(user="사용자이름", password="비밀번호", dsn="호스트이름:port/SID")   # DB에 연결 (호스트이름 대신 IP주소 가능)
cursor = conn.cursor()   # 연결된 DB 지시자(커서) 생성

```python
cursor.execute("SQL문장")       # DB 명령 실행 (cursor가 임시 보관)
out_data = cursor.fetchall()   # cursor가 임시 보관한 내용을 out_data에 저장 (결과는 리스트)
# out_data 내용 출력해보기
for record in ou_data :
	print(record)

conn.commit()

cursor.close() 
con.close()   # DB 연결 해제
```

- *SQL:Structured Query Language* 
	a. DB에서 데이터를 추출하고 조작하는 데에 사용하는 데이터 처리 언어
	b. DB에 저장된 데이터를 효율적으로 추출하고 분석할 수 있기 때문에 SQL은 빅-데이터를 다루기 위한 필수적인 언어로 자리 잡음. (기초적인 쿼리 언어 간단한 절차나 통계는 가능)
	c. 하지만 복잡한 모델이나 분석기법은 사용할 수 없음 


- DB 연결 기본 python 연결 방법 : 

```python

import sqlite3

# con = sqlite3.connect(':memory:') # 메모리 사용
conn = sqlite3.connect('./test.db') # DB 파일을 사용

# 커서 생성
cur = conn.cursor()
# 테이블 생성
cur.execute("CREATE TABLE PhoneBook(Name text, PhoneNum text);")

# 테이블에 데이터 입력
cur.execute("INSERT INTO PhoneBook Values('Derick', '010-1234-5678');")

# seq 데이터를 사용한 데이터 입력
dataList = (('Tom', '010-543-5432'), ('DSP', '010-123-1234'))
cur.executemany("INSERT INTO PhoneBook VALUES(?, ?);", dataList)

# 테이블 조회
cur.execute('SELECT * FROM PhoneBook')
for row in cur:
    print(row)

cur.execute('SELECT * FROM PhoneBook')
cur.fetchone()
cur.fetchone(2)
cur.fetchmany(size=3)
cur.fetchall()

conn.commit()

cursor.close() 
conn.close()   # DB 연결 해제

```


#### CRUD :

|                    |           |         |
| ------------------ | --------- | ------- |
| **이름**             | **조작**    | **SQL** |
| Create             | 생성        | INSERT  |
| Read(또는 Retrieve)  | 읽기(또는 인출) | SELECT  |
| Update             | 갱신        | UPDATE  |
| Delete(또는 Destroy) | 삭제(또는 파괴) | DELETE  |

```python

import sqlite3
import datetime

print('sqlite3 version : ', sqlite3.version)

conn = sqlite3.connect("sqldb.db")

cur = conn.cursor()
print('cursor type : ',type(cur))

# 데이터 추가 날짜 값 추출하기
now = datetime.datetime.now()
print('now : ', now)
nowDatetime = now.strftime('%Y-%m-%d %H:%M:%S')
print('nowDatetime : ', nowDatetime)

# 테이블 생성( Data Type : TEXT, NUMERIC, INTEGER, REAL, BLOB)
cur.execute(
    """
    create table if not exists sqldb(
    id interger primary key,
    username text, 
    email text, 
    phone text,
    website text,
    regdate text)
    """
)

# 데이터 입력
cur.execute(
    """
    INSERT INTO sqldb 
    VALUES(
        1,
        'Kim',
        'kim@cozlab.com',
        '010-1234-5678',
        'cozlab.com',
        ?
        )
    """,
    (nowDatetime,))
    # ? 부분에 날짜(nowDatetime,)를 뒤에 값을 튜플 형태로 집어 넣어준다.

# 데이터 insert 튜플형식
cur.execute(
    """
    INSERT INTO sqldb(id, 
                      username, 
                      email, 
                      phone, 
                      website, 
                      regdate
                    )
    VALUES (?,?,?,?,?,?)
    """,
    (2,
    'Park',
    'park@naver.com',
    '010-3456-4567',
    'park.com',
    nowDatetime
    )
)

# 데이터 insert many 형식(튜플, 리스트)
# 많은 양의 데이터를 한꺼번에 넣는 방법

userList = (
    (3,'Lee','lee@navercom','010-3333-3333','lee.com',nowDatetime),
    (4,'Cho','cho@navercom','010-4444-4444','cho.com',nowDatetime),
    (5,'Yue','yue@navercom','010-5555-5555','yue.com',nowDatetime),
    (6,'Sea','sea@navercom','010-6666-6666','sea.com',nowDatetime),
)

cur.executemany(
    """
    INSERT INTO sqldb(
                      id,
                      username,
                      email,
                      phone,
                      website,
                      regdate
                    )
    VALUES (?,?,?,?,?,?)
    """,
    userList
)

conn.commit()

# DB 접속을 더 이상 하지 않는다면, 접속 해제, 자원 반환하기
cur.close()
conn.close()

```



---
# *3. API - Request*
---
```python
!pip install lxml
!pip install html5lib
import pandas as pd 
tables = pd.read_html('https://astro.kasi.re.kr//life/pageView/5',encoding='utf-8') 
df_astro = tables[0]
print(df_astro.shape)
print(df_astro.info())
df_astro.head()

df_krx = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13',encoding='cp949')[0]
df_krx['종목코드'] = df_krx['종목코드'].map('{:06d}'.format)
df_krx.sort_values(by='종목코드')
df_krx.head(10)
```


- *API(Application Programming Interface)* 란?
	: 응용 프로그램에서 사용할 수 있도록, 운영 체제나 프로그래밍 언어가 제공하는 기능을 제어할 수 있게 만든 인터페이스 -위키백과

- python HTTP, API 관련 주요 라이브러리 : urllib , requests
	1) (Client) Request 측면
	2) (Server) 측면

- requests 라이브러리를 사용하면 직관적인 method로 요청을 
	https://pypi.org/project/requests/
	https://requests.readthedocs.io/en/latest/
	
	- GET 방식: `requests.get()`
	- POST 방식: `requests.post()`
	- PUT 방식: `requests.put()`
	- DELETE 방식: `requests.delete()`

- openAPI : system의 서비스를 공개적으로 연결해서 제공

- RPC(Remote Procedure Call)
	: 현재 실행 중인 프로세스의 주소공간 내부가 아닌, 외부의 프로세스 또는 원격지의 프로세스와 상호작용하기 위한 기능


- json 과 pandas 로 dict  형태 변형 통신

```python
import numpy as np
import pandas as pd
import json

df = pd.read_csv('./data/movie/movies.csv')   

with open('./data/csv_to_json.json', 'w', encoding='utf-8') as f:
    df.to_json(f, force_ascii=False, orient='columns')

ata = {'name': ['아이유', '김연아', '홍길동'], 
        'dept': ['CS', 'Math', 'Mgmt'],
        'age': [25, 29, 30]}

# dict => df 
df = pd.DataFrame(data)  

'''
name	dept	age
0	아이유	CS	25
1	김연아	Math	29
2	홍길동	Mgmt	30
'''   

with open('./data/student_json_column.json', 'w', encoding='utf-8') as f:
    df.to_json(f, force_ascii=False, orient='columns')


```

- urllib
```python

import numpy as np
import pandas as pd
import json
import urllib
# 서버 쪽에 request 를 보내야함. httprequest  라이브러리를 통해서 해야함 >> urllib

############ 2. 영화진흥위원회 Open API를 호출하기를 위한 url이 있어야함
movie_url = 'http://www.kobis.or.kr/kobisopenapi/webservice/rest/boxoffice/searchDailyBoxOfficeList.json'
# key 값과 targetDt 는 들어가야함. 이 api의 필수 조건임. 
query_string = '?key=<여기는 키값입니다>&targetDt=20210801'
# 그래서 url을 합쳐준다면...
last_url = movie_url + query_string

# 이 Url을 이용해서 서버프로그램을 호출
result_obj = urllib.request.urlopen(last_url) 
# response
print(result_obj) 

# 3. 가져온 json 데이터를 dictionary로 
result_json = result_obj.read()  
result_dict = json.loads(result_json)  
print(result_dict)

rank_list = []  # 빈 리스트를 만들자
title_list = []
sales_list = []

for tmp in result_dict['boxOfficeResult']['dailyBoxOfficeList']:
    rank_list.append(tmp['rank'])
    title_list.append(tmp['movieNm'])
    sales_list.append(tmp['salesAmt'])
# list 확인
print(rank_list)
print(title_list)
print(sales_list)

df = pd.DataFrame({
    'rank': rank_list,
    'title': title_list,
    'salesAmt': sales_list
})  

```


### (Server) FastAPI, SQLite3, Pydantic

```python
!pip install fastapi uvicorn
# !pip install sqlite
```

- *app.py*
```python
# app.py

from fastapi import FastAPI  
from db_conn import *
 

# create_table() # Call this function to create the table

# create API
app = FastAPI()  
  
@app.get("/")  
def read_root():  
	return {"message": "Welcome to the CRUD API"}

@app.post("/books/")  
def create_book_endpoint(book: BookCreate):  
	book_id = create_book(book)  
	return {"id": book_id, **book.dict()}


@app.post("/books/")  
def create_book(book: BookCreate):  
	# Logic to add the book to the database  
	return book

```

- *db_conn.py*
```python

import sqlite3
from dataclass import *

# create sqlite3 DB connect def   
def create_connection():  
	connection = sqlite3.connect("books.db")  
	return connection

def create_table():  
	connection = create_connection()  
	cursor = connection.cursor()  
	cursor.execute("""  
			CREATE TABLE IF NOT EXISTS books (  
			id INTEGER PRIMARY KEY AUTOINCREMENT,  
			title TEXT NOT NULL,  
			author TEXT NOT NULL  
			)  
		""")  
	connection.commit()  
	connection.close()  
	  
# create_table() # Call this function to create the table


# Insert data into DB use dataclass 
def create_book(book: BookCreate):  
	connection = create_connection()  
	cursor = connection.cursor()  
	cursor.execute(
				"INSERT INTO books (title, author) VALUES (?, ?)",
				(book.title, book.author)
				)  
	connection.commit()  
	connection.close()
```


- *dataclass.py*
```python

from pydantic import BaseModel  

# create dataclass 
class BookCreate(BaseModel):  
	title: str  
	author: str  
  
class Book(BookCreate):  
	id: int
```


```cmd
uvicorn app:app --reload
```

- Browser : http://127.0.0.1:8000/docs

- DB Browser for sqlite3
https://sqlitebrowser.org/dl/

 ### Sqlalchemy
- ORM:Object Relational Mapping :  객체지향 언어를 이용하여 호환되지 않는 type system 간의 데이터 전환을 의미. 관계형 데이터베이스에 객체지향 언어로 접근할 수 있게끔 매핑을 해주는 다리 역할.
	1)  장점
		- SQL을 별도로 익히지 않아도 DB를 활용.
		- DB를 변경할 때 쿼리를 하나하나 수정하지 않아도 됨.
		- SQL injection 를 방지할 수 있음.
	2) 단점
		- SQL을 아는 사람이라면 ORM을 또 별도로 배워야 함..
		- 복잡한 쿼리가 필요한 경우 성능 저하를 일으키거나 ORM으로 치환이 어려움.



>[!NOTE]
> - Pandas 의 DataFram 형태의 데이터에서 행 별로 dict 형태 또는 json 형태로 변환하는 프로세스 고려해야 함. 
> - 데이터 구조적인 활용 또는 효율성 등에 대해서 고려해야 함.
> - pydantic 과 같은 class 형태를 사용할 경우 효율성은 떨이 짐.  대신 검정과 활용이 쉬움.
> - Sqlalchemy 등을 사용할 경우 DB  연결과 관리가 쉬워짐. 대신 변환 등이 복잡해짐.



### (Client) requests

**OpenAPI 에 접근해서 Data 가져오기**


[**PublicDataReader**](https://github.com/WooilJeong/PublicDataReader) [**제작자분 블로그**](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwooiljeong.github.io%2Fpython%2Fpdr-ecos%2F)

```python

!pip install PublicDataReader #--upgrade

from PublicDataReader import Ecos

service_key = "개별 api-key  # Ecos에 접속할 수 있는 개별 발급 받은 API key 를 입력

api = Ecos(service_key)               # API 접속
df_list = api.get_statistic_table_list()   # 데이터 조회 - 통계자료에 대한 리스트 자료
df_list.head()

# 100대 주요 경제 통계지표
df_100_list = api.get_key_statistic_list()
df_100_list.head(5)

# 경제통계 용어 에 대한 정보도 조회 가능.
info = api.get_statistic_word(용어="소비자동향지수")
info
```

[ECOS API 개발 가이드](https://ecos.bok.or.kr/api/#/)



---
# *4. Scraper - BS4*
---
*Scraper* is a very simple (but limited) data mining extension for facilitating online research when you need to get data into spreadsheet form quickly.

https://pypi.org/project/beautifulsoup4/

_DOM_. Document Object Model의 약자로, HTML요소를 JavaScript Object처럼 조작할 수 있는 Model 

- 정적 - 링크나  post 등  또는 js 등의 동적 페이지 추적이 어려움.
```python
!pip install requests
!pip install bs4
```

```python
import requests
from bs4 import BeautifulSoup as bs
import pandas as pd

res = requests.get('https://finacnce.naver.com/')
html = res.text
soup = bs(html, 'html.parser')
articles = soup.select('.news_area a') # DOM 에 사용된 클라스 명으로 접근

title = []
url = []

for news in articles:
	title.append(news.text)
	url.append(news['href'])

df = pd.DataFrame()
df['제목'] = title
df['URL'] = url

df.head()
```


- 동적 - 브라우저 드라이브를 직접적으로 조작하는 method로 조작하여 추적
```python
!pip install selenium
```

- find_element(By.ID)
- find_element(By.CLASS_NAME)
- find_element(By.XPATH)
- find_element(By.CSS_SELECTOR)

사용하여 원하는 입력창이나 실행 버튼을 조작
- 클릭 : .click( )
- 키 입력: .send_keys( )

최근 google chrome driver 는 driver 경로 지정 없어도 된다고 하는데 확인 필요.

```python
from selenium import webdriver 
import from selenium.webdriver.common.keys import Keys 
from selenium.webdriver.common.by import By
import import time


target_url = 'https://www.google.co.kr/'
driver = webdriver.Chrome() ## <- 드라이버 경로
driver.get(target_url) 
time.sleep(3)


```

 이미지 다운로드 from urllib.request import urlretrieve

pyautogui

|Library|GitHub Stars|Key Features|License|
|---|---|---|---|
|Beautiful Soup|84|HTML/XML parsing  <br>Easy navigation  <br>Modifying parse tree|MIT|
|Requests|50.9K|Custom headers  <br>Session objects  <br>SSL/TLS verification|Apache 2.0|
|Scrapy|49.9K|Web crawling  <br>Selectors  <br>Built-in support for exporting data|3-Clause BSD|
|Selenium|28.6K|Web browser automation  <br>Supports major browsers,  <br>Record and replay actions|Apache 2.0|
|Playwright|58.3K|Automation for modern web apps  <br>Headless mode  <br>Network manipulation|Apache 2.0|
|Lxml|2.5K|High-performance XML processing  <br>XPath and XSLT support  <br>Full Unicode support|BSD|
|Urllib3|3.6K|HTTP client library  <br>Reusable components  <br>SSL/TLS verification|MIT|
|MechanicalSoup|4.5K|Beautiful Soup integration  <br>Browser-like web scraping  <br>Form submission|MIT|

- 1. [Beatiful Soup](https://www.crummy.com/software/BeautifulSoup/)
- 2. [Requests](https://requests.readthedocs.io/en/latest/)
- 3. [Requests](https://requests.readthedocs.io/en/latest/user/advanced/#proxies)
- 4. [Scrapy](https://docs.scrapy.org/en/latest/index.html)
- 5. [Selenium](https://www.selenium.dev/)
- 6. [Playwright](https://playwright.dev/)
- 7. [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)
- 8. [Urllib2](https://docs.python.org/2.7/library/urllib2.html)
- 9. [GitHub](https://github.com/urllib3/urllib3)
- 10. [MechanicalSoup](https://mechanicalsoup.readthedocs.io/en/stable/)
