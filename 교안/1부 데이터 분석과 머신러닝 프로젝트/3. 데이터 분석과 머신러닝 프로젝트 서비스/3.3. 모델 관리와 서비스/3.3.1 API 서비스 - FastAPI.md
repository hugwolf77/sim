---
categories: 
title: 3.3.1 API 서비스 - FastAPI
created: 2025-03-19
tags:
---
---
#### 3.3.1 API 서비스 - FastAPI
---

---
# *3. API - Request*
---
```python
!pip install lxml
!pip install html5lib
import pandas as pd 
tables = pd.read_html('https://astro.kasi.re.kr//life/pageView/5',encoding='utf-8') 
df_astro = tables[0]
print(df_astro.shape)
print(df_astro.info())
df_astro.head()

df_krx = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13',encoding='cp949')[0]
df_krx['종목코드'] = df_krx['종목코드'].map('{:06d}'.format)
df_krx.sort_values(by='종목코드')
df_krx.head(10)
```


- *API(Application Programming Interface)* 란?
	: 응용 프로그램에서 사용할 수 있도록, 운영 체제나 프로그래밍 언어가 제공하는 기능을 제어할 수 있게 만든 인터페이스 -위키백과

- python HTTP, API 관련 주요 라이브러리 : urllib , requests
	1) (Client) Request 측면
	2) (Server) 측면

- requests 라이브러리를 사용하면 직관적인 method로 요청을 
	https://pypi.org/project/requests/
	https://requests.readthedocs.io/en/latest/
	
	- GET 방식: `requests.get()`
	- POST 방식: `requests.post()`
	- PUT 방식: `requests.put()`
	- DELETE 방식: `requests.delete()`

- openAPI : system의 서비스를 공개적으로 연결해서 제공

- RPC(Remote Procedure Call)
	: 현재 실행 중인 프로세스의 주소공간 내부가 아닌, 외부의 프로세스 또는 원격지의 프로세스와 상호작용하기 위한 기능


- json 과 pandas 로 dict  형태 변형 통신

```python
import numpy as np
import pandas as pd
import json

df = pd.read_csv('./data/movie/movies.csv')   

with open('./data/csv_to_json.json', 'w', encoding='utf-8') as f:
    df.to_json(f, force_ascii=False, orient='columns')

ata = {'name': ['아이유', '김연아', '홍길동'], 
        'dept': ['CS', 'Math', 'Mgmt'],
        'age': [25, 29, 30]}

# dict => df 
df = pd.DataFrame(data)  

'''
name	dept	age
0	아이유	CS	25
1	김연아	Math	29
2	홍길동	Mgmt	30
'''   

with open('./data/student_json_column.json', 'w', encoding='utf-8') as f:
    df.to_json(f, force_ascii=False, orient='columns')


```

- urllib
```python

import numpy as np
import pandas as pd
import json
import urllib
# 서버 쪽에 request 를 보내야함. httprequest  라이브러리를 통해서 해야함 >> urllib

############ 2. 영화진흥위원회 Open API를 호출하기를 위한 url이 있어야함
movie_url = 'http://www.kobis.or.kr/kobisopenapi/webservice/rest/boxoffice/searchDailyBoxOfficeList.json'
# key 값과 targetDt 는 들어가야함. 이 api의 필수 조건임. 
query_string = '?key=<여기는 키값입니다>&targetDt=20210801'
# 그래서 url을 합쳐준다면...
last_url = movie_url + query_string

# 이 Url을 이용해서 서버프로그램을 호출
result_obj = urllib.request.urlopen(last_url) 
# response
print(result_obj) 

# 3. 가져온 json 데이터를 dictionary로 
result_json = result_obj.read()  
result_dict = json.loads(result_json)  
print(result_dict)

rank_list = []  # 빈 리스트를 만들자
title_list = []
sales_list = []

for tmp in result_dict['boxOfficeResult']['dailyBoxOfficeList']:
    rank_list.append(tmp['rank'])
    title_list.append(tmp['movieNm'])
    sales_list.append(tmp['salesAmt'])
# list 확인
print(rank_list)
print(title_list)
print(sales_list)

df = pd.DataFrame({
    'rank': rank_list,
    'title': title_list,
    'salesAmt': sales_list
})  

```


### (Server) FastAPI, SQLite3, Pydantic


- [FastAPI](https://fastapi.tiangolo.com/ko/)


```python
!pip install fastapi uvicorn
# !pip install sqlite
```

- *app.py*
```python
# app.py

from fastapi import FastAPI  
from db_conn import *
 

# create_table() # Call this function to create the table

# create API
app = FastAPI()  
  
@app.get("/")  
def read_root():  
	return {"message": "Welcome to the CRUD API"}

@app.post("/books/")  
def create_book_endpoint(book: BookCreate):  
	book_id = create_book(book)  
	return {"id": book_id, **book.dict()}


@app.post("/books/")  
def create_book(book: BookCreate):  
	# Logic to add the book to the database  
	return book

```

- *db_conn.py*
```python

import sqlite3
from dataclass import *

# create sqlite3 DB connect def   
def create_connection():  
	connection = sqlite3.connect("books.db")  
	return connection

def create_table():  
	connection = create_connection()  
	cursor = connection.cursor()  
	cursor.execute("""  
			CREATE TABLE IF NOT EXISTS books (  
			id INTEGER PRIMARY KEY AUTOINCREMENT,  
			title TEXT NOT NULL,  
			author TEXT NOT NULL  
			)  
		""")  
	connection.commit()  
	connection.close()  
	  
# create_table() # Call this function to create the table


# Insert data into DB use dataclass 
def create_book(book: BookCreate):  
	connection = create_connection()  
	cursor = connection.cursor()  
	cursor.execute(
				"INSERT INTO books (title, author) VALUES (?, ?)",
				(book.title, book.author)
				)  
	connection.commit()  
	connection.close()
```


- *dataclass.py*
```python

from pydantic import BaseModel  

# create dataclass 
class BookCreate(BaseModel):  
	title: str  
	author: str  
  
class Book(BookCreate):  
	id: int
```


```cmd
uvicorn app:app --reload
```

- Browser : http://127.0.0.1:8000/docs

- DB Browser for sqlite3
https://sqlitebrowser.org/dl/

 ### Sqlalchemy
- ORM:Object Relational Mapping :  객체지향 언어를 이용하여 호환되지 않는 type system 간의 데이터 전환을 의미. 관계형 데이터베이스에 객체지향 언어로 접근할 수 있게끔 매핑을 해주는 다리 역할.
	1)  장점
		- SQL을 별도로 익히지 않아도 DB를 활용.
		- DB를 변경할 때 쿼리를 하나하나 수정하지 않아도 됨.
		- SQL injection 를 방지할 수 있음.
	2) 단점
		- SQL을 아는 사람이라면 ORM을 또 별도로 배워야 함..
		- 복잡한 쿼리가 필요한 경우 성능 저하를 일으키거나 ORM으로 치환이 어려움.



>[!NOTE]
> - Pandas 의 DataFram 형태의 데이터에서 행 별로 dict 형태 또는 json 형태로 변환하는 프로세스 고려해야 함. 
> - 데이터 구조적인 활용 또는 효율성 등에 대해서 고려해야 함.
> - pydantic 과 같은 class 형태를 사용할 경우 효율성은 떨이 짐.  대신 검정과 활용이 쉬움.
> - Sqlalchemy 등을 사용할 경우 DB  연결과 관리가 쉬워짐. 대신 변환 등이 복잡해짐.



### (Client) requests

**OpenAPI 에 접근해서 Data 가져오기**


[**PublicDataReader**](https://github.com/WooilJeong/PublicDataReader) [**제작자분 블로그**](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwooiljeong.github.io%2Fpython%2Fpdr-ecos%2F)

```python

!pip install PublicDataReader #--upgrade

from PublicDataReader import Ecos

service_key = "개별 api-key  # Ecos에 접속할 수 있는 개별 발급 받은 API key 를 입력

api = Ecos(service_key)               # API 접속
df_list = api.get_statistic_table_list()   # 데이터 조회 - 통계자료에 대한 리스트 자료
df_list.head()

# 100대 주요 경제 통계지표
df_100_list = api.get_key_statistic_list()
df_100_list.head(5)

# 경제통계 용어 에 대한 정보도 조회 가능.
info = api.get_statistic_word(용어="소비자동향지수")
info
```

[ECOS API 개발 가이드](https://ecos.bok.or.kr/api/#/)



---
# *4. Scraper - BS4*
---
*Scraper* is a very simple (but limited) data mining extension for facilitating online research when you need to get data into spreadsheet form quickly.

https://pypi.org/project/beautifulsoup4/

_DOM_. Document Object Model의 약자로, HTML요소를 JavaScript Object처럼 조작할 수 있는 Model 

- 정적 - 링크나  post 등  또는 js 등의 동적 페이지 추적이 어려움.
```python
!pip install requests
!pip install bs4
```

```python
import requests
from bs4 import BeautifulSoup as bs
import pandas as pd

res = requests.get('https://finacnce.naver.com/')
html = res.text
soup = bs(html, 'html.parser')
articles = soup.select('.news_area a') # DOM 에 사용된 클라스 명으로 접근

title = []
url = []

for news in articles:
	title.append(news.text)
	url.append(news['href'])

df = pd.DataFrame()
df['제목'] = title
df['URL'] = url

df.head()
```


- 동적 - 브라우저 드라이브를 직접적으로 조작하는 method로 조작하여 추적
```python
!pip install selenium
```

- find_element(By.ID)
- find_element(By.CLASS_NAME)
- find_element(By.XPATH)
- find_element(By.CSS_SELECTOR)

사용하여 원하는 입력창이나 실행 버튼을 조작
- 클릭 : .click( )
- 키 입력: .send_keys( )

최근 google chrome driver 는 driver 경로 지정 없어도 된다고 하는데 확인 필요.

```python
from selenium import webdriver 
import from selenium.webdriver.common.keys import Keys 
from selenium.webdriver.common.by import By
import import time


target_url = 'https://www.google.co.kr/'
driver = webdriver.Chrome() ## <- 드라이버 경로
driver.get(target_url) 
time.sleep(3)


```

 이미지 다운로드 from urllib.request import urlretrieve

pyautogui

|Library|GitHub Stars|Key Features|License|
|---|---|---|---|
|Beautiful Soup|84|HTML/XML parsing  <br>Easy navigation  <br>Modifying parse tree|MIT|
|Requests|50.9K|Custom headers  <br>Session objects  <br>SSL/TLS verification|Apache 2.0|
|Scrapy|49.9K|Web crawling  <br>Selectors  <br>Built-in support for exporting data|3-Clause BSD|
|Selenium|28.6K|Web browser automation  <br>Supports major browsers,  <br>Record and replay actions|Apache 2.0|
|Playwright|58.3K|Automation for modern web apps  <br>Headless mode  <br>Network manipulation|Apache 2.0|
|Lxml|2.5K|High-performance XML processing  <br>XPath and XSLT support  <br>Full Unicode support|BSD|
|Urllib3|3.6K|HTTP client library  <br>Reusable components  <br>SSL/TLS verification|MIT|
|MechanicalSoup|4.5K|Beautiful Soup integration  <br>Browser-like web scraping  <br>Form submission|MIT|

- 1. [Beatiful Soup](https://www.crummy.com/software/BeautifulSoup/)
- 2. [Requests](https://requests.readthedocs.io/en/latest/)
- 3. [Requests](https://requests.readthedocs.io/en/latest/user/advanced/#proxies)
- 4. [Scrapy](https://docs.scrapy.org/en/latest/index.html)
- 5. [Selenium](https://www.selenium.dev/)
- 6. [Playwright](https://playwright.dev/)
- 7. [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)
- 8. [Urllib2](https://docs.python.org/2.7/library/urllib2.html)
- 9. [GitHub](https://github.com/urllib3/urllib3)
- 10. [MechanicalSoup](https://mechanicalsoup.readthedocs.io/en/stable/)
