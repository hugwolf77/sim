---
categories: 글쓰기
title: 5.1. 시계열 데이터 분석의 이해
created: 2024-10-29
tags:
  - 수업
  - 교재
  - 시계열
  - timeseries
---
---
### 5.1. 시계열 데이터 분석의 이해 
---
##### 1) 횡단 분석(연구)와 종단 분석(연구)
---
 : 연구하고자 또는 분석하고자 하는 현상이나 대상의 정보가 일정한 시간적 또는 공간적 범위에서 변하지 않고 관찰되는가?
 - 횡단 분석: 일정한 범위 안에서 관찰된 또는 수집된 데이터가 분석하고자 하는 인과관계 또는 상관 관계에서 변하지 않는 안정적인 상태이다.
 - 종단 분석: 안정적인 상태들의 일정한 주기 또는 일정하지 않은 주기로 변화하는 변동성을 인과관계 또는 상관관계에 고려하는 분석이다. 

##### 2) 시계열 데이터와 시계열 분석
---

(1) 시계열(Time-Series) 데이터
- 시계열(Time-Series) 데이터란 시간의 흐름에 따라 순차적으로(sequentially) 기록된 데이터
- 주로 일정 시점 간격에서 반복적으로 변수 또는 특성(feature)의 변화를 관찰하여 수집한 데이터
- 이러한 일정한 시점 간격을 **"빈도(frequency)"** 라고한다. (ex: 년, 월, 일, 시, 분, 초)
- 일정 빈도(frequency)가 아닌 시간 흐름에 따른 데이터도 존재 한다. 그러나 일반적으로 일정한 시점 간격의 데이터를 분석에 사용하는 것이 일반적이다.
- 경제 금융 데이터, 센터 데이터, 신호 데이터

(2) 시계열 분석 (Time-Series-Analysis)
	(2-1) 시계열 예측 (Time-Series-Forecasting):
		- 과거로부터 현재 시점까지 수집된 자료를 분석하여 미래를 예측. 
		- 특정 미래 시점 또는 미래 기간의 관심 변수에 대한 (확률-통계적) 예측하는 것이 시계열 분석의 목적이다.
	(2-2) 영향 요인 : 
		- 관심 변수 결과에 영향을 미치는 변수를 의미한다.
		- 횡적 회귀분석 모델에서와 같이 단일 변수(univariable) 요인, 또는 다중 변수(multivariable)로 예측 모형을 만들 수 있다.
		- 불규칙요인(irregular component) 
		- 계통요인(systematic component)
			 ​추세요인(trend component)
			 계절요인(seasonal component)
			 순환요인(cyclical component)
		- **시계열 분석에서 가장 중요한 영향 요인 중 하나는 관심 변수 자신의 과거 데이터이다.**
		- 그러나 이러한 요인은 보통 각 시간 계열 데이터 하나 마다 복합적으로 녹아 있다.
		- 이를 분해하는 것은 "시계열 분해"라고 한다.
	(2-3)  변화에 대한 이해 기준 : 
		- 시계열 자료를 통해 구성된 시간변화를 가지는 시스템, 또는 현상을 확률과정으로 모형화하여 이해하고 제어하고자 하는 목적
##### 3) 시계열 데이터의 다루기
---
(1) python libarary pandas 에서 시계열 인덱스 다루기

```python
import pandas as pd 

df = pd.DataFrame({'날짜': [
									   '2021-01-10 07:10:00',
									   '2021-02-15 08:20:30', 
									   '2021-03-20 09:30:00', 
									   '2021-04-25 10:40:30', 
									   '2021-05-27 11:50:00', 
									   '2021-06-21 12:00:30', 
									   '2021-07-01 13:10:00', 
									   '2021-08-16 14:50:30'
									   ]
								})
df.info()
df['날짜'] = pd.to_datetime.(df['날짜'], format='%Y-%m-%d %H:%M:%S', errors='raise')
df.info()
```

```python
df['날짜_date'] = df['날짜'].dt.date  # YYYY-MM-DD(문자) 
df['날짜_year'] = df['날짜'].dt.year  # 연(4자리숫자) 
df['날짜_month'] = df['날짜'].dt.month # 월(숫자) 
df['날짜_month_name'] = df['날짜'].dt.month_name() # 월(문자) 
df['날짜_day'] = df['날짜'].dt.day # 일(숫자) 
df['날짜_time'] = df['날짜'].dt.time # HH:MM:SS(문자) 
df['날짜_hour'] = df['날짜'].dt.hour # 시(숫자) 
df['날짜_minute'] = df['날짜'].dt.minute # 분(숫자) 
df['날짜_second'] = df['날짜'].dt.second # 초(숫자)
```

```python
df['날짜_quarter'] = df['날짜'].dt.quarter # 분기(숫자) 
df['날짜_weekday'] = df['날짜'].dt.weekday # 요일숫자(0-월, 1-화) (=dayofweek) 
df['날짜_weekofyear'] = df['날짜'].dt.weekofyear # 연 기준 몇주째(숫자) (=week) 
df['날짜_dayofyear'] = df['날짜'].dt.dayofyear # 연 기준 몇일째(숫자) 
df['날짜_days_in_month'] = df['날짜'].dt.days_in_month # 월 일수(숫자) (=daysinmonth) 
#df['날짜_weekday_name'] = df['날짜'].dt.weekday_name # 요일이름(문자) (=day_name())
```

```python
df['날짜_is_leap_year'] = df['날짜'].dt.is_leap_year # 윤년 여부 
df['날짜_is_month_start'] = df['날짜'].dt.is_month_start # 월 시작일 여부 
df['날짜_is_month_end'] = df['날짜'].dt.is_month_end # 월 마지막일 여부 
df['날짜_is_quarter_start'] = df['날짜'].dt.is_quarter_start # 분기 시작일 여부 
df['날짜_is_quarter_end'] = df['날짜'].dt.is_quarter_end # 분기 마지막일 여부 
df['날짜_is_year_start'] = df['날짜'].dt.is_year_start # 연 시작일 여부 
df['날짜_is_year_end'] = df['날짜'].dt.is_year_end # 연 마지막일 여부
```

```
%Y: Year, ex) 2019, 2020 
%m: Month as a zero-padded, ex) 01~12 
%d: Day of the month as a zero-padded ex) 01~31 
%H: Hour (24-hour clock) as a zero-padded ex) 01~23 
%M: Minute as a zero-padded ex) 00~59 
%S: Second as a zero-padded ex) 00~59 
ex) 2019-09-01 19:30:00 =(Directivs)=> %Y-%m-%d %H:%M:%S
```

```python
pd.date_range("2018-4-1", "2018-4-30")
pd.date_range("2018-4-1", periods=30)
```

```
-s: 초 
-T: 분 
-H: 시간 
-D: 일(day) 
-B: 주말이 아닌 평일 
-W: 주(일요일) 
-W-MON: 주(월요일) 
-M: 각 달(month)의 마지막 날 
-MS: 각 달의 첫날 
-BM: 주말이 아닌 평일 중에서 각 달의 마지막 날 
-BMS: 주말이 아닌 평일 중에서 각 달의 첫날 
-WOM-2THU: 각 달의 두번째 목요일 
-Q-JAN: 각 분기의 첫달의 마지막 날 
-Q-DEC: 각 분기의 마지막 달의 마지막 날 

#참고 - https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects 
사용시 - 
pd.date_range("2018-4-1", "2018-4-30",freq ='B')
```

- resample : 시간 간격을 재조정하는 리샘플링(resampling)
```python
# up resampling
ts = pd.Series(np.random.randn(100), index=pd.date_range( "2018-1-1", periods=100, freq="D")) ts.tail(20)
# down resampling
ts.resample('W').mean()
```

(2) 시계열 데이터의 주요 이슈
- 결측값 (missing value)
```python
# df is pandas DataFrame
import pandas as pd

df.isnull()
df.isnull().sum()
df.isnotnull().sum()
df.isna().sum()

df.dropna(axis=1) # 행, 열 방향을 정해주지 않으면 결측값 있는 모든 행 열 지움

df.fillna(0) # 특정값 채우기 여기서는 '0'
df.fillna(method='ffill') # 'ffill' or 'pad' 전진하면 앞쪽 결측값 채우기
df.fillna(method='bfill') # 'bfill' or 'backfill' 전진하면 위쪽 결측값을 채우기
df.fillna(method='ffill', limit=1) # 채우는 횟수에 한계두기

#df.fillna(df.mean(),df.where(pd.notnull(df), df.mean(), axis='columns'))
df.fillna(df.mean())
df.fillna(df.mean()['column-name'])
```

-  결측값을 보간하는 방법(interpolation of missing values)
	: 시계열데이터 선형 비례 결측값 보간(interpolate)
	
```python
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
from datetime import datetime

 datetxt = ['11/10/2024', '11/11/2024', '11/12/2024', '11/13/2024', '11/14/2024']
 dates = pd.to_datetime(datetxt)
 tsdata = Series([1, np.nan, np.nan, 10, 15], index=dates)

linear_itp = tsdata.interpolate() # 선형 보간 자동
time_itp = tsdata.interpolate(method='time') # 시간에 따른 빈도 고려 선형 보간

df = DataFrame({
				'C1': [5, 6, np.nan, np.nan,, np.nan 9, 12],
				'C2': [7, 8, 10, np.nan,, np.nan 20, 25]
				})
multi_itp = df.interpolate(method='values')
multi_itp = df.interpolate(method='values', limit=1)
multi_itp = df.interpolate(method='values', limit=1, limit_direction='backward')
```

>	- 다른 빈도 주기 (difference frequency cycle)
>	- 빅데이터에 대한 입력 변수 처리 : 차원의 저주(dimentional curse) 

##### 4) 자기상관성(Autocorrelation)
---
 (1) 자기상관성
 - 시계열 데이터도 일반 통계분석의 상관계수 분석과 같이 상관성을 분석한다.
 - 시계열 데이터(또는 순차 sequence)에서 시간에 따른 두 시점 간(시차 lag)의 상관관계를 뜻한다. (일반적으로 그 시점 이전 데이터 시점)
 - 시계열 데이터의 분석은 과거의 변수의 상태가 미래에 얼마나 영향이 있는지를 분석한다.
 - 이때 특정 시점에서 다른 시점들(과거) 간의 상관관계가 그시점을 예측하는 모형에 큰 영향을 주는 요인으로 작용할 수 있기 때문이다.
 
 (2) 자기상관계수
 - 이러한 자기 상관성을 나타내는 수치로 자기상관계수를 사용한다.
 - 범위는 일반적으로 -1 ~ 1의 값을 나타낸다.
 - 일반적으로 절대값이 1보다 작아야 한다.
 - 그리고 일반적으로 시차(order)가 멀어질수록 감소하는 경향을 보여야 한다.
 - 즉, 한 시점의 상태의 영향력은 언젠 가는 없어져야만 한다. 그렇지 않다는 것은 다른 영향력을 받아서 왜곡됐거나 스스로 증분하고 있다고 생각 할 수 있다.
 - 이는 모델의 분석을 어렵게 한다. 추세(trend)가 나타나는 이유는 다음 영향력이 오기전에 충분히 이전 영향력이 사라지지 않기 때문이다. 

$$
\begin{align}
	&r_{k}=\frac{\sum^{T}_{t=k+1}(y_{t}-\bar y)(y_{t-k}-\bar y)}{\sum^{T}_{t=1}(y_{t}-\bar y)^{2}}
\end{align}
$$

(2) ACF(Auto-Correlation Function: 자기상관함수), PACF (Partial ACF: 편 자기상관함수)
- 피어슨 상관계수 형태의 함수식으로 데이터의 자기상관성을 표현한 것 
- 보통 그래프의 패턴을 통해서 몇 시차 데이터까지 자기상관성을 나타낼 때 사용된다
- alpha : 그래프에서 표시되는 Bartlett 검정에서 95% 신뢰구간으로 추정한 표준편차
$$
\begin{align}
	ACF(k)&= \frac{cov(y_{t},y_{t+k})}{var(y_{t})}\\ \\ 
	       &=\frac{\sum^{T}_{t=k+1}(y_{t}-\bar y)(y_{t-k}-\bar y)}{\sum^{T}_{t=1}(y_{t}-\bar y)^{2}} \cdot \frac{N}{N - k}
\end{align}
$$

- PACF (Partial ACF: 편 자기상관함수)은 시차 간 사이의 다른 시점들에 대한 영향력을 배제하고 측정하고자 하는 방법이다.

$$
\begin{align}
	&PACF(k)=Corr(e_{t},e_{t-h})\\ \\ 
    &e_{t}= y_{t} - (\alpha_{1}y_{t-1}+\cdots+\alpha_{h-1}y_{t-h+1})  \\
	&e_{t-h}= y_{t-h} - (\beta_{1}y_{t-1}+\cdots+\beta_{h-1}y_{t-h+1}) 
\end{align}
$$
- AR(p) : p 차  AR모델이 있다고 할 때, h에 대해서 ACF 그래프가 지수적으로 감소한다. PACF는 반면 p 차 이후 상관성이 없는 것으로 나타나기 때문에 이를 이용하여 p를 확인할 수 있다.
- 이와 같은 방법으로 AR(p), MA(q), ARMA(p,q)에 대해서 적용할 수 있다.

| graph | AR(p)     | MA(q)     | ARMA(p,q) |
| ----- | --------- | --------- | --------- |
| ACF   | 지수 감소     | q 시차 후 절단 | 지수 감소     |
| PACF  | p 시차 후 절단 | 지수 감소     | 지수 감소     |

```python
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

fig = plot_acf(data, lags=n_lags, alpha=alpha)
plt.xlabel(f'Lag at 0 to {n_lags}')
plt.ylable(f"Lag at k's autocorrelation")
plt.show()

fig = plot_pacf(data, lags=n_lags, alpha=alpha)
plt.xlabel(f'Lag at 0 to {n_lags}')
plt.ylable(f"Lag at k's partial autocorrelation")
plt.show()

```

##### 5) 시계열 패턴 분해
---
(1) 추세(trend)
: 시간이 지남에 따라 데이터가 장기적으로 증가하거나 감소하는 경향을 의미
- 선형추세, 곡선형 추세, 패턴형 추세,무작위 추세 등 다양하게 나타남
- 시계열 데이터를 분해하는 기법과 그 파라메터에 의해서도 영향을 받음
: 과거 데이터 추세를 미래에 지속적으로 외삽하는 것은 새로운 변동성에 의해 추세가 변화할 때 위험성이 발생한다.
(2) 계절성(seasonality)
: 자연현상이나 인류문화 현상으로 특정한 빈도(frequency)로 나타나는 영향요인이 시계열 데이터에 영향을 주는 크기를 나타냄. 
(3) 주기성(cycle)
: 고정된 빈도가 아닌 형태로 (계절성 요인이 아닌) 자체적인 시간에 따른 변동성을 나타낸다. (계절성과 같이 고정된 빈도와 패턴을 보이지 않고 진폭도 일반적인 자연현상이나 정해진 규칙에 의해서 발생하는 것과 다르다.)
(4) 백색잡음(White Noise)
- 서로 독립으로 동일한 분포(i.i.d: independently and identically distributed)를 따르는 확률 변수들의  계열로  구성된 확률 과정으로 $\epsilon_{t} \sim WN(0,\sigma^{2}_{\epsilon})$ 

![[decompose_TS.png]]

- python libarary statsmodels의 tsa.seasonal.seasonal_decompose를 이용하여 시계열 분해 사용
```python
result=seasonal_decompose(data, model = 'additive' or 'multiplicative',
						  filt = None,
						  period = None,
						  two_sided = True or False,
						  extrapolate_trend = True or False, 
						  freq : ' ',
						   weighted : True or False )
```
- data : 분해할 시계열 데이터 셋, 보통 Pandas의 Series나 DataFrame 사용 
- model : 분해 모델 지정, 우리 말로는 가법과 승법으로 나눈다.

| obtion   | Additive Model                      | Multiplicative Model                            |
| -------- | ----------------------------------- | ----------------------------------------------- |
| 추세의 특징   | 시차에 따라 일정 비율로 변화                    | quadratic, exponetial 등의 비선형적 곡선 또는 지수적 변화      |
| 추세의 모양   | 선형적 상승과 하락                          | 비선형적 곡선의 상승과 하락                                 |
| 계절성의 모양  | frequency와 amplitude가 일정            | frequency와 amplitude가 비일정                       |
| 추정 model | $\hat y(t)=Trend+Seasonality+Noise$ | $\hat y(t)=Trend\times Seasonality\times Noise$ |
|          |                                     |                                                 |

- filt : 추세 추정에 사용되는 필터, 기본값은 None
	hp = Hodrick-Prescott Filter : 장기적인 추세를 부드럽게 만듬
	convolution = constrained Linear LeastSquares Filter : 특정한 제약조건을 적용하는 필터
- period :  계절성 주기의 길이를 지정. (default = None : 자동으로 지정)
- two-side : 분해의 filter 적용을 양방향으로 진행 할지 지정
- extrapolate_trend: 외삽의 형태로 데이터의 시작과 끝지점에서의 추세 추정함.
- freq : 시계열 데이터의 시계열 인덱스 확인
- weighted : 가중치를 적용한다.


```python
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt

start_date = '2020-01-01'
end_date = '2024-11-20'

# 애플 주식의 데이터를 불러오기
aapl = yf.download('AAPL', start=start_date, end=end_date)
# 데이터 확인
aapl.head()

# 종가만 남기고 새로운 df에 정리
df = pd.DataFrame()
df['Close'] = aapl['Close']
# 시계열 인덱스 정리
df.index = pd.to_datetime(df.index).strftime('%Y-%m-%d')
df.info()

# 데이터 시각화
plt.figure(figsize=(8, 3))
plt.plot(df, color='navy', linewidth=1)
plt.title('AAPL Stock Close Price Over Time')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.xticks(np.arange(0,1230,50), rotation=90,fontsize=5)
plt.yticks(fontsize=9)
plt.grid(True)
plt.show()
```

![[aapl_plot.png]]

``` python
ts_decompose = seasonal_decompose(df, model='addictive', period=30, extrapolate_trend=True)
ts_decompose.trend

  # 결과 시각화
plt.figure(figsize=(12, 10))
plt.subplot(4, 1, 1)
plt.plot(ts_decompose.observed, label='Original', color='navy')
plt.legend(loc='upper left',fontsize=9)
plt.xticks(np.arange(0,1230,50), rotation=90,fontsize=5)
plt.yticks(fontsize=7)
# 추세
plt.subplot(4, 1, 2)
plt.plot(ts_decompose.trend, label='Trend', color='navy')
plt.legend(loc='upper left',fontsize=9)
plt.xticks(np.arange(0,1230,50), rotation=90,fontsize=5)
plt.yticks(fontsize=7)
# 계절성 
plt.subplot(4, 1, 3)
plt.plot(ts_decompose.seasonal, label='Seasonal', color='navy')
plt.legend(loc='upper left',fontsize=9)
plt.xticks(np.arange(0,1230,50), rotation=90,fontsize=5)
plt.yticks(fontsize=7)
# 잔차
plt.subplot(4, 1, 4)
plt.plot(ts_decompose.resid, label='Residual', color='navy')
plt.legend(loc='upper left',fontsize=9)
plt.xticks(np.arange(0,1230,50), rotation=90,fontsize=5)
plt.yticks(fontsize=7)
plt.tight_layout()
plt.show()
```

![[aapl_close_decompose.png]]

- 기타 미국 인구 조사국 (the US Census Bureau)과 캐나타 통계청이 만든 X11 기법.
- 스페인 은행에서 개발한 SEATS(Seasonal Extraction in ARIMA Time Series(ARIMA 시계열에서 계절성 추출))
- STL(Seasonal and Trend decomposition using Loess(Loess를 사용한 계절성과 추세 분해)) 등 이 있다.
- 현재 우리나라 한국은행은 미국 Census에서 사용하는 X11-SEATS을 사용하고 있다. 미국은 X12-SEATS를 사용하고 있다.

##### 6) 정상성 (Stationarity)
---
(1) 정상성
- 데이터의 변화에 대해서 선형적인 상관관계를 파악해야 하지만 데이터의 변화가 다른 외부적인 시간의 변화에 따른 영향을 받는 패턴이 존재하거나
- 시간에 따라 스스로 증분해 가는 비선형적인 성장 추세를 가지고 있다면
- 데이터 분석에서 각 시점의 상관관계와 인과관계를 분석하는데 왜곡을 가져 올 수 있다.
- 대표적인 비정상성 영향 요인은 위에 두가지 이사, 추세(trend), 계절성(seasonality) 
- 따라서 변화의 폭이 랜덤하지만 일정한 범위 내에서 변화하여야 한다. 즉, 데이터의 시간에 따른 분포가 일정한 평균과 분산, 자기 상관관계를 나타내야 한다는 뜻이다.

(1-1) 강-정상성 조건과 약-정상성 조건
	- 강정상성(strict stationarity): 시간이 지나도 시계열 데이터의 통계적 분포 특징이 변하지 않음. 확률적 관점에서는 확률변수의 결합 분포가 변하지 않는 것을 의미(확률과정을 나타내는 일정 구간마다의 확률분포가 변하지 않는다는 의미) **실제 데이터에서 존재하기 어려움**
		- 일차원 분포의 시간 불변성: 시계열의 임의의 시점에서 관측된 값의 확률 분포가 시간에 관계없이 항상 동일함.
		- 다차원 결합 분포의 시간 불변성: 시계열의 임의의 두 시점에서 관측된 값들의 결합 확률 분포가 시간에 관계없이 항상 동일해야 함. 
	- 약정상성(weak stationality): 기대값 또는 평균과 공분산, 그리고 자기상관성이 일정하게 유지됨. 데이터의 분포에 대해서는 상관하지 않음.  **실제 데이터 분석에서 사용함**

(2) 정상성을 테스트 하는 단위근 검정 
- ADF test (Augmented Dickey-Fuller test)
- KPSS test (Kwiatkowski-Phillips-Schmidt-Shin test)

```python 
# 출처 https://signature95.tistory.com/22
from statsmodels.tsa.stattools import adfuller, kpss

# 함수 형성
def adf_test(timeseries, pvalue = .05, regression_option = 'ct'):
	print ('Results of Dickey-Fuller Test:')
	dftest = adfuller(timeseries, autolag='AIC', regression = regression_option)
	dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','Lags Used','Number of Observations Used'])
	for key,value in dftest[4].items():
		dfoutput['Critical Value (%s)'%key] = value
	print (dfoutput)
	if dfoutput[1] < pvalue:
		print(f"정상시계열이 아니라는 귀무가설을 {pvalue*100}%의 유의수준으로 기각할 수 있으므로 해당 데이터는 정상성이 보장됩니다.")
	else:
		print(f"정상시계열이 아니라는 귀무가설을 {pvalue*100}%의 유의수준으로 기각할 수 없으므로 해당 데이터는 정상성을 보장하지 못합니다.")

def kpss_test(timeseries, pvalue = .05, regression_option = 'ct'):
	print ('Results of KPSS Test:')
	kpsstest = kpss(timeseries, regression= regression_option)
	kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','Lags Used'])
	for key,value in kpsstest[3].items():
		kpss_output['Critical Value (%s)'%key] = value
	print (kpss_output)
	if kpss_output[1] < pvalue:
		print(f"정상시계열이 맞다는 귀무가설을 {pvalue*100}%의 유의수준으로 기각할 수 있으므로 해당 데이터는 정상성을 보장하지 못합니다.")
	else:
		print(f"정상시계열이 맞다는 귀무가설을 {pvalue*100}%의 유의수준으로 기각할 수 없으므로 해당 데이터는 정상성이 보장됩니다.")

```

```python
adf_test(ts_decompose.observed)
kpss_test(ts_decompose.observed)
```

##### 7) 차분(differencing)
---
- 현재 시점의 값에서 이전 시점의 값을 빼서 새로운 시계열을 생성하는 것으로 데이터의 변동량을 구하는 방식이다. 즉, 직접적인 원 시계열의 정보에서 변동량의 정보를 구하는 방법.
$$
\begin{align}
	&differencing\ 1 :\quad y_{t}' = y_{t} - y_{t-1} \\
	&differencing\ 2 :\quad y_{t}'' = y_{t}' - y_{t-1}' = (y_{t}-y_{t-1}) - (y_{t-1}-y_{t-2}) = y_{t} - 2y_{t-1}+y_{t-2} \\
	&log\ differencing\ 1 :\quad y_{t}' = ln(y_{t}) - ln(y_{t-1}) \\
\end{align}
$$- 목적:
	- 정상성 확보: 원 시계열의 비정상적인 요소를 제거하고 시점 간 변동량을 사용함으로써 정상성 확보.
	- 추세 제거: 대부분 비정상성 데이터가 가진 추세를 제거하는데 사용됨.

- 종류
	- reqular 차분 : 바로 전시점의 자료를 빼서 차이를 사용 (추세 제거)
	- seasonal 차분:  특정 주기 이상 떨어진 시점의 자료와의 차이 이용 ( 예:전년 동기 대비 ) (계절성 제거)
	- log 차분: 원 데이터에 로그를 취한 데이터를 차분하는 방법으로 데이터의 상대적 변화율 포착하기 때문에 변동성이 큰 데이터에 주로 사용한다.
- 적분 차수(Order of integration) : 정상성 시계열 데이터를 얻기 위해 필요한 차분 횟수를 뜻한다. 일반적으로 2차분 이상 진행하지 않는다.
- 주의 
	- 차분 과정에서 원래 데이터의 일부 정보가 손실되어 원데이터의 전체 구간의 대한 정보가 점점 단기의 순간 정보만 남게 된다. 
	- 따라서 차분의 차 수는 유의해서 진행하여야 한다. 보통 경제 데이터의 경우 2차분 이상 진행하지 않는다.

```python
df['diff_1_Close'] = df['Close'] - df['Close'].shift(1) # 판다스의 shift메소드를 이용해 차분하는 방법
df['diff_1_Close'] = df['Close'].diff() # 판다스의 diff 메소드를 이용해 차분하는 방법`
```

##### 8) 평활화법
---
 - 시간의 관측 주기에 따른 작은 노이즈와 비정상성을 제거하기 위해서 변화의 폭을 일반화하는 방법이다.
 - 이동평균법 : 일정한 주기 간의 기간을 들로 전체 시계열을 평균해 나가는 방법이다. 이를 통해 정해진 간격의 값들의 변화를 안정시킴 (이동평균법에도 여러 방법이 있지만 일반적으로 쉽게 산술적 평균을 이동해 나가는 것을 주로 사용한다.)

```python
# rolling method를 이용한 이동평균 평활하
# monthly
temp['window_month'] = temp['Temp'].rolling(window = 30, min_periods=1).mean()
# quaterly
temp['window_quarter'] = temp['Temp'].rolling(window = 120, min_periods=1).mean()
# yearly
temp['window_year'] = temp['Temp'].rolling(window = 365, min_periods=1).mean()
```


 - 지수평활법 : 시간의 변화에 따른 지수적인 비율의 가중치를 가하여 변화율을 약화시키는 방법

```python
## exponential smoothing in Python
from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt

# Simple Exponential Smoothing
fit1 = SimpleExpSmoothing(df_train, initialization_method="estimated").fit()
# Trend
fit2 = Holt(df_train, initialization_method="estimated").fit()
# Exponential trend
fit3 = Holt(df_train, exponential=True, initialization_method="estimated").fit()
# Additive damped trend
fit4 = Holt(df_train, damped_trend=True, initialization_method="estimated").fit()
# Multiplicative damped trend
fit5 = Holt(df_train, exponential=True, damped_trend=True, initialization_method="estimated").fit()

```

 - 평활화법을 사용하면 일정한 추세의 경향성을 파악할 수 있는 경우가 많다. 큰 비선형적인 패턴이이 존재하지 않는다면 상당히 예측의 경향성을 파악할 수 있다.

##### 9) 시계열 모델 분석의 기본
---
(1) 데이터의 분할
	- 일반적인 머신러닝의 데이터 분할과 같이 학습 데이터 구간 80% 와 테스트 구간 20%를 기준으로 기간에 따라서 (보통 훈련을 과거 테스트를 최근의 기간으로 정함) 나누워 주지만 데이터 상황에 따라서 조정 가능하고 절대적인 규칙은 아니다.
	- 딥러닝의 경우 테스트와 별도로 검증 구간을 두어야 하기 때문에 데이터의 분할 시 훈련 구간과 테스트 구간의 비율에 맞춰 (보통 테스트 구간과 유사한 비율로) 검증 구간을 별도로 분할한다. 보통은 훈련 구간을 60% 검증 구간을 20% 테스트 구간을 20% 정도 가져 가지만 데이터가 충분한 경우이다. 
	- 주의 해야할 점은 시간의 변화에 따라서 데이터의 분포와 입력변수 간 관계 패턴이 변할 수 있다는 점을 고려하여야 한다.

(2) 데이터의 입력 간격 크기(WIndow)와 입력 기법에 따른 분류
	- **Window** 
		: 시계열에서 일반적으로 입력되는 한 계열의 데이터의 기간의 구간을 말함
	- AR - OLS 관점 : 자기상관함수(ACF) 관점에서 입력 기간 고려
	- 다변량일 경우 정확한 인과 관계를 파악하기 힘듬.
	- 입력 기간의 생성
		-  입력되는 기간을 생성하는 방법은 일반적으로 sliding으로 입력 시점의 window를 전진시키며 입력한다.
		- 딥러닝 모델의 경우, 의도적으로 일정 구간들로 이러한 window 들을 생성하여 이를 shuffle하기도 한다.
		- 교차검증(cross-validation)의 기법을 적용하는데 있어서 nested 방식을 정석으로 이야기 하지만 정확하게 기존 cv와 같은 효과라고 이야기 하기는 어렵다.
		
(3) 예측 시점과 크기에 따른 분류와 기법
	- 일반적인 예측은 미래의 한 시점을 예측하는 것이 기본이지만 시계열의 관성을 고려할 때 한 시점만을 예측하는 경우 follow step하여 의미 있는 예측성을 확보하기 어렵다.
	- 따라서 일반적으로 미래 다중 시점이나 구간을 예측하는 것이 일반적이다. 이에 따라서 예측하는 기법이 달라지고 이는 입력 데이터에 대한 학습 라벨 입력이 달라지게 된다.
	- Rolling window forecasting
		- 예측 시점(보통 한 시점)을 새로운 미래 입력으로 사용하여 그 다음 예측 시점을 연쇄적으로 예측하는 방법
		- 각 시점의 입력 변수의 변화에 민감하게 미래 예측 기간의 변화를 예측할 수 있다.
		- 그러나 시간적인 앞 시점의 예측 오류가 지속적으로 누적되어 결과적으로 앞 시점의 예측력에 따라서 뒤에 예측 시점들의 오차가 누적 증가 하게 된다.
	- Direct forecasting
		- 한번의 예측하고자 하는 모든 미래 시점을 예측하는 방법
		- 순간적인 변동성에 반응하는 민감도는 떨어지게 된다.
		- 반면 일정 예측 기간에 대한 추세를 예측하는 것은 안정적이게 된다.
	- 복합적인 방법을 사용할 수도 있다.

##### 10) 시계열 데이터의 통계적 선형 모델의 기초
----

> -  확률과정( = 무작위 행보: random walk) : 시간에 따른 편차의 평균이 0이지만 분산은 시간에 비례하여 증가하게 된다. 따라서, 앞뒤로 움직일 확률이 동일하다고 해도 시간이 흐름에 따라 평균에서 점차 벗어나는 경향. (브라운 운동,)

(1) MA 모델 (Moving Average model: 이동평균 모델)

$$Y_{t}=\alpha_{t}-\theta_{1}\alpha_{t-1}-\theta_{2}\alpha_{t-2}-\cdots-\theta_{p}\alpha_{t-p}$$

(2) AR  모델 (Autoregressive model : 자기회귀 모델)

$$ Y_{t} = \phi_{0} + \phi_{1}Y_{t-1} + \phi_{2}Y_{t-2}+ \cdots + \phi_{p}Y_{t-p} + \epsilon_{t}$$

- 일반화 형태

$$
\begin{align}
	y_{t} 
	&= ay_{t-1} + \epsilon_{t-1}\\
	&=a(ay_{y-2}+\epsilon_{t-2})+\epsilon_{t-1} \\
	&=a^{2}y_{t-2}+a\epsilon_{t-2}+\epsilon_{t-1} \\
	& \vdots \\
	&=a^{n}y_{t-n}+\sum \epsilon_{t-i}a^{i} 
	
\end{align}

$$


| model                     | contents                                        |
| ------------------------- | ----------------------------------------------- |
| AR(Auto-regressive)/AR(p) | AR(p) model p개의 과거 시차 값들을 이용해 예측                |
| MA(Moving average)/MA(q)  | MA(q) model q개의 과거 오차 값들을 이용해 예측                |
| ARMA(p,q)                 | ARMA(p,q) model p개의 과거 값과 q개의 과거 오차 값들을 이용하여 예측 |
| ARIMA(p,d,q)              | 데이터를 d회 차분하고 pr개의 과거 값과 q개의 과거 오차 값들을 이용하여 예측   |

###### 11) 잔차분석
---
(1) 회귀분석의 잔차분석
- 회귀모형에 대한 정규성, 등분산성, 독립성 등을 충족하는지에 대한 검정
- 잔차를 통해서 분석모형으로 표현하지 못한 데이터에 남아 있는 영향요인 관계가 있는지 검사 (잔차의 규칙적 패턴이나 추세 존재 여부)
- 이상치 개입에 의한 영향 발생 등을 검정

```python
import numpy as np
import pandas as pd
from statsmodels.formula.api import ols
from statsmodels.stats.outliers_influence import OLSInfluence
from statsmodels.stats.outliers_influence import variance_inflation_factor
import scipy
import scipy.stats
import matplotlib.pyplot as plt 
import seaborn as sns 

df = pd.read_csv('data_path')
model = ols('formula' , data=df).fit() # ols
model = ols.from_formula("formula", data=df)

# ols
fitted = model.predict(df) 
residual = df['dist'] - fitted # 잔차 계산

# 모형의 선형성을 확인하기 위한 기본 시각화
sns.regplot(fitted, residual, lowess=True, line_kws={'color': 'red'}) 
plt.plot([fitted.min(), fitted.max()], [0, 0], '--', color='grey')

# Q-Q plot for normal distribution residual
sr = scipy.stats.zscore(residual)
(x,y),_ = scipy.stats.probplot(sr)
sns.scatterplot(x,y)
plt.plot([-3,3],[-3,3],'--', color='grey') # 대각 기준선, 범위는 필요에 따라서

# test shapiro
test_shapiro = scipy.stats.shapiro(residual)

# outliers
cd,_ = OLSInfluence(model).cooks_distance
cd.sort_values(ascending=False).head()

# VIF : 다중공선성 검정
# vif = pd.DataFrame() 
# vif["VIF Factor"] = [variance_inflation_factor(df.values, i) for i in range(dfX.shape[1])] 
# vif["features"] = dfX.columns vif = vif.sort_values("VIF Factor").reset_index(drop=True) 
# vif

```

(2) 시계열 잔차진단
: 시계열 모델에 사용된 시계열 데이터에서 각 시점의 데이터는 그 이전 시점의 관측치를 이용하여 예측할 수 있는 정도를 적합값(fitted values)이라 한다.
$$
\begin{align}
&y_{t} \sim f(y_{1},\dots,y_{t-1}) = \hat y_{t|t-1} = \hat y_{t}
&
\end{align}
$$
- [잔차 (Residuals)]
	: 시계열 모델의 관측값(입력값)에 대응하는 적합값(fitted value)과 진짜 관측값(Ground Trues)와 차이를 말한다.
$$
\begin{align}
&e_{t} = y_{t} - \hat y_{t} 
\end{align}
$$
- 좋은 모델의 잔차의 특징
	a. 잔차 사이에 상관 관계가 없음(상관관계가 남아 있다면, 모델에서 데이터에 대해서 반영하지 못한 정보가 남아 있다는 뜻일 수 있음)
	b. 잔차의 평균이 0 이다. (잔차의 평균이 0이 아니라면 예측값에 편향(bias)있다는 뜻일 수 있다.)
	* 일반적으로 편향 문제는 대처가 용이하지만 상관관계 문제는 수정이 어렵다.
	c. 잔차의 분포가 정규 분포를 따른다. (또한 잔차의 분산이 상수이면 예측의 구간(prediction interval)을 얻을 수 있다. 그렇지 못하다면 신뢰구간을 구하기 어렵다.)
	
- [Ljung-Box test] 융-박스 검정 (portmanteau 검정)
 : 모델에서 사용한 데이터가 T 개의 관측값 개수이고, 고려할 최대 시차(lagged value)가 h 일 때 각 잔차에 대한 자기상관관계(autocorrelation) $r_{k}$ 에 대해 $Q^{\ast}$는 작아질 것이고 백색잡음(white noise)에서 나오지 않은 잔차의 자기상관관계는 큰 $Q^{\ast}$ 를 보일 것이다.   
$$
\begin{align}
	&Q^{\ast} = T(T+2)\sum_{k=1}^{h}(T-k)^{-1}r^{2}_{k}
\end{align}
$$
: $Q^{\ast}$ 의 결정 기준은 잔차의 자기상관관계(autocorrelation)가 백색잡음(white noise)에서 온 것이라면 모델의 매개변수 개수 K에 대해서 (h - K) 자유도(degree of freedom)을 갖는 $\chi^{2}$ 분포를 따르게 된다는 것을 이용한다..  (보통 OLS 의 단변량 에서는 매개 k = 0으로 둠)

```python 
import statsmodels.api as sm
sm.stats.acorr_ljunbox(model_results.resid, lags=[20], return_df=True)
```

: 융-박스 검정의 가정
	- 귀무가설($H_{0}$) : 잔차는 백색 잡음이다. (잔차들 간에 자기상관 관계가 없다.)
	- 대립가설($H_{1}$) : 잔차는 백색 잡음이 아니다. (잔차들 간에 자기상관 관계가 존재한다.)
: 융-박스 검정 기준
	- p-value: 일반적 기준인 0.05 또는 0.01
	- p-value 가 기준보다 커서 귀무가설을 선택하여야 잔차가 백색잡음에서 온것이 된다.

- 잔차의 정규분포 검정이나 ACF 검정을 통해서 상관관계를 분석

##### 12) 예측 정확도 평가
---
(1) 평균 절대 오차 (Mean absolute error) MAE = $mean(|e_{t}|)$
(2) 제곱근 평균 제곱 오차 (Root Mean squared error) RMSE = $\sqrt{mean(e^{2}_{t})}$  
(3) 평균 절대 백분율 오차 (Mean absolute percentage error) MAPE = $mean(|percentage\ error_{t}|)$



###### 13) 단변량 시계열 선형 모델 
----
-  ARIMA 모델 (Autoregresive integrated moving average model: 자기회귀누적이동평균 모델)
$$ 
\begin{align}
	&Y_{t} = \phi_{0} + \phi_{1}Y_{t-1} + \phi_{2}Y_{t-2}+ \cdots + \phi_{p}Y_{t-p} + \epsilon_{t} + \epsilon_{t}-\theta_{1}\epsilon_{t-1}-\theta_{2}\epsilon_{t-2}-\cdots-\theta_{p}\epsilon_{t-p} \\ \\ 
	&y_{t}=c + a_{t} + \sum_{i=1}^{p}\phi_{i}y_{t-1} + \sum_{i=1}^{q}\theta_{i}a
	_{t-i} 
\end{align}
$$

```python

import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns 
import scipy.stats 
import os 
import warnings 
warnings.filterwarnings("ignore") 

## time series package 
import statsmodels.graphics.tsaplots as sgt 
import statsmodels.tsa.stattools as sts 
from statsmodels.tsa.seasonal import seasonal_decompose 
from statsmodels.tsa.arima_model import ARMA 
from scipy.stats.distributions import chi2 

sns.set() 
org_path = "--/Time series" 
os.chdir(org_path) 
## Data loading 
raw_csv_data= pd.read_csv("snp_index.csv") 
df = raw_csv_data[["Date","Close"]].copy() 

## from text to date 
df.Date = pd.to_datetime(df.Date, dayfirst = False) 
## setting index 
df.set_index("Date", inplace = True) 
df=df.asfreq(freq="B") 
## fillna 
df=df.fillna(method='ffill') 
## Splitting the data 
size = int(len(df)*0.8) 
df_train = df.iloc[:size] 
df_test = df.iloc[size:]


def LLR_test(mod_1, mod_2, DF=1): 
	L1 = mod_1.llf 
	L2 = mod_2.llf 
	LR = (2*(L2-L1)) 
	p = chi2.sf(LR, DF).round(3) 
	return p

## additive - y(t) = Level + Trend + Seasonality + Noise 
decomposition =seasonal_decompose(df["Close"], model = "additive", period =1) 
fig = decomposition.plot() 
fig.set_size_inches(10,10) 
plt.show() 
## ACF, PACF 
fig = plt.figure() 
ax1 = fig.add_subplot(2, 1, 1) 
ax2 = fig.add_subplot(2, 1, 2) 
sgt.plot_acf(df['Close'], lags = 20, zero = False, ax=ax1) 
ax1.set_title("ACF S&P") 
sgt.plot_pacf(df['Close'], lags = 20, zero = False, method = ('ols'), ax=ax2) ax2.set_title("PACF S&P") 
plt.show() 

## ADF 
sts.adfuller(df["Close"])

df["diff_Close"]=df["Close"].diff() 

## ACF, PACF 
fig = plt.figure() 
ax1 = fig.add_subplot(2, 1, 1) 
ax2 = fig.add_subplot(2, 1, 2) 
sgt.plot_acf(df["diff_Close"].iloc[1:], lags = 20, zero = False, ax=ax1) 
ax1.set_title("ACF S&P Diff") 
sgt.plot_pacf(df["diff_Close"].iloc[1:], lags = 20, zero = False, method = ('ols'), ax=ax2) 
ax2.set_title("PACF S&P Diff") 
plt.show() 

## ADF 
sts.adfuller(df["diff_Close"].iloc[1:]) 

## lag
model_ret_ar_1_ma_1 = ARMA(df["diff_Close"][1:], order=(1,1)) results_ret_ar_1_ma_1 = model_ret_ar_1_ma_1.fit(maxiter=100) results_ret_ar_1_ma_1.summary()

## Higher lag 
model_ret_ar_5_ma_5 = ARMA(df["diff_Close"][1:], order=(5,5)) results_ret_ar_5_ma_5 = model_ret_ar_5_ma_5.fit(maxiter=100) results_ret_ar_5_ma_5.summary()

## LLR Test 
LLR_test(results_ret_ar_1_ma_1 ,results_ret_ar_5_ma_5, DF=8) 
```

```python
##Grid search Finding Best Model 
import itertools 
p = range(0,7) 
q = range(0,6) 
pq = list(itertools.product(p,q))

dict_model ={} 
for i in pq: 
	try: 
		model = ARMA(df["diff_Close"][1:], order=(i)) 
		print(i) 
		model_fit = model.fit(maxiter = 50) 
		# print("\n ARMA:", i, "\tLL = ", model_fit.llf, "\tAIC = ", model_fit.aic)
		dict_model[i]=[model_fit.llf, model_fit.aic] 
	except: 
		print("error lag:",i) 
information=pd.DataFrame.from_dict(dict_model, orient ="index", columns =["llf", "AIC"]) 

information.loc[information["llf"] == information["llf"].max()]

information.loc[information["AIC"] == information["AIC"].min()]

model_ret_ar_6_ma_5 = ARMA(df["diff_Close"][1:], order=(6,5)) results_ret_ar_6_ma_5 = model_ret_ar_6_ma_5.fit(maxiter= 100) results_ret_ar_6_ma_5.summary() 
LLR_test(results_ret_ar_5_ma_5 ,results_ret_ar_6_ma_5, DF=1)

df['res_ar_5_ma_5'] = results_ret_ar_5_ma_5.resid 
sgt.plot_acf(df['res_ar_5_ma_5'] [1:], zero = False, lags = 20) 
plt.title("ACF Of Residuals for ARMA(5,5)", size=20) 
plt.show()
```


- ARCH (AutoRegressive Conditional Heteroskedasticity)

- GARCH (Generalized AutoRegressive Conditional Heteroskedasticity)


##### 14) 모델의 설명변수의 관계성 : 다변량 분석을 위한 변수 간 관계
---
###### (1) 공적분 (Cointegration)
---
Time Series data에서 사용되는 Cointegration analysis

- **허구적 회귀**(spurious regression) : 시계열 데이터에서 불안정적인(정상성이 없는) 경우 두 시계열 변수 사이에 아무런 관계가 없다고 하더라도 산점도(scatter plot)에서 볼 때는 상관관계가 있는 것처럼 표현되어 지는 경우가 있음. 즉, 두 계열 사이에 아무런 관련성이 없어도 회귀모형을 추정하면 유의미한 관계가 있는 것처럼 나타날 수 있다는 뜻이다.

- 시계열의 적분 차수가 모두  d일때, 시계열의 선형 결합의 적분 차수가 d보다 작을 때 시계열 사이에 공적분 관계가 존재한다고 한다.

> - 두 시계열 사이에 공적분 관계가 있다는 것은, 단기적으로 다를 수도 있지만 장기적으로 보았을 때는 서로 일정한 관계가 있다는 것을 의미한다.
> - 쉬운 예로는 술취한 사림이 반려경을 끈에 묶어서 돌아다니는 것과 같이 사람과 반련견은 아무런 관계 없이 서로 움직이지만, 서로 멀어지지 않고 결국엔 집으로 가는 것과 비슷하다.

- 공적분을 파악하는 이유
	(1)  **장기적인 관계 파악:** 공적분 분석을 통해 시계열 변수들 간의 장기적인 관계를 파악할 수 있습니다. 이는 투자, 경제 정책 수립 등 다양한 분야에서 중요한 정보를 제공합니다.
	(2) **예측 모델 개선:** 공적분 관계가 있는 변수들을 함께 모델링하면 예측 모델의 정확도를 높일 수 있습니다.
	(3) **변수 선택:** 다변량 시계열 분석에서 공적분 분석을 통해 모델에 포함할 변수를 선정하는 데 도움을 받을 수 있습니다.

 $\Longrightarrow$ VAR 모델에 공적분 관계가 있는 시계열 데이터를 차분하여 사용하면 과잉차분의 위험이 있음.

- **오차 수정 모델 (ECM):** 공적분 관계가 존재한다면, 단기적인 불일치를 수정하는 오차 수정 모델을 설정할 수 있다.

	> - 주식투자 등에서는 "pair trading" 이라 하여 서로 다른 종목 간에 일시적으로 불일 치를 보이지만 장기적으로 같은 추세를 나타내는 종목을 이용하여 두 종목 간의 스프레드(가격 차이 간격)을 계산합니다. 
	> - 평균 회귀 전략 사용.
    - 스프레드가 평균보다 크게 벌어졌을 때: 상대적으로 높은 가격의 자산을 매도하고, 낮은 가격의 자산을 매수.
    - 스프레드가 평균보다 작게 좁혀졌을 때: 상대적으로 낮은 가격의 자산을 매도하고, 높은 가격의 자산을 매수.

<span style="color:yellow">
### python statsmodels 를 통한 공적분 분석
</span>
```python 
statsmodels.tsa.stattools.coint(
	y0,
	y1,
	trend = 'c',
	method = 'aeg', # only one  method "augmented Engle-Granger"
	maxlag = None,
	autolag = 'aic',
	return_results = None #  “AIC” (default) or “BIC”,
	)
	
```
: statsmodels 의 coint는 Engle-Granger 이론을 기반으로 하고 있다.
1) 두 시계열 $X_{t}, Y_{t}$ 이 공적분 관계라고 가정할 때, $Y_{t}$ 를 종속변수, $X_{t}$를 독립변수로 회귀분석을 하여 만들어지는 잔차 $Z_{t}$에 대하여 다음과 같은 식이 성립한다. 이러한 식을 ECM식이라고 한다.
2) 여기서 $e_{t}$는 회귀분석으로 만들어진 ECM 형식의 잔차이raview다.
$$
\Delta y_{t} \; = \;
\gamma_{1}Z_{t-1} \; + \;
\sum_{i=1}^{K}\Psi_{1,i}\Delta X_{t-i}\; + \;
\sum_{i=1}^{L}\Psi_{2,i}\Delta Y_{t-i}\; + \;
e_{1,t}
$$
3) 여기서 $Z_{t}$ ( $Y_{t},X_{t}$ 를 회귀 분석한 결과 )와 $e_{t}$ ( 회귀분석 ECM 형식 잔차)가 정상 시계열이면 두 시계열이 공적분인지 알 수 있다.
4) 해당 공적분에서 귀무가설과 대립가설은 아래와 같다.
	- 귀무가설 : 공적분 관계가 존재하지 않는다.
	- 대립가설 : 공적분 관계가 존재한다.
 5) 즉, p-value를 5%라고 지정하면, 그 이하면 귀무가설을 기각할 수 있다.

###### ** 개량형인 요한슨 검정에 대한 python library 는 아직 확인 된 것이 없음. (자료 수집 필요)**

###### (2) 그랜져 인과관계 (Granger causality)
---
- 두 개 이상의 시계열 데이터 사이에서, 한 변수의 과거 값이 다른 변수의 현재 값을 예측하는 데 유의미한 정보를 제공하는지를 판단하는 통계적 개념이다.
- 시간의 흐름 속에서 변수 간의 선후관계를 파악, 한 변수의 과거 값을 포함하여 다른 변수를 예측하는 모델이, 과거 값만을 사용하는 모델보다 더 나은 예측 성능을 나타낼 수 있다 점을 이용한 인과관계.
- 설명력을 바탕으로한 표현적(통계적) 인과관계를 뜻하며 완변한 인과관계를 나타내는 지표는 아니다.

```python
statsmodels.tsa.stattools.grangercausalitytests(
	x,
	maxlag,
	addconst=True,
	verbose=None,
	)
```

###### 15) 다변량 시계열 선형 모델
---
- VAR [[VAR 분석 과정]]
- VECM

##### 16) 시계열 비선형 모델 
---
(1) 신경망 모델
(2) 기타 비선형 모형
- ARX, NARX
- adapted filter
(3) RNN
(4) Attention (Transfomer)
(5) State Space Model
(6) Gausian Process
(7) Dynamic Factor Model

##### 16) 확률 과정 관점
---
- 시계열 현상 과정이 인과관계의 영향력의 크기로 분석되는 관점과 다르게 일련의 확률 과정으로 파악하는 과점.
 
(1) 확률과정(Stochastic Process): 
	확률법칙에 의해 생성되는 일련의 통계적 현상 - 연속적 확률과정

(2) 무작위 확률과정(Random Walk Process)
	대표적인 무작위 확률 과정 $\longrightarrow$ 백색잡음과정(White Noise Process)




