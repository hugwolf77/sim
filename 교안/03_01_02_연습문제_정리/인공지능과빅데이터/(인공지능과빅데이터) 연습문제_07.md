---
categories: 수업
title: (인공지능과빅데이터) 연습문제_07
created: 2025-04-27
tags:
  - 연습문제
  - 인공지능빅데이터
---
---
#### *(인공지능과빅데이터) 연습문제_07*
---

1. 다음 CNN:Convolution Neural Network의 설명 중 옳은 것은 무엇인가?
	1) Local connectivity 이라 함은 필터의 특정 위치 가중치는 재활용되면서 특정 요소만 학습한다. 👍
	2) CNN은 filter의 크기는 입력되는 데이터에 따라서 모든 모델에서 동일하다.
	3) stride는 입력 데이터와 filter에 따라서 항상 고정되어 있다.
	4) CNN은 주로 시계열 데이터 분석에 사용된다.
	5) CNN에 입력되는 데이터는 3차원의 입력 데이터는 처리 할 수 없다.

2. CNN에 사용되는 기법으로  주로 입력되는 데이터의 크기를 조절하는 데 사용되는 기법이다. 필터를 통하여 평균 또는 최대값으로 데이터를 변환하여 크기를 조절하는 기법은 다음 중 어느 것인가? 
	1) stride 
	2) dilated 
	3) pooling 👍
	4) padding 
	5) channel

3. CNN을 적용할 때 데이터의 가장자리 정보가 손실되어 데이터 크기가 축소되는 것을 방지하기 위하여 사용하는 기법으로 다음 중 어느 것인가?
	1) stride 
	2) dilated 
	3) pooling 
	4) padding 👍
	5) channel

4. CNN 기법의 일종으로 파라미터 수를 증가 시키지 않고 필터 내부에 홀을 추가 receptive field를 확장하는 방법으로 넓은 맥락 정보를 활용하여 세그멘테이션 결과를 얻는데 유용한 방법은?
	1) stride 
	2) dilated 👍
	3) pooling 
	4) padding 
	5) channel

5. 컨볼루션 신경망(CNN) 모델이 다층 퍼셉트론(MLP)과 비교하여 갖는 주요한 차이점으로 가장 적절하지 않은 것은 무엇인가?
	1) CNN은 이미지, 오디오 등 격자 구조 데이터를 처리하는 데 더 효과적인 반면, MLP는 일반적인 벡터 형태의 데이터에 적합하다. 
	2) CNN은 컨볼루션 연산을 통해 지역적인 특징을 추출하고, 풀링(pooling)을 통해 특징 맵의 차원을 감소시키는 반면, MLP는 모든 입력 뉴런과 모든 출력 뉴런이 연결된다. 
	3) CNN은 파라미터 공유(weight sharing) 기법을 사용하여 모델의 파라미터 수를 줄이고 공간적 불변성(spatial invariance)을 학습하는 반면, MLP는 각 연결마다 독립적인 파라미터를 갖는다. 
	4) CNN은 순차적인(sequential) 데이터 처리에 특화된 구조를 가지는 반면, MLP는 시계열 데이터나 자연어 처리에서 뛰어난 성능을 보인다. 👍
	5) CNN은 계층적인 특징 추출 방식을 통해 복잡한 패턴을 학습할 수 있는 반면, MLP는 일반적으로 깊은 네트워크를 쌓더라도 지역적인 특징을 효과적으로 포착하기 어렵다.

6. Vision 딥러닝 기법에서 ROI (Region of Interest, 관심 영역)는 분석하고자 하는 이미지 내의 특정 영역을 의미한다. 다음 설명 중 ROI를 활용하는 가장 주된 이유로 적절한 것은 무엇인가?
	1) 이미지 전체의 해상도를 높여 미세한 특징을 더 잘 감지하기 위해 
	2) 모델이 불필요한 배경 정보에 집중하는 것을 방지하고, 중요한 영역에 연산 자원을 집중시켜 효율성과 정확성을 높이기 위해 👍
	3) 이미지의 색상 분포를 균일하게 만들어 조명 변화에 강인한 특징을 추출하기 위해 
	4) 다양한 크기의 객체를 하나의 고정된 크기의 입력으로 만들기 위해 
	5) 이미지의 노이즈를 제거하고 시각적 품질을 향상시키기 위해

7. Vision 딥러닝에서 객체 감지 모델을 원스텝(one-step) 방식과 투스텝(two-step) 방식으로 분류할 때, 이 두 방식의 가장 큰 차이점은 무엇입니까?
	1) 원스텝 방식은 이미지의 특징(feature) 추출에 더 많은 계층을 사용하는 반면, 투스텝 방식은 특징 추출 네트워크가 더 얕다. 
	2) 원스텝 방식은 객체의 위치 예측과 클래스 분류를 동시에 수행하는 반면, 투스텝 방식은 객체 후보 영역을 먼저 제안하고 그 후 각 영역을 분류한다. 👍
	3) 원스텝 방식은 작은 객체 감지에 더 효과적인 반면, 투스텝 방식은 큰 객체 감지에 더 강점을 가진다. 
	4) 원스텝 방식은 학습 데이터의 양이 더 많이 필요한 반면, 투스텝 방식은 상대적으로 적은 데이터로도 학습이 가능하다. 
	5) 원스텝 방식은 모델의 크기가 더 큰 경향이 있는 반면, 투스텝 방식은 더 가벼운 모델 구조를 가진다.

8. 다음 객체 감지 모델 중, 제시된 방식(원스텝 또는 투스텝)으로 **잘못** 분류된 것은?
	1) YOLO (원스텝) 
	2) SSD (원스텝) 
	3) Faster R-CNN (투스텝) 
	4) RetinaNet (원스텝) 
	5) Mask R-CNN (원스텝) 👍

9. 딥러닝 모델, 특히 시각적 인공지능(AI) 모델에서 모델이 내린 예측 결과에 대한 시각적 설명(visual explanation)을 제공하는 기법으로 모델이 특정 클래스를 예측하는 데 가장 영향을 미친 이미지 영역을 시각적으로 보여는 기법은?
	1) Grad-CAM: Gradient-weighted Class Activation Mapping 👍
	2) SGD: Stochastic Gradient Descent
	3) Adam: Adaptive Moment Estimation
	4) Vanishing Gradient & Exploding Gradient
	5) LSTM: Long Short-Term Memory, 

10. CNN을 통해서 추출된 정보를 특징 맵으로 전환하고, 전환된 맵을 채널별로 내적하여 계산한 행열을 만들 다음, 행열을 전치행렬과 곱하여 만드는 행렬로서 이미지 분석 시 색상, 질감, 패턴 등의 스타일 정보를 담고 있는 것으로 공간 정보를 제거하고 특징 간의 상관관계 정보를 나태느는 이미지 "스타일 전이"에 사용되는 이것은 무엇인가?
	1) Min-Max Scaling
	2) Grad-CAM
	3) Dilated Convolution
	4) Gram Matrix 👍
	5) Internal Covariate Shift

11. Grad-CAM (Gradient-weighted Class Activation Mapping) 기법은 컨볼루션 신경망(CNN) 모델이 특정 이미지를 특정 클래스로 분류할 때, 이미지 내의 어떤 영역에 주목했는지 시각적으로 설명하는 데 사용된다. Grad-CAM의 핵심 아이디어로 가장 적절한 것은 무엇인가?
	1) 모델의 마지막 컨볼루션 계층의 활성화 맵(activation map)을 단순히 시각화하여 중요한 영역을 파악한다. 
	2) 모델의 모든 컨볼루션 계층의 활성화 맵을 평균하여 최종적인 중요도 맵을 생성한다.
	3) 특정 클래스에 대한 마지막 컨볼루션 계층의 그래디언트(gradient) 정보를 활용하여 각 특징 맵의 중요도를 가중치로 계산하고, 이를 선형 결합하여 클래스 활성화 맵을 생성한다. 👍
	4) 모델의 완전 연결 계층(fully connected layer)의 가중치를 역전파하여 입력 이미지의 각 픽셀이 분류 결과에 미치는 영향을 계산한다. 
	5) 이미지의 특정 영역을 가리고 모델의 예측 변화를 관찰하여 중요한 영역을 추론한다.

> - 특정 클래스에 대한 모델의 예측값에 대한 마지막 컨볼루션 계층의 각 활성화 맵의 그래디언트를 계산
> - 계산된 그래디언트 가중치를 사용하여 마지막 컨볼루션 계층의 활성화 맵들을 선형 결합하여 클래스 활성화 맵(Class Activation Map, CAM)
> - 생성된 클래스 활성화 맵을 입력 이미지에 겹쳐 시각화함. 이미지 내의 어떤 영역에 집중적으로 반응했는지 직관적으로 확인.

12. YOLO (You Only Look Once) 객체 감지 모델의 주요 특징으로 가장 적절하지 않은 것은 무엇인가?
	1) 이미지를 격자(grid) 셀로 나누고, 각 셀에서 여러 개의 경계 상자(bounding box)와 그에 대한 클래스 확률을 동시에 예측한다. 
	2) 이미지 전체를 한 번의 네트워크 통과(single forward pass)로 처리하여 실시간 객체 감지에 적합한 빠른 속도를 제공한다. 
	3) 투스텝(two-step) 방식에 비해 배경(background)을 객체로 잘못 예측하는 오탐(false positive) 비율이 낮다. 👍
	4) 각 경계 상자는 예측된 클래스 확률과 해당 경계 상자의 신뢰도 점수(confidence score)를 함께 출력한다. 
	5) 다양한 크기와 종횡비(aspect ratio)의 객체를 감지하기 위해 여러 개의 앵커 박스(anchor boxes) 개념을 사용한다.

13. 최근 비전 딥러닝 모델 개발에서 NAS(Neural Architecture Search)는 효율성과 성능을 극대화하는 맞춤형 신경망 구조를 자동으로 탐색하는 중요한 기술로 부상하였다. 다음 중 NAS가 비전 모델 개발에 기여하는 가장 핵심적인 방식은 무엇인가?
	1) 인간 전문가가 설계한 기존의 비전 모델 구조를 분석하여 단점을 개선하는 방식을 제안한다. `모델 분석의 기법일 수 있지만 NAS의 주 기능은 아님`
	2) 대규모 이미지 데이터셋을 효율적으로 레이블링하고 관리하는 자동화 파이프라인을 구축한다. 
	3) 주어진 하드웨어 자원 제약 조건 하에서 목표 성능(정확도, 속도 등)을 최적화하는 새로운 비전 모델 구조를 자동으로 탐색하고 생성한다.  👍
	4) 학습된 비전 모델의 파라미터 수를 압축하거나 양자화하여 모델의 크기를 줄이고 추론 속도를 향상시킨다. `이미 학습된 모델의 효율성을 높이는 기술을 말하며 구조 탐색인 NAS와 거리`
	5) 다양한 학습 전략(learning rate scheduling, optimizer 선택 등)을 자동으로 탐색하여 모델의 수렴 속도와 최종 성능을 개선한다. `AutoML의 한 분야이지만 NAS는 모델 구조 탐색에 집중`

14. 다음 중 신경망 구조 탐색(NAS, Neural Architecture Search) 기술을 주요하게 활용하여 개발되지 않은 비전 모델은 무엇인가?
	1) NASNet  
	2) EfficientNet 
	3) MobileNetV3 (일부 NAS 활용) 
	4) ResNet 👍
	5) AmoebaNet

15. 딥러닝 모델 양자화(Quantization)는 모델의 파라미터와 활성화 값들을 낮은 정밀도의 데이터 타입으로 변환하여 모델을 압축하고 연산 속도를 향상시키는 기술이다. 다음 중 양자화의 주요 이점으로 가장 적절하지 않은 것은 무엇인가?
	1) 모델 파일 크기 감소로 인한 저장 공간 및 전송 대역폭 절약 
	2) 낮은 정밀도 연산에 최적화된 하드웨어에서의 추론 속도 향상 
	3) 모델의 에너지 소비 감소로 인한 모바일 및 임베디드 기기에서의 효율성 증대 
	4) 모델의 정확도 향상 및 과적합(overfitting) 방지 효과 👍
	5) 특수 하드웨어 없이 CPU 환경에서의 추론 속도 소폭 향상 

16. LoRA (Low-Rank Adaptation) 기법은 대규모 언어 모델(LLM)과 같은 거대 모델을 효율적으로 파인튜닝하기 위한 파라미터 효율적인 방법이다. 다음 중 LoRA 기법의 핵심 아이디어와 양자화와의 주요 연관성을 가장 잘 설명하는 것은 무엇인가?
	1) LoRA는 모델의 모든 파라미터를 낮은 정밀도로 양자화하여 파인튜닝 과정의 메모리 요구량을 줄이고 연산 속도를 높인다. 
	2) LoRA는 모델의 원래 파라미터를 동결시키고, 낮은 랭크(rank)의 작은 행렬 쌍을 추가하여 이 추가 파라미터만 학습시켜 다운스트림 태스크에 적응시킨다. 👍
	3) LoRA는 모델의 활성화 함수를 낮은 정밀도로 양자화하여 메모리 사용량을 줄이고, 파인튜닝 시 발생하는 그래디언트 폭주 문제를 완화한다. 
	4) LoRA는 모델의 구조를 재설계하여 파라미터 수를 줄이고, 양자화 후에도 성능 저하를 최소화하도록 한다. 
	5) LoRA는 Knowledge Distillation 기법을 사용하여 거대 모델의 지식을 작은 양자화된 모델로 이전하는 방식으로 파인튜닝 효과를 얻는다.

17. 프롬프트 엔지니어링(Prompt Engineering)은 대규모 언어 모델(LLM)이 원하는 답변이나 행동을 생성하도록 유도하기 위해 입력 텍스트(프롬프트)를 설계하고 최적화하는 기술이다. 다음 중 효과적인 프롬프트 엔지니어링의 가장 핵심적인 목표는 무엇인가?
	1) 모델이 답변 생성 시 사용하는 파라미터 수를 최소화하여 연산 효율성을 높이는 것 
	2) 모델이 학습 데이터에 존재하지 않는 완전히 새로운 정보를 생성하도록 유도하는 것 
	3) 모델이 질문의 의도를 정확하게 이해하고, 사용자의 요구에 부합하는 정확하고 일관성 있는 답변을 생성하도록 하는 것 👍
	4) 모델이 다양한 스타일과 어투로 답변을 생성하도록 무작위성을 최대화하는 것
	5) 모델이 특정 주제에 대해 사용자의 선입견이나 편향된 시각을 강화하는 답변을 생성하도록 유도하는 것

18. 대규모 언어 모델(LLM)의 답변 생성 능력을 향상시키는 기술인 RAG (Retrieval-Augmented Generation)의 핵심적인 작동 방식으로 가장 적절한 것은 무엇인가?
	1) 모델이 사전에 학습한 방대한 지식만을 활용하여 질문에 답변하는 방식 
	2) 모델이 외부 지식 베이스에서 관련 정보를 검색(retrieval)하고, 검색된 정보를 기반으로 답변을 생성(generation)하는 방식 👍
	3) 모델이 생성한 답변의 품질을 향상시키기 위해 별도의 평가 모델을 사용하는 방식 
	4) 모델이 질문의 의도를 파악하기 위해 다양한 프롬프팅 기법을 조합하여 사용하는 방식 
	5) 모델이 답변 생성 과정에서 발생하는 환각 현상(hallucination)을 줄이기 위해 디코딩 전략을 조정하는 방식

19. Knowledge Distillation(지식 증류)은 크고 복잡한 "교사(Teacher)" 모델의 지식을 작고 가벼운 "학생(Student)" 모델로 이전하여 학생 모델의 성능을 향상시키는 기법법으로 다음 중 Knowledge Distillation의 핵심적인 목표로 가장 적절한 것은 무엇인가?
	1) 학생 모델의 학습 속도를 교사 모델보다 빠르게 만드는 것 
	2) 학생 모델의 구조를 교사 모델과 동일하게 만들어 성능을 최대한 복사하는 것 
	3) 학생 모델이 교사 모델의 예측 분포를 모방하도록 학습시켜, 일반화 성능을 향상시키면서 모델 크기를 줄이는 것 👍
	4) 학생 모델이 교사 모델의 내부 표현(intermediate representation)을 학습하여 특징 추출 능력을 향상시키는 것 
	5) 학생 모델의 파라미터 수를 교사 모델보다 훨씬 많게 만들어 더 강력한 표현력을 갖도록 하는 것

20. 대규모 언어 모델(LLM)의 추론 능력을 향상시키는 효과적인 프롬프팅 전략 중 하나인 CoT (Chain-of-Thought) 프롬프팅의 핵심 아이디어로 가장 적절한 것은 무엇입니까?
	1) 모델에게 최종 답변만을 직접적으로 요구하여 답변 생성 속도를 높이는 것 
	2) 모델에게 다양한 예시를 제공하여 답변 형식을 학습시키고, 최종 답변을 생성하도록 유도하는 것 (Few-shot learning) 
	3) 모델이 문제 해결 과정을 단계별로 생각하고 설명하도록 유도하여, 복잡한 추론 과정을 거쳐 정확한 최종 답변을 도출하도록 하는 것 👍
	4) 모델에게 특정 지식이나 정보를 직접적으로 주입하여 답변의 정확성을 높이는 것 (Retrieval-augmented generation) 
	5) 모델의 답변 생성 시 무작위성을 증가시켜 더욱 창의적이고 다양한 답변을 얻도록 하는 것

21. 대규모 언어 모델(LLM)을 기반으로 구축된 에이전트(Agent) 시스템은 LLM의 강력한 언어 이해 및 생성 능력에 더하여 특정 작업을 수행하기 위한 추가적인 기능을 통합한다. 다음 중 LLM 기반 에이전트의 가장 핵심적인 특징으로 볼 수 없는 것은 무엇인가?
	1) 자연어 이해(NLU) 능력을 바탕으로 사용자의 지시나 질문의 의도를 파악한다. 
	2) 외부 도구나 API를 호출하여 정보를 검색하거나 특정 작업을 실행할 수 있는 능력 (Tool Use) 
	3) 복잡한 목표를 달성하기 위해 여러 단계를 거치는 계획(Planning) 능력 
	4) 오직 텍스트 형식의 정보만을 처리하고 생성할 수 있는 제한적인 능력  👍
	5) 이전 대화의 맥락을 기억하고 활용하여 일관성 있는 상호작용을 제공하는 능력 (Memory) 